{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de606b8",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# è‡ªç„¶è¯­è¨€æ¨æ–­ï¼šå¾®è°ƒBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8292939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:03:01.182131Z",
     "iopub.status.busy": "2023-08-18T07:03:01.181284Z",
     "iopub.status.idle": "2023-08-18T07:03:04.072192Z",
     "shell.execute_reply": "2023-08-18T07:03:04.070948Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b8eaa",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## [**åŠ è½½é¢„è®­ç»ƒçš„BERT**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c4aae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:03:04.081229Z",
     "iopub.status.busy": "2023-08-18T07:03:04.079078Z",
     "iopub.status.idle": "2023-08-18T07:03:04.087710Z",
     "shell.execute_reply": "2023-08-18T07:03:04.086601Z"
    },
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
    "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "d2l.DATA_HUB['bert.small'] = (d2l.DATA_URL + 'bert.small.torch.zip',\n",
    "                              'c72329e68a732bef0452e4b96a1c341c8910f81f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6087d2",
   "metadata": {},
   "source": [
    "| å‚æ•°                     | å«ä¹‰                     | å…¸å‹å€¼            |\n",
    "| ---------------------- | ---------------------- | -------------- |\n",
    "| **`pretrained_model`** | é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼ˆå¦‚'bert.small'ï¼‰ | `'bert.small'` |\n",
    "| **`num_hiddens`**      | BERTéšè—å±‚ç»´åº¦              | 256            |\n",
    "| **`ffn_num_hiddens`**  | å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦              | 512            |\n",
    "| **`num_heads`**        | å¤šå¤´æ³¨æ„åŠ›å¤´æ•°                | 4              |\n",
    "| **`num_layers`**       | Transformerå±‚æ•°          | 2              |\n",
    "| **`dropout`**          | Dropoutæ¯”ç‡              | 0.2            |\n",
    "| **`max_len`**          | æœ€å¤§åºåˆ—é•¿åº¦                 | 512            |\n",
    "| **`devices`**          | è®¡ç®—è®¾å¤‡åˆ—è¡¨                 | `[cuda:0]`     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d49c25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:03:04.095593Z",
     "iopub.status.busy": "2023-08-18T07:03:04.093665Z",
     "iopub.status.idle": "2023-08-18T07:03:04.106137Z",
     "shell.execute_reply": "2023-08-18T07:03:04.104967Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
    "                          num_heads, num_layers, dropout, max_len, devices):\n",
    "    # 1. ä¸‹è½½å¹¶è§£å‹é¢„è®­ç»ƒæ¨¡å‹ï¼šä»d2læœåŠ¡å™¨ä¸‹è½½é¢„è®­ç»ƒçš„BERTæ¨¡å‹æ–‡ä»¶ï¼ˆåŒ…å«è¯æ±‡è¡¨å’Œå‚æ•°ï¼‰ï¼Œè§£å‹åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•ï¼Œè¿”å›è·¯å¾„\n",
    "    data_dir = d2l.download_extract(pretrained_model)\n",
    "    '''\n",
    "    2. åˆ›å»ºç©ºè¯æ±‡è¡¨å¹¶åŠ è½½é¢„å®šä¹‰è¯æ±‡\n",
    "    å…ˆåˆ›å»ºç©ºVocabå¯¹è±¡ï¼šå› ä¸ºè¦ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„è¯æ±‡è¡¨ï¼Œè€Œéç°åœºæ„å»º\n",
    "    åŠ è½½è¯æ±‡è¡¨ï¼šä»vocab.jsonè¯»å–è¯å…ƒåˆ°ç´¢å¼•çš„æ˜ å°„åˆ—è¡¨\n",
    "        æ–‡ä»¶å†…å®¹ç¤ºä¾‹ï¼š[\"<pad>\",\"<unk>\",\"the\",\"a\",...,\"bert\"]\n",
    "    '''\n",
    "    vocab = d2l.Vocab()\n",
    "    vocab.idx_to_token = json.load(open(os.path.join(data_dir,\n",
    "        'vocab.json')))\n",
    "    # 3. æ„å»ºåå‘æ˜ å°„\n",
    "    # token_to_idxï¼šä»è¯å…ƒåˆ°ç´¢å¼•çš„å­—å…¸ï¼Œç”¨äºå¿«é€ŸæŸ¥æ‰¾\n",
    "    # å¿…è¦æ€§ï¼šæ¨¡å‹è¾“å…¥éœ€è¦ç´¢å¼•ï¼Œè€Œvocab.jsonåªä¿å­˜äº†æ­£å‘æ˜ å°„\n",
    "    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n",
    "        vocab.idx_to_token)}\n",
    "    '''\n",
    "    4. åˆå§‹åŒ–BERTæ¨¡å‹ç»“æ„\n",
    "    å…³é”®ï¼šlen(vocab)ç¡®ä¿æ¨¡å‹è¯æ±‡è¡¨å¤§å°ä¸é¢„è®­ç»ƒä¸€è‡´\n",
    "    è¶…å‚æ•°åŒ¹é… ï¼šæ‰€æœ‰ç»´åº¦å‚æ•°å¿…é¡»ä¸é¢„è®­ç»ƒæ—¶å®Œå…¨ç›¸åŒï¼Œå¦åˆ™æ— æ³•åŠ è½½æƒé‡\n",
    "        num_hiddens=256ï¼šéšè—å±‚ç»´åº¦\n",
    "        num_heads=4ï¼š4å¤´æ³¨æ„åŠ›\n",
    "        num_layers=2ï¼š2å±‚Transformer\n",
    "        key_size=256, query_size=256, value_size=256ï¼šQ/K/VæŠ•å½±ç»´åº¦\n",
    "        ffn_num_input=256, ffn_num_hiddens=512ï¼šå‰é¦ˆç½‘ç»œè¾“å…¥/éšè—ç»´åº¦\n",
    "    '''\n",
    "    bert = d2l.BERTModel(len(vocab), num_hiddens, norm_shape=[256],\n",
    "                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n",
    "                         num_heads=4, num_layers=2, dropout=0.2,\n",
    "                         max_len=max_len, key_size=256, query_size=256,\n",
    "                         value_size=256, hid_in_features=256,\n",
    "                         mlm_in_features=256, nsp_in_features=256)\n",
    "    '''\n",
    "    5. åŠ è½½é¢„è®­ç»ƒå‚æ•°\n",
    "    pretrained.paramsï¼šä¿å­˜é¢„è®­ç»ƒæƒé‡çš„PyTorchåºåˆ—åŒ–æ–‡ä»¶\n",
    "    torch.load()ï¼šä»ç£ç›˜è¯»å–æƒé‡å­—å…¸\n",
    "    load_state_dict()ï¼šå°†æƒé‡åŠ è½½åˆ°æ¨¡å‹ä¸­\n",
    "    è¦æ±‚ï¼šæ¨¡å‹ç»“æ„å¿…é¡»ä¸é¢„è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼Œå¦åˆ™ä¼šæŠ¥é”™\n",
    "    ä¸¥æ ¼åŒ¹é…ï¼šå±‚åã€å‚æ•°å½¢çŠ¶å¿…é¡»ä¸€ä¸€å¯¹åº”\n",
    "    '''\n",
    "    bert.load_state_dict(torch.load(os.path.join(data_dir,\n",
    "                                                 'pretrained.params')))\n",
    "    # 6. è¿”å›æ¨¡å‹å’Œè¯æ±‡è¡¨\n",
    "    return bert, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83fa73f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:03:04.114631Z",
     "iopub.status.busy": "2023-08-18T07:03:04.112010Z",
     "iopub.status.idle": "2023-08-18T07:03:08.335657Z",
     "shell.execute_reply": "2023-08-18T07:03:08.334563Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "devices = d2l.try_all_gpus()\n",
    "bert, vocab = load_pretrained_model(\n",
    "    'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n",
    "    num_layers=2, dropout=0.1, max_len=512, devices=devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4be336",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "## [**å¾®è°ƒBERTçš„æ•°æ®é›†**]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75784508",
   "metadata": {},
   "source": [
    "```Python\n",
    "åŸå§‹ dataset:\n",
    "(\n",
    "    [\"A man is walking\", \"The woman sings\", ...],      # premises\n",
    "    [\"A person moves\", \"The lady is singing\", ...],    # hypotheses\n",
    "    [0, 1, 2, ...]                                    # labels\n",
    ")\n",
    "\n",
    "â†“ dataset[:2]\n",
    "\n",
    "[\n",
    "    [\"A man is walking\", \"The woman sings\", ...],      # ç¬¬ä¸€å±‚éå†\n",
    "    [\"A person moves\", \"The lady is singing\", ...]\n",
    "]\n",
    "\n",
    "â†“ [s.lower() for s in sentences]\n",
    "\n",
    "[\n",
    "    [\"a man is walking\", \"the woman sings\", ...],\n",
    "    [\"a person moves\", \"the lady is singing\", ...]\n",
    "]\n",
    "\n",
    "â†“ d2l.tokenize(...)\n",
    "\n",
    "[\n",
    "    [[\"a\", \"man\", \"is\", \"walking\"], [\"the\", \"woman\", \"sings\"], ...],\n",
    "    [[\"a\", \"person\", \"moves\"], [\"the\", \"lady\", \"is\", \"singing\"], ...]\n",
    "]\n",
    "\n",
    "â†“ *ï¼ˆè§£å‹ï¼‰\n",
    "\n",
    "list1 = [[\"a\", \"man\", ...], [\"the\", \"woman\", ...], ...]\n",
    "list2 = [[\"a\", \"person\", ...], [\"the\", \"lady\", ...], ...]\n",
    "\n",
    "â†“ zip(list1, list2)\n",
    "\n",
    "[\n",
    "    ([\"a\", \"man\", \"is\", \"walking\"], [\"a\", \"person\", \"moves\"]),\n",
    "    ([\"the\", \"woman\", \"sings\"], [\"the\", \"lady\", \"is\", \"singing\"]),\n",
    "    ...\n",
    "]\n",
    "\n",
    "â†“ æœ€ç»ˆåˆ—è¡¨æ¨å¯¼å¼\n",
    "\n",
    "[\n",
    "    [[\"a\", \"man\", \"is\", \"walking\"], [\"a\", \"person\", \"moves\"]],\n",
    "    [[\"the\", \"woman\", \"sings\"], [\"the\", \"lady\", \"is\", \"singing\"]],\n",
    "    ...\n",
    "]\n",
    "```\n",
    "```python\n",
    "# å¯¹äºå‰æ\"A man walks\" + å‡è®¾\"A person moves\"\n",
    "token_ids: [101, 1037, 2153, 5231, 102, 1037, 2153, 5231, 102, 0, 0, ...]\n",
    "segments:  [0,   0,    0,    0,    0,   1,    1,    1,    1,   0, 0, ...]\n",
    "valid_len:  9  (å®é™…è¯å…ƒæ•°)\n",
    "label:      0  (è•´å«)\n",
    "```\n",
    "**ä¸ºä»€ä¹ˆéœ€è¦è¿™ç§å¤æ‚ç»“æ„ï¼Ÿ**\n",
    "\n",
    "BERTè¦æ±‚è¾“å…¥åŒ…å«ï¼š\n",
    "1. è¯å…ƒIDï¼šè¯è¯­çš„æ•°å­—è¡¨ç¤º\n",
    "2. æ®µè½æ®µIDï¼šåŒºåˆ†å¥å­Aå’Œå¥å­B\n",
    "3. æœ‰æ•ˆé•¿åº¦ï¼šé¿å…æ³¨æ„åŠ›è®¡ç®—åˆ°å¡«å……éƒ¨åˆ†\n",
    "```python\n",
    "token_ids: [101, 1037, 2153, 5231, 102, 1037, 2153, 5231, 102, 0, 0, ...]\n",
    "            # [CLS]  a   man walks [SEP]  a  person moves [SEP] [PAD]...\n",
    "segments:  [0,   0,    0,    0,     0,   1,   1,    1,    1,   0, 0, ...]\n",
    "            # å‰æéƒ¨åˆ†ä¸º0          å‡è®¾éƒ¨åˆ†ä¸º1      å¡«å……éƒ¨åˆ†ä¸º0\n",
    "valid_len: 9  # å®é™…è¯å…ƒæ•°ï¼ˆä¸å«å¡«å……ï¼‰\n",
    "```\n",
    "```python\n",
    "è¾“å…¥: (['a', 'man', 'walks'], ['a', 'person', 'moves'])\n",
    "\n",
    "1. æˆªæ–­ï¼ˆæœ¬ä¾‹æ— éœ€ï¼‰\n",
    "   â†’ p_tokens: ['a', 'man', 'walks']\n",
    "   â†’ h_tokens: ['a', 'person', 'moves']\n",
    "\n",
    "2. get_tokens_and_segments\n",
    "   â†’ tokens:  ['<cls>', 'a', 'man', 'walks', '<sep>', 'a', 'person', 'moves', '<sep>']\n",
    "   â†’ segments: [0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "3. è½¬IDå¹¶å¡«å……è‡³max_len=12\n",
    "   â†’ token_ids: [101, 1037, 2153, 5231, 102, 1037, 2153, 5231, 102, 0, 0, 0]\n",
    "   â†’ segments:  [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0]\n",
    "\n",
    "4. valid_len = 9\n",
    "\n",
    "è¿”å›: ([101, 1037, 2153, 5231, 102, 1037, 2153, 5231, 102, 0, 0, 0],\n",
    "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
    "       9)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6424fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:03:08.341547Z",
     "iopub.status.busy": "2023-08-18T07:03:08.340816Z",
     "iopub.status.idle": "2023-08-18T07:03:08.359529Z",
     "shell.execute_reply": "2023-08-18T07:03:08.358243Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    datasetï¼šåŸå§‹SNLIæ•°æ®å…ƒç»„ (premises,hypotheses,labels)\n",
    "    max_lenï¼šBERTè¾“å…¥çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆå¦‚128æˆ–512ï¼‰\n",
    "    vocabï¼šBERTè¯æ±‡è¡¨ï¼ˆè‹¥ä¸æä¾›åˆ™ç°åœºæ„å»ºï¼Œä½†é€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒè¯è¡¨ï¼‰\n",
    "    '''\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        # 1. è¯å…ƒåŒ–æ‰€æœ‰å¥å­å¯¹ï¼šç»“æœï¼šæ¯ä¸ªå…ƒç´ æ˜¯[å‰æè¯å…ƒåˆ—è¡¨,å‡è®¾è¯å…ƒåˆ—è¡¨]\n",
    "        '''\n",
    "        å±‚1ï¼šæå–å‰æå’Œå‡è®¾åˆ—è¡¨dataset[:2]\n",
    "            datasetæ˜¯å…ƒç»„ (premises,hypotheses,labels)\n",
    "            dataset[:2]åˆ‡ç‰‡åå¾—åˆ°[premises,hypotheses]ï¼ˆå‰ä¸¤ä¸ªå…ƒç´ ï¼‰\n",
    "        å±‚2ï¼šéå†ä¸¤ä¸ªå¥å­åˆ—è¡¨for sentences in dataset[:2]\n",
    "            ç¬¬ä¸€æ¬¡è¿­ä»£ï¼šsentences=premisesï¼ˆå‰æå¥å­åˆ—è¡¨ï¼‰\n",
    "            ç¬¬äºŒæ¬¡è¿­ä»£ï¼šsentences=hypothesesï¼ˆå‡è®¾å¥å­åˆ—è¡¨ï¼‰\n",
    "        å±‚3ï¼šå°å†™åŒ–å¤„ç†[s.lower() for s in sentences]\n",
    "            ä½œç”¨ï¼šå°†æ¯ä¸ªå¥å­çš„æ‰€æœ‰å­—ç¬¦è½¬ä¸ºå°å†™\n",
    "            åŸå› ï¼šç»Ÿä¸€å¤§å°å†™ï¼Œå‡å°‘è¯æ±‡è¡¨å†—ä½™ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›\n",
    "            ç¤ºä¾‹ï¼š\"A man is walking\"â†’\"a man is walking\"\n",
    "        å±‚4ï¼šæ‰¹é‡åˆ†è¯\n",
    "            d2l.tokenize([s.lower() for s in sentences])\n",
    "            å¯¹ä¸¤ä¸ªåˆ—è¡¨åˆ†åˆ«è°ƒç”¨ï¼štokenizeå°†å¥å­å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºè¯å…ƒåˆ—è¡¨\n",
    "            ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šå¯¹premisesåˆ†è¯\n",
    "            è¾“å…¥ï¼š[\"a man is walking\",\"the woman sings\"]\n",
    "            è¾“å‡ºï¼š[[\"a\",\"man\",\"is\",\"walking\"],[\"the\",\"woman\",\"sings\"]]\n",
    "            ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šå¯¹hypothesesåˆ†è¯\n",
    "            è¾“å…¥ï¼š[\"a person moves\",\"the lady is singing\"]\n",
    "            è¾“å‡ºï¼š[[\"a\",\"person\",\"moves\"], [\"the\",\"lady\",\"is\",\"singing\"]]\n",
    "        å±‚5ï¼šè§£å‹æ“ä½œï¼ˆæœ€å…³é”®ï¼‰*[d2l.tokenize(...)]\n",
    "            *æ˜¯\"è§£å‹è¿ç®—ç¬¦\"ï¼Œå°†åˆ—è¡¨ä¸­çš„å…ƒç´ ä½œä¸ºç‹¬ç«‹å‚æ•°ä¼ é€’\n",
    "            ç»“æœï¼šç›¸å½“äºå°†ä¸¤ä¸ªåˆ†è¯ç»“æœåˆ—è¡¨å¹³é“ºä¸ºä¸¤ä¸ªç‹¬ç«‹å‚æ•°\n",
    "            # ç­‰ä»·äºï¼š\n",
    "            zip(\n",
    "                [[\"a\", \"man\", ...], [\"the\", \"woman\", ...]],  # å‰æè¯å…ƒåˆ—è¡¨\n",
    "                [[\"a\", \"person\", ...], [\"the\", \"lady\", ...]]  # å‡è®¾è¯å…ƒåˆ—è¡¨\n",
    "            )\n",
    "        å±‚6ï¼šæŒ‰æ ·æœ¬é…å¯¹for p_tokens, h_tokens in zip(...)\n",
    "            zipå°†ä¸¤ä¸ªåˆ—è¡¨çš„å¯¹åº”å…ƒç´ é…å¯¹\n",
    "            ç¬¬ä¸€æ¬¡è¿­ä»£ï¼šp_tokens=[\"a\",\"man\",\"is\",\"walking\"],h_tokens=[\"a\",\"person\",\"moves\"]\n",
    "            ç¬¬äºŒæ¬¡è¿­ä»£ï¼šp_tokens=[\"the\",\"woman\",\"sings\"],h_tokens=[\"the\",\"lady\",\"is\",\"singing\"]\n",
    "        å±‚7ï¼šæ„å»ºé…å¯¹åˆ—è¡¨[[p_tokens, h_tokens] for ...]\n",
    "            æœ€ç»ˆç»“æ„ï¼šæ¯ä¸ªå…ƒç´ æ˜¯ [å‰æè¯å…ƒ,å‡è®¾è¯å…ƒ]çš„åˆ—è¡¨\n",
    "        '''\n",
    "        all_premise_hypothesis_tokens = [[\n",
    "            p_tokens, h_tokens] for p_tokens, h_tokens in zip(\n",
    "            *[d2l.tokenize([s.lower() for s in sentences])\n",
    "              for sentences in dataset[:2]])]\n",
    "        '''\n",
    "        2. æ ‡ç­¾å¼ é‡åŒ–\n",
    "        dataset[2]ï¼šæ•´æ•°æ ‡ç­¾åˆ—è¡¨ï¼ˆ0=è•´å«,1=çŸ›ç›¾,2=ä¸­æ€§ï¼‰\n",
    "        è½¬æ¢ä¸ºPyTorchå¼ é‡ï¼Œä¾¿äºåç»­è®¡ç®—æŸå¤±\n",
    "        '''\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        # 3. ä¿å­˜é…ç½®ï¼šä¿å­˜è¯æ±‡è¡¨å’Œæœ€å¤§é•¿åº¦ä¾›åç»­ä½¿ç”¨\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        '''\n",
    "        4. é¢„å¤„ç†ä¸ºBERTè¾“å…¥æ ¼å¼ï¼ˆå…ƒç»„è§£åŒ…ï¼‰\n",
    "        all_token_idsï¼šåˆå¹¶å‰æå’Œå‡è®¾å¹¶å¡«å……åçš„è¯å…ƒIDåºåˆ—\n",
    "            æ ¼å¼ï¼š[CLS]+premise+[SEP]+hypothesis+[SEP]+[PAD]...\n",
    "            å½¢çŠ¶ï¼š(num_examples,max_len)\n",
    "        all_segmentsï¼šæ®µè½IDï¼ˆåŒºåˆ†å‰æå’Œå‡è®¾ï¼‰\n",
    "            å‰æéƒ¨åˆ†æ ‡è®°ä¸º0ï¼Œå‡è®¾éƒ¨åˆ†æ ‡è®°ä¸º1\n",
    "            å½¢çŠ¶ï¼š(num_examples,max_len)\n",
    "        valid_lensï¼šæœ‰æ•ˆé•¿åº¦ï¼ˆä¸å«å¡«å……çš„å®é™…è¯å…ƒæ•°ï¼‰\n",
    "            ç”¨äºæ³¨æ„åŠ›æ©ç \n",
    "            å½¢çŠ¶ï¼š(num_examples,)\n",
    "        '''\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        # 5. æ‰“å°ç»Ÿè®¡\n",
    "        print('read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        '''\n",
    "        1. åˆ›å»ºè¿›ç¨‹æ± \n",
    "        ä½œç”¨ï¼šåˆ›å»ºä¸€ä¸ªåŒ…å«4ä¸ªworkerè¿›ç¨‹çš„è¿›ç¨‹æ± \n",
    "        ç›®çš„ï¼šåŠ é€Ÿå¤§è§„æ¨¡æ•°æ®é¢„å¤„ç†ï¼ˆSNLIæœ‰55ä¸‡æ ·æœ¬ï¼‰ï¼Œåˆ©ç”¨å¤šæ ¸CPUå¹¶è¡Œè®¡ç®—\n",
    "        åŸç†ï¼šå°†æ•°æ®åˆ†ç‰‡ååˆ†é…ç»™å¤šä¸ªè¿›ç¨‹åŒæ—¶å¤„ç†ï¼Œé¿å…å•è¿›ç¨‹CPUç“¶é¢ˆ\n",
    "        '''\n",
    "        pool = ThreadPool(4)\n",
    "        # pool = multiprocessing.Pool(4)  # ä½¿ç”¨4ä¸ªè¿›ç¨‹\n",
    "        '''\n",
    "        2. å¹¶è¡Œæ˜ å°„å¤„ç†\n",
    "        self._mp_workerï¼šå¤šè¿›ç¨‹å·¥ä½œå‡½æ•°ï¼ˆæœªæ˜¾ç¤ºä½†å¯æ¨æ–­ï¼‰ï¼Œå¤„ç†å•ä¸ªæ ·æœ¬ï¼š\n",
    "        è¾“å…¥ï¼š[premise_tokens,hypothesis_tokens]ï¼ˆä¸€ä¸ªè¯å…ƒå¯¹ï¼‰\n",
    "        å¤„ç†ï¼š\n",
    "            æ·»åŠ ç‰¹æ®Šç¬¦å·ï¼š[CLS]ã€[SEP]\n",
    "            è½¬æ¢ä¸ºè¯å…ƒIDï¼šself.vocab[...]\n",
    "            ç”Ÿæˆæ®µè½IDï¼šå‰æéƒ¨åˆ†ä¸º0ï¼Œå‡è®¾éƒ¨åˆ†ä¸º1\n",
    "            æˆªæ–­/å¡«å……åˆ°max_len\n",
    "            è®¡ç®—æœ‰æ•ˆé•¿åº¦ï¼ˆä¸å«å¡«å……ï¼‰\n",
    "        è¿”å›ï¼š(token_ids,segments,valid_len)ä¸‰å…ƒç»„\n",
    "        pool.map()ï¼šå°†all_premise_hypothesis_tokensåˆ—è¡¨åˆ†ç‰‡ç»™å„è¿›ç¨‹å¹¶è¡Œå¤„ç†\n",
    "        æ•ˆæœï¼š4ä¸ªè¿›ç¨‹åŒæ—¶å¤„ç†ä¸åŒå­é›†ï¼Œé€Ÿåº¦æå‡è¿‘4å€\n",
    "        '''\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        '''\n",
    "        3. æå–å¹¶è¡Œç»“æœ\n",
    "        outæ˜¯æ¯ä¸ªè¿›ç¨‹çš„è¿”å›ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯(token_ids,segments,valid_len)\n",
    "        åˆ—è¡¨æ¨å¯¼å¼ï¼šå°†ä¸‰ä¸ªç»„ä»¶åˆ†åˆ«æå–åˆ°ç‹¬ç«‹åˆ—è¡¨\n",
    "        token_idsï¼šè¯å…ƒIDåˆ—è¡¨\n",
    "            åˆ—è¡¨æ¨å¯¼å¼ï¼šéå†outä¸­æ¯ä¸ªå…ƒç»„\n",
    "            æ¨¡å¼åŒ¹é…ï¼štoken_ids,segments,valid_lenè§£åŒ…æ¯ä¸ªå…ƒç»„\n",
    "            ç»“æœï¼šåªä¿ç•™ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆtoken_idsï¼‰ï¼Œæ„æˆæ–°åˆ—è¡¨\n",
    "            å½¢çŠ¶ï¼š[(æ ·æœ¬0çš„token_ids),(æ ·æœ¬1çš„token_ids),...]\n",
    "            åç»­è½¬æ¢ï¼štorch.tensor(all_token_ids)â†’å½¢çŠ¶(num_examples,max_len)\n",
    "        segmentsï¼šæ®µè½IDåˆ—è¡¨\n",
    "            é€»è¾‘åŒä¸Šï¼šéå†å¹¶æå–ç¬¬äºŒä¸ªå…ƒç´ ï¼ˆsegmentsï¼‰\n",
    "            ç»“æœï¼šæ‰€æœ‰æ ·æœ¬çš„æ®µè½IDåˆ—è¡¨\n",
    "            åç»­è½¬æ¢ï¼štorch.tensor(all_segments)â†’å½¢çŠ¶(num_examples,max_len)\n",
    "        valid_lenï¼šæœ‰æ•ˆé•¿åº¦ï¼ˆæ•´æ•°ï¼‰\n",
    "            éå†å¹¶æå–ç¬¬ä¸‰ä¸ªå…ƒç´ ï¼ˆvalid_lenï¼‰\n",
    "            ç»“æœï¼šæ‰€æœ‰æ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦åˆ—è¡¨\n",
    "            åç»­è½¬æ¢ï¼štorch.tensor(valid_lens)â†’å½¢çŠ¶(num_examples,)\n",
    "        '''\n",
    "        all_token_ids = [token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments  = [segments  for token_ids, segments, valid_len in out]\n",
    "        valid_lens    = [valid_len for token_ids, segments, valid_len in out]\n",
    "        '''\n",
    "        4. è½¬æ¢ä¸ºå¼ é‡\n",
    "        dtype=torch.longï¼šè¯å…ƒIDå’Œæ®µè½IDéœ€ä¸ºæ•´æ•°ç±»å‹\n",
    "        è¿”å›å€¼ï¼š\n",
    "            all_token_idsï¼š(num_examples,max_len)-æ‰€æœ‰æ ·æœ¬çš„è¯å…ƒIDçŸ©é˜µ\n",
    "            all_segmentsï¼š(num_examples,max_len)-æ®µè½IDçŸ©é˜µ\n",
    "            valid_lensï¼š(num_examples,)-æ¯ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦å‘é‡\n",
    "        '''\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        # 1. è§£åŒ…è¯å…ƒå¯¹ï¼šè¾“å…¥ï¼špremise_hypothesis_tokensæ˜¯[premise_tokens,hypothesis_tokens]åˆ—è¡¨\n",
    "        # ç¤ºä¾‹ï¼š[['a','man','walks'],['a','person','moves']]\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        '''\n",
    "        2. æˆªæ–­è¯å…ƒå¯¹\n",
    "        ä½œç”¨ï¼šç¡®ä¿å‰æå’Œå‡è®¾çš„æ€»é•¿åº¦ï¼ˆå«ç‰¹æ®Šç¬¦å·ï¼‰ä¸è¶…è¿‡max_len-3ï¼ˆä¿ç•™ç©ºé—´ç»™[CLS]å’Œä¸¤ä¸ª[SEP]ï¼‰\n",
    "        ç­–ç•¥ï¼šä»è¾ƒé•¿ä¸€æ–¹å°¾éƒ¨æˆªæ–­ï¼Œä¿æŒä¸¤è€…ç›¸å¯¹å®Œæ•´\n",
    "        '''\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        '''\n",
    "        3. è·å–å¸¦ç‰¹æ®Šç¬¦å·çš„è¯å…ƒå’Œæ®µè½ID\n",
    "        tokensï¼š\n",
    "            æ ¼å¼ï¼š[CLS]+p_tokens+[SEP]+h_tokens+[SEP]\n",
    "            ç¤ºä¾‹ï¼š['<cls>','a','man','walks','<sep>','a','person','moves','<sep>']\n",
    "        segments *ï¼šåˆ†æ®µæ ‡è®°\n",
    "        å‰æéƒ¨åˆ†ï¼ˆå«CLSå’Œç¬¬ä¸€ä¸ªSEPï¼‰æ ‡è®°ä¸º0\n",
    "        å‡è®¾éƒ¨åˆ†ï¼ˆå«ç¬¬äºŒä¸ªSEPï¼‰æ ‡è®°ä¸º1\n",
    "        ç¤ºä¾‹ï¼š[0,0,0,0,0,1,1,1,1]\n",
    "        '''\n",
    "        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        '''\n",
    "        4. è½¬æ¢ä¸ºè¯å…ƒIDå¹¶å¡«å……\n",
    "        self.vocab[tokens]ï¼šå°†è¯å…ƒåˆ—è¡¨è½¬ä¸ºç´¢å¼•åˆ—è¡¨ã€‚ç¤ºä¾‹ï¼š[101,1037,2153,5231,102,1037,2153,5231,102]\n",
    "        +[self.vocab['<pad>']]*(self.max_len-len(tokens))ï¼š\n",
    "            åœ¨æœ«å°¾å¡«å……<pad>è‡³æœ€å¤§é•¿åº¦ï¼šç¡®ä¿æ‰€æœ‰æ ·æœ¬é•¿åº¦ä¸€è‡´ï¼Œä¾¿äºæ‰¹é‡å¤„ç†\n",
    "        '''\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        # 5. å¡«å……æ®µè½IDï¼šå¡«å……éƒ¨åˆ†ä¹Ÿæ ‡è®°ä¸º0ï¼ˆæˆ–å…¶ä»–å€¼ï¼Œé€šå¸¸ä¸å½±å“ï¼Œå› ä¸ºä¼šè¢«valid_lenå¿½ç•¥ï¼‰\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        # 6. è®°å½•æœ‰æ•ˆé•¿åº¦ï¼š å®é™…è¯å…ƒæ•°ï¼ˆä¸å«å¡«å……ï¼‰ï¼Œç”¨äºåç»­æ³¨æ„åŠ›æ©ç \n",
    "        valid_len = len(tokens)\n",
    "        '''\n",
    "        7. è¿”å›\n",
    "        token_idsï¼šå¡«å……åçš„è¯å…ƒIDåˆ—è¡¨ï¼Œé•¿åº¦=max_len\n",
    "        segmentsï¼šå¡«å……åçš„æ®µè½IDåˆ—è¡¨ï¼Œé•¿åº¦=max_len\n",
    "        valid_lenï¼šæ•´æ•°ï¼Œè¡¨ç¤ºå®é™…è¯å…ƒæ•°é‡\n",
    "        '''\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # ä¸ºBERTè¾“å…¥ä¸­çš„'<CLS>'ã€'<SEP>'å’Œ'<SEP>'è¯å…ƒä¿ç•™ä½ç½®\n",
    "        '''\n",
    "        max_len-3ï¼šä¸ºä¸‰ä¸ªç‰¹æ®Šç¬¦å·é¢„ç•™ä½ç½®\n",
    "        [CLS]ï¼šåºåˆ—å¼€å¤´æ ‡è®°ï¼ˆ1ä¸ªï¼‰\n",
    "        ç¬¬ä¸€ä¸ª[SEP]ï¼šåˆ†éš”å‰æå’Œå‡è®¾ï¼ˆ1ä¸ªï¼‰\n",
    "        ç¬¬äºŒä¸ª[SEP]ï¼šåºåˆ—ç»“å°¾æ ‡è®°ï¼ˆ1ä¸ªï¼‰\n",
    "        ç¤ºä¾‹ï¼šè‹¥max_len=128ï¼Œåˆ™å‰æ+å‡è®¾çš„å®é™…è¯å…ƒæ•°ä¸èƒ½è¶…è¿‡125\n",
    "        '''\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            '''\n",
    "            å¹³è¡¡æˆªæ–­ç­–ç•¥\n",
    "            ç­–ç•¥ï¼šæ€»æ˜¯ä»è¾ƒé•¿çš„å¥å­ä¸­åˆ é™¤è¯å…ƒ\n",
    "            ç›®çš„ï¼šå°½å¯èƒ½ä¿ç•™ä¸¤å¥è¯çš„ ç›¸å¯¹å®Œæ•´ä¿¡æ¯\n",
    "            pop()ï¼šä»åˆ—è¡¨æœ«å°¾åˆ é™¤å…ƒç´ ï¼ˆåˆ é™¤æœ€ä¸é‡è¦çš„å°¾éƒ¨è¯å…ƒï¼‰\n",
    "            '''\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop() # åˆ é™¤è¾ƒé•¿å¥å­çš„æœ€åä¸€ä¸ªè¯å…ƒ\n",
    "            else:\n",
    "                h_tokens.pop() # åˆ é™¤è¾ƒé•¿å¥å­çš„æœ€åä¸€ä¸ªè¯å…ƒ\n",
    "    '''\n",
    "    ç‰¹å¾å…ƒç»„ï¼š\n",
    "        self.all_token_ids[idx]ï¼šæ ·æœ¬çš„è¯å…ƒIDåºåˆ—ï¼ˆå·²å¡«å……è‡³max_lenï¼‰\n",
    "        self.all_segments[idx]ï¼šæ®µè½IDåºåˆ—ï¼ˆåŒºåˆ†å‰æ/å‡è®¾ï¼‰\n",
    "        self.valid_lens[idx]ï¼šæœ‰æ•ˆé•¿åº¦ï¼ˆä¸å«å¡«å……ï¼‰\n",
    "    æ ‡ç­¾ï¼šself.labels[idx]ï¼šç±»åˆ«æ ‡ç­¾ï¼ˆ0/1/2ï¼‰\n",
    "    '''\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx]), self.labels[idx]\n",
    "    '''\n",
    "    ä½œç”¨ï¼š\n",
    "        å½“è°ƒç”¨len(dataset)æ—¶è‡ªåŠ¨è§¦å‘\n",
    "        è¿”å›æ•°æ®é›†ä¸­æ ·æœ¬çš„æ€»æ•°\n",
    "        ä½¿ç”¨self.all_token_idsçš„é•¿åº¦ä½œä¸ºæ ·æœ¬æ•°ï¼ˆå› ä¸ºæ¯ä¸ªæ ·æœ¬å¯¹åº”ä¸€ä¸ªtoken IDåºåˆ—ï¼‰\n",
    "    åœ¨PyTorchä¸­çš„é‡è¦æ€§ï¼š\n",
    "        DataLoader ä¾èµ–æ­¤æ–¹æ³•ç¡®å®šæ•°æ®æ‰¹æ¬¡çš„æ•°é‡å’Œè¾¹ç•Œ\n",
    "        ä¸__getitem__é…åˆï¼Œæ„æˆå®Œæ•´çš„æ•°æ®é›†æ¥å£\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba13f731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:03:08.365140Z",
     "iopub.status.busy": "2023-08-18T07:03:08.364679Z",
     "iopub.status.idle": "2023-08-18T07:04:03.295593Z",
     "shell.execute_reply": "2023-08-18T07:04:03.294685Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    }
   ],
   "source": [
    "# å¦‚æœå‡ºç°æ˜¾å­˜ä¸è¶³é”™è¯¯ï¼Œè¯·å‡å°‘â€œbatch_sizeâ€ã€‚åœ¨åŸå§‹çš„BERTæ¨¡å‹ä¸­ï¼Œmax_len=512\n",
    "'''\n",
    "åŸå§‹æ–‡æœ¬ â†’ read_snli â†’ åˆ†è¯ â†’ SNLIBERTDataset â†’ å¤šè¿›ç¨‹è½¬ID â†’ å¡«å……/æˆªæ–­ â†’ å¼ é‡åŒ– â†’ DataLoader â†’ æ‰¹é‡æ•°æ® â†’ BERTæ¨¡å‹\n",
    "1. è®¾ç½®è¶…å‚æ•°\n",
    "batch_size=512ï¼šæ¯ä¸ªè®­ç»ƒæ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡ï¼ˆè¾ƒå¤§æ‰¹æ¬¡æå‡GPUåˆ©ç”¨ç‡ï¼‰\n",
    "max_len=128ï¼šBERTè¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼ˆå«ç‰¹æ®Šç¬¦å·ï¼‰ï¼Œæˆªæ–­è¶…é•¿å¥å­\n",
    "num_workersï¼šæ•°æ®åŠ è½½è¿›ç¨‹æ•°ï¼Œè‡ªåŠ¨è·å–CPUæ ¸å¿ƒæ•°ä»¥åŠ é€Ÿæ•°æ®è¯»å–\n",
    "'''\n",
    "batch_size, max_len, num_workers = 32, 128, 0\n",
    "# 2. ä¸‹è½½SNLIæ•°æ®é›†\n",
    "data_dir = '..\\\\data\\\\snli_1.0'\n",
    "'''\n",
    "3. åˆ›å»ºè®­ç»ƒé›†å¯¹è±¡\n",
    "d2l.read_snli(data_dir,True)ï¼šè¯»å–è®­ç»ƒé›†åŸå§‹æ•°æ®\n",
    "è¿”å›(premises,hypotheses,labels)å…ƒç»„\n",
    "SNLIBERTDataset(...)ï¼šåˆå§‹åŒ–BERTæ ¼å¼æ•°æ®é›†\n",
    "å¤šå±‚é¢„å¤„ç†ï¼š\n",
    "    åˆ†è¯+å°å†™åŒ–\n",
    "    å¤šè¿›ç¨‹å¹¶è¡Œè½¬IDï¼ˆ4ä¸ªworkerï¼‰\n",
    "    æ·»åŠ ç‰¹æ®Šç¬¦å·ï¼ˆ[CLS]/[SEP]ï¼‰\n",
    "    ç”Ÿæˆæ®µè½ID\n",
    "    å¡«å……/æˆªæ–­è‡³max_len\n",
    "    è®¡ç®—æœ‰æ•ˆé•¿åº¦\n",
    "'''\n",
    "train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n",
    "'''\n",
    "4. åˆ›å»ºæµ‹è¯•é›†å¯¹è±¡\n",
    "    read_snli(...,False)ï¼šè¯»å–æµ‹è¯•é›†\n",
    "    å¤ç”¨vocabï¼šç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„è¯è¡¨\n",
    "    æ— shuffleï¼šæµ‹è¯•é›†ä¸éœ€è¦æ‰“ä¹±\n",
    "'''\n",
    "test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n",
    "'''\n",
    "5. åˆ›å»ºè®­ç»ƒæ•°æ®è¿­ä»£å™¨\n",
    "    DataLoaderï¼šPyTorchæ ‡å‡†æ•°æ®åŠ è½½å™¨\n",
    "    shuffle=Trueï¼šæ¯è½®è®­ç»ƒæ‰“ä¹±æ•°æ®é¡ºåºï¼Œé˜²æ­¢æ¨¡å‹è®°å¿†æ•°æ®æ¨¡å¼\n",
    "    å¤šworkeråŠ è½½ï¼šåœ¨è®­ç»ƒæ—¶å¹¶è¡Œè¯»å–æ•°æ®ï¼Œé¿å…GPUç­‰å¾…\n",
    "'''\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n",
    "                                   num_workers=0)\n",
    "'''\n",
    "6. åˆ›å»ºæµ‹è¯•æ•°æ®è¿­ä»£å™¨\n",
    "    shuffle=Falseï¼šé»˜è®¤ä¸æ‰“ä¹±ï¼Œä¿æŒåŸå§‹é¡ºåºä¾¿äºè¯„ä¼°\n",
    "    ç›¸åŒbatch_sizeï¼šæµ‹è¯•æ—¶æ‰¹é‡å¤§å°å¯ä¸åŒï¼Œä½†é€šå¸¸ä¿æŒä¸€è‡´\n",
    "'''\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                  num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae5ccc",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "## å¾®è°ƒBERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8430e",
   "metadata": {},
   "source": [
    "```python\n",
    "# å‡è®¾ batch_size=32, seq_len=128\n",
    "input: (tokens_X, segments_X, valid_lens_x)\n",
    "       tokens_X: (32, 128)      # [CLS] premise [SEP] hypothesis [SEP] [PAD]...\n",
    "       segments_X: (32, 128)    # 0 0 0 0 0 1 1 1 1 0 0 0...\n",
    "       valid_lens_x: (32,)      # [45, 52, 38, ...]\n",
    "\n",
    "â†“ self.encoder (3å±‚Transformer)\n",
    "encoded_X: (32, 128, 256)      # æ¯ä¸ªä½ç½®çš„ä¸Šä¸‹æ–‡è¡¨ç¤º\n",
    "\n",
    "â†“ æå– [CLS] (ä½ç½®0)\n",
    "cls_rep: (32, 256)              # [CLS]èšåˆäº†æ•´ä¸ªå¥å­çš„è¯­ä¹‰\n",
    "\n",
    "â†“ self.hidden (MLP)\n",
    "hidden_rep: (32, 256)           # ä»»åŠ¡ç‰¹å®šçš„ç‰¹å¾å˜æ¢\n",
    "\n",
    "â†“ self.output (Linear)\n",
    "logits: (32, 3)                 # ä¸‰åˆ†ç±»è¾“å‡º\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e26a41e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:04:03.300395Z",
     "iopub.status.busy": "2023-08-18T07:04:03.299910Z",
     "iopub.status.idle": "2023-08-18T07:04:03.306353Z",
     "shell.execute_reply": "2023-08-18T07:04:03.305313Z"
    },
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    '''\n",
    "    æ¨¡å—å¤ç”¨ç­–ç•¥\n",
    "        self.encoderï¼šå¤ç”¨é¢„è®­ç»ƒBERTçš„2å±‚Transformerç¼–ç å™¨ï¼Œæå–ä¸Šä¸‹æ–‡è¯­ä¹‰è¡¨ç¤º\n",
    "        self.hiddenï¼šå¤ç”¨BERTçš„éšè—å±‚å‰é¦ˆç½‘ç»œï¼ˆé€šå¸¸ç”¨äºMLMä»»åŠ¡ï¼‰ï¼Œä½œä¸ºç‰¹å¾è½¬æ¢å±‚\n",
    "        self.outputï¼šæ–°å¢ä»»åŠ¡ç‰¹å®šè¾“å‡ºå±‚\n",
    "        è¾“å…¥ç»´åº¦ï¼š256ï¼ˆBERTéšè—å±‚ç»´åº¦ï¼‰\n",
    "        è¾“å‡ºç»´åº¦ï¼š3ï¼ˆSNLIä¸‰åˆ†ç±»ï¼šè•´å«/çŸ›ç›¾/ä¸­æ€§ï¼‰\n",
    "    '''\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__() \n",
    "        self.encoder = bert.encoder # BERTçš„Transformerç¼–ç å™¨\n",
    "        self.hidden = bert.hidden # BERTçš„éšè—å±‚ï¼ˆMLMä»»åŠ¡å¤´ï¼‰\n",
    "        self.output = nn.Linear(256, 3) # è‡ªå®šä¹‰è¾“å‡ºå±‚ï¼ˆ3åˆ†ç±»ï¼‰\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        è¾“å…¥è§£åŒ…\n",
    "            inputsï¼šå…ƒç»„(token_ids,segments,valid_lens)\n",
    "            tokens_Xï¼šè¯å…ƒIDå¼ é‡ï¼Œå½¢çŠ¶(batch_size,seq_len)\n",
    "            segments_Xï¼šæ®µè½IDå¼ é‡ï¼ŒåŒºåˆ†å‰æ/å‡è®¾\n",
    "            valid_lens_xï¼šæœ‰æ•ˆé•¿åº¦å¼ é‡ï¼Œç”¨äºæ³¨æ„åŠ›æ©ç \n",
    "        '''\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        '''\n",
    "        ç¼–ç å™¨å¤„ç†\n",
    "            ä½œç”¨ï¼šé€šè¿‡12å±‚Transformerï¼Œç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„è¯å…ƒè¡¨ç¤º\n",
    "            è¾“å‡ºå½¢çŠ¶ï¼š(batch_size,seq_len,hidden_dim)ï¼Œå¦‚(32,128,256)\n",
    "        '''\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        '''\n",
    "        ç‰¹å¾æå–ä¸åˆ†ç±»\n",
    "        æå–[CLS]è¡¨ç¤º\n",
    "            encoded_X[:,0,:]ï¼šé€‰å–æ‰€æœ‰æ ·æœ¬ä¸­ç¬¬0ä¸ªä½ç½®ï¼ˆ[CLS]è¯å…ƒï¼‰çš„å‘é‡\n",
    "            : ï¼šæ‰¹é‡ç»´åº¦ï¼Œé€‰å–æ‰€æœ‰æ ·æœ¬\n",
    "            0 ï¼šåºåˆ—ç»´åº¦ï¼Œå›ºå®šé€‰å–[CLS]ä½ç½®\n",
    "            : ï¼šç‰¹å¾ç»´åº¦ï¼Œé€‰å–å…¨éƒ¨256ç»´\n",
    "            ä¸ºä»€ä¹ˆé€‰[CLS]ï¼Ÿ ï¼šBERTé¢„è®­ç»ƒæ—¶ï¼Œ[CLS]èšåˆäº†æ•´ä¸ªåºåˆ—çš„è¯­ä¹‰ï¼Œæ˜¯åˆ†ç±»ä»»åŠ¡çš„æ ‡å‡†åšæ³•\n",
    "        é€šè¿‡éšè—å±‚\n",
    "            self.hidden(...)ï¼šå°†[CLS]è¡¨ç¤ºé€šè¿‡MLPè¿›è¡Œéçº¿æ€§å˜æ¢\n",
    "            ä½œç”¨ï¼šä»é€šç”¨è¯­ä¹‰ç©ºé—´æ˜ å°„åˆ°ä»»åŠ¡ç‰¹å®šç‰¹å¾ç©ºé—´\n",
    "        æœ€ç»ˆåˆ†ç±»\n",
    "            self.output(...)ï¼šè¾“å‡º3ä¸ªç±»åˆ«çš„logits\n",
    "            ç»“æœå½¢çŠ¶ï¼š(batch_size,3)\n",
    "        '''\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b814689b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:04:03.310896Z",
     "iopub.status.busy": "2023-08-18T07:04:03.310446Z",
     "iopub.status.idle": "2023-08-18T07:04:03.315332Z",
     "shell.execute_reply": "2023-08-18T07:04:03.314376Z"
    },
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "net = BERTClassifier(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185d207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 1000 examples\n",
      "read 200 examples\n",
      "âœ“ ä½¿ç”¨GPUåŠ é€Ÿ\n",
      "âœ“ å¯ç”¨torch.compileåŠ é€Ÿ\n",
      "==================================================\n",
      "å¿«é€ŸéªŒè¯å¼€å§‹ï¼æ•°æ®é‡: 1000, è®¾å¤‡: cuda:0\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== 1. è¶…å‚æ•°è°ƒæ•´ï¼ˆæ¿€è¿›å¿«é€Ÿæ¨¡å¼ï¼‰ =====\n",
    "batch_size, max_len = 16, 64  # æ‰¹é‡å‡åŠï¼Œé•¿åº¦å‡åŠ\n",
    "num_workers = 2  # å¿…é¡»å¼€å¯å¤šè¿›ç¨‹\n",
    "num_epochs = 1   # åªè·‘1è½®ï¼\n",
    "lr = 1e-3        # æé«˜å­¦ä¹ ç‡åŠ é€Ÿæ”¶æ•›ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰\n",
    "\n",
    "# ===== 2. æ•°æ®å­é›†ï¼ˆåªç”¨1000ä¸ªæ ·æœ¬ï¼‰=====\n",
    "def get_subset_dataset(dataset, max_samples=1000):\n",
    "    \"\"\"æå–å‰max_samplesä¸ªæ ·æœ¬ç”¨äºå¿«é€ŸéªŒè¯\"\"\"\n",
    "    premises, hypotheses, labels = dataset\n",
    "    return (premises[:max_samples], \n",
    "            hypotheses[:max_samples], \n",
    "            labels[:max_samples])\n",
    "\n",
    "# åˆ›å»ºè¿·ä½ æ•°æ®é›†\n",
    "mini_train_data = get_subset_dataset(d2l.read_snli(data_dir, True), 1000)\n",
    "mini_test_data = get_subset_dataset(d2l.read_snli(data_dir, False), 200)\n",
    "\n",
    "train_set = SNLIBERTDataset(mini_train_data, max_len, vocab)\n",
    "test_set = SNLIBERTDataset(mini_test_data, max_len, vocab)\n",
    "\n",
    "# ===== 3. åˆ›å»ºDataLoaderï¼ˆä¿æŒå¤šè¿›ç¨‹ï¼‰=====\n",
    "train_iter = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size, shuffle=True, \n",
    "    num_workers=num_workers, pin_memory=True\n",
    ")\n",
    "test_iter = torch.utils.data.DataLoader(\n",
    "    test_set, batch_size, shuffle=False, \n",
    "    num_workers=num_workers, pin_memory=True\n",
    ")\n",
    "\n",
    "# ===== 4. æ¨¡å‹åˆå§‹åŒ–ï¼ˆä½¿ç”¨tinyç‰ˆBERTï¼‰=====\n",
    "try:\n",
    "    # å¦‚æœæœ‰GPUï¼Œå¼ºåˆ¶ä½¿ç”¨GPU\n",
    "    devices = [torch.device('cuda:0')]\n",
    "    print(\"âœ“ ä½¿ç”¨GPUåŠ é€Ÿ\")\n",
    "except:\n",
    "    devices = [torch.device('cpu')]\n",
    "    print(\"âš  ä½¿ç”¨CPUï¼Œå°†éå¸¸æ…¢\")\n",
    "\n",
    "# ä½¿ç”¨BERT-tinyï¼ˆå¦‚æœå¯ç”¨ï¼‰æˆ–å†»ç»“éƒ¨åˆ†å±‚\n",
    "# net = d2l.BERTClassifier(...)  # æ‚¨çš„æ¨¡å‹å®šä¹‰\n",
    "\n",
    "# ===== 5. è®­ç»ƒï¼ˆå¯ç”¨æ··åˆç²¾åº¦åŠ é€Ÿï¼‰=====\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# PyTorch 2.0+ å¯ç”¨torch.compileåŠ é€Ÿ\n",
    "if hasattr(torch, 'compile'):\n",
    "    net = torch.compile(net)\n",
    "    print(\"âœ“ å¯ç”¨torch.compileåŠ é€Ÿ\")\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "print(\"=\"*50)\n",
    "print(f\"å¿«é€ŸéªŒè¯å¼€å§‹ï¼æ•°æ®é‡: {len(train_set)}, è®¾å¤‡: {devices[0]}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76b46a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ å¼€å§‹åŠ è½½æ•°æ®...\n",
      "âœ“ æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶ 15.22 ç§’\n",
      "è®­ç»ƒé›†: 50æ¡, æµ‹è¯•é›†: 20æ¡\n",
      "\n",
      "ğŸ”„ å¼€å§‹é¢„å¤„ç†ï¼ˆå•è¿›ç¨‹æ¨¡å¼ï¼Œå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰...\n",
      "   æ­£åœ¨å¤„ç† 50 ä¸ªæ ·æœ¬...\n",
      "   å·²å¤„ç† 10/50\n",
      "   å·²å¤„ç† 20/50\n",
      "   å·²å¤„ç† 30/50\n",
      "   å·²å¤„ç† 40/50\n",
      "   å·²å¤„ç† 50/50\n",
      "   âœ“ é¢„å¤„ç†å®Œæˆ\n",
      "read 50 examples\n",
      "   æ­£åœ¨å¤„ç† 20 ä¸ªæ ·æœ¬...\n",
      "   å·²å¤„ç† 10/20\n",
      "   å·²å¤„ç† 20/20\n",
      "   âœ“ é¢„å¤„ç†å®Œæˆ\n",
      "read 20 examples\n",
      "âœ“ é¢„å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶ 0.00 ç§’\n",
      "\n",
      "âœ“ DataLoaderåˆ›å»ºæˆåŠŸ\n",
      "è®­ç»ƒæ‰¹æ¬¡æ•°é‡: 7, æµ‹è¯•æ‰¹æ¬¡æ•°é‡: 3\n",
      "\n",
      "ğŸ”„ åˆå§‹åŒ–æ¨¡å‹...\n",
      "âœ“ æ£€æµ‹åˆ°GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "  æ˜¾å­˜: 8.0GB\n",
      "  æ€»å‚æ•°é‡: 16,613,635, å¯è®­ç»ƒ: 16,613,635\n",
      "\n",
      "ğŸ” æ‰§è¡Œå‰å‘ä¼ æ’­æµ‹è¯•...\n",
      "  è¾“å…¥å½¢çŠ¶: token_ids=torch.Size([8, 32]), segments=torch.Size([8, 32])\n",
      "  æ ‡ç­¾å½¢çŠ¶: torch.Size([8])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 121\u001b[0m\n\u001b[0;32m    119\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 121\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  è¾“å‡ºå½¢çŠ¶: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  é¢„æœŸå½¢çŠ¶: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\DL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# ==================== ç¬¬0æ­¥ï¼šWindowsç¯å¢ƒå¼ºåˆ¶è®¾ç½® ====================\n",
    "# å¿…é¡»åœ¨æ‰€æœ‰å¯¼å…¥ä¹‹åã€ä»»ä½•ä»£ç ä¹‹å‰æ‰§è¡Œ\n",
    "if __name__ == '__main__':  # â† è¿™è¡Œå¿…é¡»æœ‰ï¼\n",
    "    multiprocessing.freeze_support()\n",
    "    \n",
    "    # ==================== ç¬¬1æ­¥ï¼šè¶…å‚æ•°ï¼ˆæé™å‹ç¼©ï¼‰ ====================\n",
    "    batch_size, max_len = 8, 32  # è¿›ä¸€æ­¥å‹ç¼©ï¼š8Ã—32 = æ¯”åŸæ¥å¿«10å€\n",
    "    num_epochs = 1\n",
    "    lr = 1e-3\n",
    "    \n",
    "    # ==================== ç¬¬2æ­¥ï¼šæ•°æ®å­é›†ï¼ˆä»…50æ¡ï¼ï¼‰ ====================\n",
    "    def get_tiny_dataset(dataset, n=50):\n",
    "        \"\"\"æè‡´å‹ç¼©ï¼šåªç”¨50æ¡æ ·æœ¬ï¼Œ30ç§’å†…å®Œæˆ\"\"\"\n",
    "        premises, hypotheses, labels = dataset\n",
    "        return (premises[:n], hypotheses[:n], labels[:n])\n",
    "    \n",
    "    print(\"ğŸ”„ å¼€å§‹åŠ è½½æ•°æ®...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # è¯»å–å®Œæ•´æ•°æ®é›†ç»“æ„ï¼ˆä½†åªå–å‰50æ¡ï¼‰\n",
    "    full_train = d2l.read_snli(data_dir, True)\n",
    "    full_test = d2l.read_snli(data_dir, False)\n",
    "    \n",
    "    # åˆ›å»ºè¶…è¿·ä½ æ•°æ®é›†\n",
    "    tiny_train = get_tiny_dataset(full_train, 50)\n",
    "    tiny_test = get_tiny_dataset(full_test, 20)\n",
    "    \n",
    "    print(f\"âœ“ æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f} ç§’\")\n",
    "    print(f\"è®­ç»ƒé›†: {len(tiny_train[0])}æ¡, æµ‹è¯•é›†: {len(tiny_test[0])}æ¡\")\n",
    "    \n",
    "    # ==================== ç¬¬3æ­¥ï¼šå•è¿›ç¨‹å®‰å…¨æ¨¡å¼ ====================\n",
    "    print(\"\\nğŸ”„ å¼€å§‹é¢„å¤„ç†ï¼ˆå•è¿›ç¨‹æ¨¡å¼ï¼Œå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰...\")\n",
    "    \n",
    "    # ä¿®æ”¹SNLIBERTDatasetçš„_preprocessï¼ˆä¸´æ—¶è¡¥ä¸ï¼‰\n",
    "    original_preprocess = SNLIBERTDataset._preprocess\n",
    "    \n",
    "    def safe_preprocess(self, tokens_list):\n",
    "        print(f\"   æ­£åœ¨å¤„ç† {len(tokens_list)} ä¸ªæ ·æœ¬...\")\n",
    "        out = []\n",
    "        for i, tokens in enumerate(tokens_list):\n",
    "            out.append(self._mp_worker(tokens))\n",
    "            # æ¯10ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"   å·²å¤„ç† {i+1}/{len(tokens_list)}\")\n",
    "        print(\"   âœ“ é¢„å¤„ç†å®Œæˆ\")\n",
    "        \n",
    "        all_token_ids = [t for t, s, v in out]\n",
    "        all_segments = [s for t, s, v in out]\n",
    "        valid_lens = [v for t, s, v in out]\n",
    "        \n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long),\n",
    "                torch.tensor(valid_lens))\n",
    "    \n",
    "    # ä¸´æ—¶æ›¿æ¢æ–¹æ³•\n",
    "    SNLIBERTDataset._preprocess = safe_preprocess\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_set = SNLIBERTDataset(tiny_train, max_len, vocab)\n",
    "    test_set = SNLIBERTDataset(tiny_test, max_len, vocab)\n",
    "    print(f\"âœ“ é¢„å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶ {time.time() - start_time:.2f} ç§’\")\n",
    "    \n",
    "    # ==================== ç¬¬4æ­¥ï¼šDataLoaderï¼ˆnum_workers=0ï¼ï¼‰ ====================\n",
    "    # å…³é”®ï¼šWindowsä¸‹å¿…é¡»è®¾ä¸º0ï¼Œå¦åˆ™å¡æ­»\n",
    "    train_iter = DataLoader(train_set, batch_size=batch_size, \n",
    "                           shuffle=True, num_workers=0, pin_memory=True)\n",
    "    test_iter = DataLoader(test_set, batch_size=batch_size, \n",
    "                          shuffle=False, num_workers=0, pin_memory=True)\n",
    "    \n",
    "    print(f\"\\nâœ“ DataLoaderåˆ›å»ºæˆåŠŸ\")\n",
    "    print(f\"è®­ç»ƒæ‰¹æ¬¡æ•°é‡: {len(train_iter)}, æµ‹è¯•æ‰¹æ¬¡æ•°é‡: {len(test_iter)}\")\n",
    "    \n",
    "    # ==================== ç¬¬5æ­¥ï¼šæ¨¡å‹ä¸è®¾å¤‡æ£€æŸ¥ ====================\n",
    "    print(\"\\nğŸ”„ åˆå§‹åŒ–æ¨¡å‹...\")\n",
    "    \n",
    "    # æ£€æŸ¥è®¾å¤‡\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(f\"âœ“ æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"âš  æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒï¼ˆä¼šéå¸¸æ…¢ï¼‰\")\n",
    "    \n",
    "    devices = [device]\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå‡è®¾netå·²å®šä¹‰ï¼‰\n",
    "    # net = d2l.BERTClassifier(...)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    # ç»Ÿè®¡å‚æ•°é‡\n",
    "    total_params = sum(p.numel() for p in net.parameters())\n",
    "    trainable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    print(f\"  æ€»å‚æ•°é‡: {total_params:,}, å¯è®­ç»ƒ: {trainable_params:,}\")\n",
    "    \n",
    "    # ==================== ç¬¬6æ­¥ï¼šè®­ç»ƒå‰éªŒè¯ï¼ˆå…³é”®ï¼ï¼‰ ====================\n",
    "    print(\"\\nğŸ” æ‰§è¡Œå‰å‘ä¼ æ’­æµ‹è¯•...\")\n",
    "    \n",
    "    # å–ä¸€ä¸ªbatchæµ‹è¯•\n",
    "    batch = next(iter(train_iter))\n",
    "    (token_ids, segments, valid_lens), labels = batch\n",
    "    \n",
    "    # ç§»åŠ¨åˆ°è®¾å¤‡\n",
    "    token_ids = token_ids.to(device)\n",
    "    segments = segments.to(device)\n",
    "    valid_lens = valid_lens.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    print(f\"  è¾“å…¥å½¢çŠ¶: token_ids={token_ids.shape}, segments={segments.shape}\")\n",
    "    print(f\"  æ ‡ç­¾å½¢çŠ¶: {labels.shape}\")\n",
    "    \n",
    "    # å‰å‘ä¼ æ’­\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        output = net(token_ids, segments, valid_lens)\n",
    "    \n",
    "    print(f\"  è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "    print(f\"  é¢„æœŸå½¢çŠ¶: {labels.shape}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦åŒ¹é…\n",
    "    assert output.shape[0] == labels.shape[0], \"æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…ï¼\"\n",
    "    assert output.shape[1] == 3, \"è¾“å‡ºç±»åˆ«æ•°ä¸æ˜¯3ï¼\"\n",
    "    print(\"âœ“ å‰å‘ä¼ æ’­æµ‹è¯•é€šè¿‡\")\n",
    "    \n",
    "    # ==================== ç¬¬7æ­¥ï¼šè®­ç»ƒä¸€ä¸ªbatchæµ‹è¯• ====================\n",
    "    print(\"\\nğŸ” æ‰§è¡Œå•æ‰¹æ¬¡è®­ç»ƒæµ‹è¯•...\")\n",
    "    \n",
    "    net.train()\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # è®­ç»ƒä¸€ä¸ªbatch\n",
    "    trainer.zero_grad()\n",
    "    output = net(token_ids, segments, valid_lens)\n",
    "    loss_val = loss_fn(output, labels)\n",
    "    \n",
    "    print(f\"  æŸå¤±å€¼: {loss_val.item():.4f}\")\n",
    "    \n",
    "    loss_val.backward()\n",
    "    trainer.step()\n",
    "    \n",
    "    # æ£€æŸ¥æ¢¯åº¦\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "    print(f\"  æ¢¯åº¦èŒƒæ•°: {grad_norm:.4f}\")\n",
    "    print(\"âœ“ å•æ‰¹æ¬¡è®­ç»ƒæµ‹è¯•é€šè¿‡\")\n",
    "    \n",
    "    # ==================== ç¬¬8æ­¥ï¼šå®Œæ•´è®­ç»ƒ ====================\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸš€ å¼€å§‹å®Œæ•´å¿«é€Ÿè®­ç»ƒï¼ˆ1 epochï¼‰\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # ä½¿ç”¨d2lçš„train_ch13\n",
    "    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\n",
    "    \n",
    "    print(f\"\\nâœ… å¿«é€ŸéªŒè¯å®Œæˆï¼æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’\")\n",
    "    \n",
    "    # ==================== ç¬¬9æ­¥ï¼šéªŒè¯ç»“æœ ====================\n",
    "    print(\"\\nğŸ“Š ç»“æœéªŒè¯:\")\n",
    "    # è®¡ç®—å‡†ç¡®ç‡\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            (token_ids, segments, valid_lens), labels = batch\n",
    "            token_ids = token_ids.to(device)\n",
    "            segments = segments.to(device)\n",
    "            valid_lens = valid_lens.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = net(token_ids, segments, valid_lens)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"  æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2%} (éšæœºåŸºçº¿: 33.33%)\")\n",
    "    \n",
    "    if accuracy > 0.4:\n",
    "        print(\"  ğŸ‰ æ¨¡å‹æ­£åœ¨å­¦ä¹ ï¼å¿«é€ŸéªŒè¯é€šè¿‡\")\n",
    "    else:\n",
    "        print(\"  âš   å‡†ç¡®ç‡åä½ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98d38fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:04:03.320237Z",
     "iopub.status.busy": "2023-08-18T07:04:03.319363Z",
     "iopub.status.idle": "2023-08-18T07:08:57.319142Z",
     "shell.execute_reply": "2023-08-18T07:08:57.318209Z"
    },
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "lr, num_epochs = 1e-4, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
    "    devices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
