{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c5cd33",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 自然语言推断与数据集\n",
    "## 自然语言推断\n",
    "## 斯坦福自然语言推断（SNLI）数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc002fe3",
   "metadata": {},
   "source": [
    "## 报错\n",
    "### 数据集下载失败\n",
    "手动下载数据集，由于斯坦福SNLI数据集有macOS文件，在windows平台无法运行，手动在压缩包文件删除（Icon文件），再解压。然后手动更改data_dir路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ccbfd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:06:00.201212Z",
     "iopub.status.busy": "2023-08-18T07:06:00.200144Z",
     "iopub.status.idle": "2023-08-18T07:06:09.370822Z",
     "shell.execute_reply": "2023-08-18T07:06:09.368591Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "#@save\n",
    "# d2l.DATA_HUB['SNLI'] = (\n",
    "#     'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
    "#     '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "# d2l.DATA_HUB['SNLI'] = (\n",
    "#     'https://mirrors.tuna.tsinghua.edu.cn/snli/snli_1.0.zip',\n",
    "#     '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "# data_dir = d2l.download_extract('SNLI')\n",
    "# 1. 手动下载并解压\n",
    "\n",
    "extract_dir = '..\\\\data\\\\snli_1.0'\n",
    "\n",
    "# 3. 直接使用\n",
    "data_dir = extract_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02956fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理 data/snli_1.0 目录中可能包含的无效文件（比如来自 macOS 的 ``Icon\\r``）\n",
    "# 这个单元避免后续的文件操作因为非法文件名而失败\n",
    "import os\n",
    "\n",
    "# Helper: 清理可能导致 Windows 报错的无效文件名\n",
    "# 定义为函数以便在多处复用（例如在调用 read_snli/load_data_snli 前）\n",
    "def _cleanup_snli_dir(data_dir):\n",
    "    \"\"\"删除 data_dir 下包含控制字符或以 Icon 开头的文件并返回已删除路径列表\"\"\"\n",
    "    import os\n",
    "    if not os.path.isdir(data_dir):\n",
    "        return []\n",
    "    removed = []\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for name in files:\n",
    "            if '\\r' in name or '\\x00' in name or name.startswith('Icon'):\n",
    "                path = os.path.join(root, name)\n",
    "                try:\n",
    "                    os.remove(path)\n",
    "                    removed.append(path)\n",
    "                except Exception as e:\n",
    "                    print(f'无法删除 {path}:', e)\n",
    "    if removed:\n",
    "        print('已删除以下无效文件：')\n",
    "        for p in removed:\n",
    "            print(' -', p)\n",
    "    return removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e647396",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "### [**读取数据集**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa839f80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:06:09.377922Z",
     "iopub.status.busy": "2023-08-18T07:06:09.377380Z",
     "iopub.status.idle": "2023-08-18T07:06:09.392203Z",
     "shell.execute_reply": "2023-08-18T07:06:09.390984Z"
    },
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# data_dir：数据集目录路径；is_train：True读取训练集，False读取测试集\n",
    "# 返回：(premises,hypotheses,labels)三个列表\n",
    "#@save\n",
    "def read_snli(data_dir=None, is_train=True):\n",
    "    \"\"\"将SNLI数据集解析为前提、假设和标签\n",
    "\n",
    "    如果未提供 data_dir，会尝试使用 `extract_dir` 全局变量；若不存在则调用 `d2l.download_extract('SNLI')`。\n",
    "    函数会在打开 TSV 文件前尝试清理目录中的无效文件（如包含 '\\r' 的文件名），并在打开失败时重试一次。\n",
    "    \"\"\"\n",
    "    # 1. 文本清洗函数：清理文本中的括号和冗余空格\n",
    "    def extract_text(s):\n",
    "        # 删除我们不会使用的信息\n",
    "        s = re.sub('\\\\(', '', s) # 删左括号\n",
    "        s = re.sub('\\\\)', '', s) # 删右括号\n",
    "        # 用一个空格替换两个或多个连续的空格\n",
    "        s = re.sub('\\\\s{2,}', ' ', s) # 两个以上空格合并为一个\n",
    "        return s.strip()\n",
    "    \n",
    "    '''\n",
    "    2. 标签映射：将文本标签转为整数\n",
    "    'entailment'（蕴含）→0\n",
    "    'contradiction'（矛盾）→1\n",
    "    'neutral'（中性）→2\n",
    "    SNLI中有些样本标记为'-'（问题样本），将被过滤掉\n",
    "    '''\n",
    "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    '''\n",
    "    3. 文件读取与解析\n",
    "    选择文件：根据is_train选择训练集或测试集\n",
    "    读取TSV：按制表符分割每行，readlines()[1:]跳过CSV表头\n",
    "    结果：rows是一个二维列表，每行是一个字段列表\n",
    "    '''\n",
    "    # 确保 data_dir 可用\n",
    "    if data_dir is None:\n",
    "        try:\n",
    "            data_dir = extract_dir\n",
    "        except NameError:\n",
    "            data_dir = d2l.download_extract('SNLI')\n",
    "    # 尝试清理目录以删除可能导致 Windows 报错的文件\n",
    "    try:\n",
    "        _cleanup_snli_dir(data_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt' if is_train else 'snli_1.0_test.txt')\n",
    "    try:\n",
    "        with open(file_name, 'r') as f:\n",
    "            rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "    except Exception as e:\n",
    "        # 若因目录中存在无效文件名导致打开失败，先尝试清理目录后重试\n",
    "        print(f'打开 {file_name} 失败: {e}. 尝试清理目录并重试。')\n",
    "        try:\n",
    "            _cleanup_snli_dir(data_dir)\n",
    "            with open(file_name, 'r') as f:\n",
    "                rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(f'无法打开文件 {file_name}，请检查 SNLI 数据集是否正确解压到 {data_dir}，或手动删除目录中异常文件。 原始错误: {e2}')\n",
    "    '''\n",
    "    4. 数据提取与过滤\n",
    "    row[0]：标签列（如'entailment'），不在label_set中的行被跳过\n",
    "    row[1]：前提（premise）句子\n",
    "    row[2]：假设（hypothesis）句子\n",
    "    列表推导式：只保留标签有效的样本，确保数据质量\n",
    "    '''\n",
    "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
    "    hypotheses = [extract_text(row[2]) for row in rows if row[0] \\\n",
    "                in label_set]\n",
    "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
    "    '''\n",
    "    premises：字符串列表，如['A man inspects...',...]\n",
    "    hypotheses：字符串列表，如['An activity is being...',...]\n",
    "    labels：整数列表，如[0,1,2,0,...]\n",
    "    '''\n",
    "    return premises, hypotheses, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a64fd",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "现在让我们[**打印前3对**]前提和假设，以及它们的标签（“0”“1”和“2”分别对应于“蕴涵”“矛盾”和“中性”）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19101f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:06:09.397297Z",
     "iopub.status.busy": "2023-08-18T07:06:09.396407Z",
     "iopub.status.idle": "2023-08-18T07:06:23.206512Z",
     "shell.execute_reply": "2023-08-18T07:06:23.205574Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is training his horse for a competition .\n",
      "标签： 2\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is at a diner , ordering an omelette .\n",
      "标签： 1\n",
      "前提： A person on a horse jumps over a broken down airplane .\n",
      "假设： A person is outdoors , on a horse .\n",
      "标签： 0\n"
     ]
    }
   ],
   "source": [
    "# rain_data是一个元组(premises,hypotheses,labels)，打乱顺序\n",
    "train_data = read_snli(data_dir, is_train=True)\n",
    "'''\n",
    "train_data[0][:3]：取前3个前提（premises）\n",
    "train_data[1][:3]：取前3个假设（hypotheses）\n",
    "train_data[2][:3]：取前3个标签（labels）\n",
    "zip(...)：将三个列表的对应元素成对打包，生成可迭代的三元组\n",
    "for x0, x1, y in ...：每次迭代解包一个样本的三个部分\n",
    "'''\n",
    "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
    "    print('前提：', x0)\n",
    "    print('假设：', x1)\n",
    "    print('标签：', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09b2cf4",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "训练集约有550000对，测试集约有10000对。下面显示了训练集和测试集中的三个[**标签“蕴涵”“矛盾”和“中性”是平衡的**]。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "972ca3d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:06:23.210300Z",
     "iopub.status.busy": "2023-08-18T07:06:23.209728Z",
     "iopub.status.idle": "2023-08-18T07:06:23.531128Z",
     "shell.execute_reply": "2023-08-18T07:06:23.530246Z"
    },
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[183416, 183187, 182764]\n",
      "[3368, 3237, 3219]\n"
     ]
    }
   ],
   "source": [
    "# test_data是 (premises,hypotheses,labels)元组，不打乱顺序\n",
    "test_data = read_snli(data_dir, is_train=False)\n",
    "# data[2]是标签列表（labels）,依次统计训练集和测试集\n",
    "for data in [train_data, test_data]:\n",
    "    # for i in range(3)：i取值0,1,2（对应三种标签）\n",
    "    # .count(i)：统计标签i出现的次数\n",
    "    # [data[2].count(0),data[2].count(1),data[2].count(2)]\n",
    "    print([[row for row in data[2]].count(i) for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab2708",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "### [**定义用于加载数据集的类**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8b15f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:06:23.534933Z",
     "iopub.status.busy": "2023-08-18T07:06:23.534365Z",
     "iopub.status.idle": "2023-08-18T07:06:23.542550Z",
     "shell.execute_reply": "2023-08-18T07:06:23.541714Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"用于加载SNLI数据集的自定义数据集\"\"\"\n",
    "    '''\n",
    "    dataset：元组 (premises,hypotheses,labels)，来自read_snli()的输出\n",
    "    num_steps：序列最大长度（填充/截断的目标长度）\n",
    "    vocab：可选的预构建词汇表（用于共享训练/测试集词表）\n",
    "    '''\n",
    "    def __init__(self, dataset, num_steps, vocab=None):\n",
    "        # 1. 设置序列长度：保存最大序列长度，供后续填充/截断使用\n",
    "        self.num_steps = num_steps\n",
    "        '''\n",
    "        2. 分词处理\n",
    "        dataset[0]：前提（premise）字符串列表\n",
    "        dataset[1]：假设（hypothesis）字符串列表\n",
    "        tokenize：将每个句子字符串拆分为单词列表（如['a','man','is',...]）\n",
    "        '''\n",
    "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
    "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
    "        '''\n",
    "        3. 构建或复用词汇表\n",
    "        首次调用（如训练集）：合并所有前提和假设的词元，构建新词表\n",
    "            min_freq=5：过滤低频词（出现<5次的词映射为<unk>）\n",
    "            reserved_tokens=['<pad>']：添加填充符\n",
    "        后续调用（如测试集）：复用训练集词表，确保索引一致\n",
    "        '''\n",
    "        if vocab is None:\n",
    "            self.vocab = d2l.Vocab(all_premise_tokens + \\\n",
    "                all_hypothesis_tokens, min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        '''\n",
    "        4. 序列填充/截断\n",
    "        _pad方法（未显示但可推断）：\n",
    "            将每个词元列表转换为索引序列（通过vocab）\n",
    "            长度>num_steps：截断尾部\n",
    "            长度<num_steps：用<pad>索引填充\n",
    "        结果：self.premises和self.hypotheses是形状为(样本数,num_steps)的张量列表\n",
    "        '''\n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        # 5. 标签张量化：dataset[2]：原始整数标签列表（0/1/2），转换为PyTorch张量，便于后续训练和计算损失\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        # 6. 打印统计信息\n",
    "        print('read ' + str(len(self.premises)) + ' examples')\n",
    "    '''\n",
    "    1. 输入：lines\n",
    "    类型：词元列表的列表（nested list）\n",
    "    示例：[['a','man','is','walking'],['the','dog','barks'],...]；每个子列表代表一个句子的单词分词结果\n",
    "    2. 词元→索引转换：self.vocab[line]\n",
    "    调用词汇表将每个词元字符串映射为整数索引；示例：['a','man','is']→[12,85,7]\n",
    "    OOV处理：不在词表中的词映射为<unk>的索引\n",
    "    3. 截断与填充：d2l.truncate_pad(...,self.num_steps,self.vocab['<pad>'])\n",
    "    self.num_steps：目标固定长度（如50）\n",
    "    self.vocab['<pad>']：填充符的索引（通常是0）\n",
    "    逻辑：\n",
    "        序列长度>num_steps：截断尾部，只保留前num_steps个词\n",
    "        序列长度<num_steps：在末尾补填充符，直到长度为num_steps\n",
    "    4. 转换为张量：torch.tensor([... for line in lines])\n",
    "    列表推导式遍历所有句子\n",
    "    最终返回二维张量，形状：(句子数量,num_steps)\n",
    "    '''\n",
    "    def _pad(self, lines):\n",
    "        return torch.tensor([d2l.truncate_pad(\n",
    "            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n",
    "                         for line in lines])\n",
    "    # 特殊方法：当使用dataset[i]访问元素时自动调用\n",
    "    # idx：要获取的样本索引（整数，如0,1,2...）\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        返回一个嵌套元组：((premise,hypothesis),label)\n",
    "        self.premises[idx]：第idx个前提句子的张量\n",
    "            形状：(num_steps,)，如tensor([12,85,7,33,0,0,...])\n",
    "        self.hypotheses[idx]：第idx个假设句子的张量\n",
    "            形状：(num_steps,)，与前提格式相同\n",
    "        self.labels[idx]：第idx个样本的标签\n",
    "            类型：torch.tensor(0)或tensor(1)或tensor(2)\n",
    "            对应关系：0=蕴含(entailment),1=矛盾(contradiction),2=中性(neutral)\n",
    "        '''\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "    # 数据集的总样本数\n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5efd5df",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "### [**整合代码**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96c46f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:06:23.546033Z",
     "iopub.status.busy": "2023-08-18T07:06:23.545509Z",
     "iopub.status.idle": "2023-08-18T07:06:23.551107Z",
     "shell.execute_reply": "2023-08-18T07:06:23.550286Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# batch_size：每个训练批次的样本数量\n",
    "# num_steps：序列最大长度（默认50），用于填充/截断句子\n",
    "\n",
    "#@save\n",
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    \"\"\"下载SNLI数据集并返回数据迭代器和词表\"\"\"\n",
    "    # 1. 设置多进程加载\n",
    "    # num_workers = d2l.get_dataloader_workers()\n",
    "    num_workers = 0\n",
    "    # 2. 下载数据集\n",
    "    # data_dir = d2l.download_extract('SNLI')\n",
    "    data_dir = '..\\\\data\\\\snli_1.0'\n",
    "    # 3. 读取原始数据\n",
    "    train_data = read_snli(data_dir, True) # 训练集，(premises,hypotheses,labels)元组\n",
    "    test_data = read_snli(data_dir, False) # 测试集，(premises,hypotheses,labels)元组\n",
    "    # 4. 创建数据集对象\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    '''\n",
    "    5. 创建数据迭代器\n",
    "    train_iter：shuffle=True：打乱数据顺序，确保每轮训练顺序不同，用于模型训练\n",
    "    test_iter：shuffle=False：保持原始顺序，便于复现评估结果，用于模型验证/测试\n",
    "    '''\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    '''\n",
    "    train_iter：训练数据迭代器\n",
    "    test_iter：测试数据迭代器\n",
    "    train_set.vocab：词汇表对象（后续可用于词向量加载或解码）\n",
    "    '''\n",
    "    return train_iter, test_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08d0c755",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:06:23.554839Z",
     "iopub.status.busy": "2023-08-18T07:06:23.554288Z",
     "iopub.status.idle": "2023-08-18T07:07:02.488484Z",
     "shell.execute_reply": "2023-08-18T07:07:02.487658Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 549367 examples\n",
      "read 9824 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18678"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. 数据加载（耗时操作）\n",
    "    自动下载：首次执行会从网络下载SNLI数据集（约900MB）\n",
    "    预处理：读取原始TSV文件，清洗文本，构建词汇表\n",
    "    批量化：创建两个DataLoader，批量大小为128，序列长度截断/填充为50\n",
    "2. 返回值\n",
    "    train_iter：训练集迭代器（约55万样本，4300个批次/轮）\n",
    "    test_iter：测试集迭代器（约1万样本，77个批次）\n",
    "    vocab：词汇表对象\n",
    "'''\n",
    "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7411a33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:02.492220Z",
     "iopub.status.busy": "2023-08-18T07:07:02.491909Z",
     "iopub.status.idle": "2023-08-18T07:07:02.966465Z",
     "shell.execute_reply": "2023-08-18T07:07:02.965137Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 50])\n",
      "torch.Size([128, 50])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. 循环取值：for X, Y in train_iter\n",
    "    train_iter：DataLoader创建的训练数据迭代器\n",
    "    每个批次返回：(X, Y)，其中X是特征，Y是标签\n",
    "2. X的结构\n",
    "    在SNLI数据集中，X是一个元组，包含两个元素：\n",
    "    X[0]：前提（premise）张量，形状(batch_size,num_steps)\n",
    "    X[1]：假设（hypothesis）张量，形状(batch_size,num_steps)\n",
    "'''\n",
    "for X, Y in train_iter:\n",
    "    print(X[0].shape)\n",
    "    print(X[1].shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
