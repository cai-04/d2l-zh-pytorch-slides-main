{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f498e022",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 来自Transformers的双向编码器表示（BERT）\n",
    "## 从上下文无关到上下文敏感\n",
    "## 从特定于任务到不可知任务\n",
    "## BERT：把两个最好的结合起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6042930c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:04.580152Z",
     "iopub.status.busy": "2023-08-18T06:58:04.579563Z",
     "iopub.status.idle": "2023-08-18T06:58:06.551921Z",
     "shell.execute_reply": "2023-08-18T06:58:06.551014Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3092258",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## 输入表示\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5d0098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.556248Z",
     "iopub.status.busy": "2023-08-18T06:58:06.555588Z",
     "iopub.status.idle": "2023-08-18T06:58:06.561006Z",
     "shell.execute_reply": "2023-08-18T06:58:06.560200Z"
    },
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "输入：两个词元列表（tokens_a和可选的tokens_b）\n",
    "输出：添加特殊标记后的词元序列和片段标记序列\n",
    "'''\n",
    "#@save\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"获取输入序列的词元及其片段索引\"\"\"\n",
    "    '''\n",
    "    1. 构建基础序列\n",
    "    '<cls>'：在序列开头添加分类标记（Class Token），BERT用它表示整个句子的语义\n",
    "    tokens_a：第一个句子/段落的词元列表\n",
    "    '<sep>'：在句子A末尾添加分隔标记（Separator Token），标记句子A结束\n",
    "    '''\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0和1分别标记片段A和B\n",
    "    '''\n",
    "    2. 构建片段标记（Segment A）\n",
    "    为序列中所有属于句子A的词元分配片段ID 0\n",
    "    len(tokens_a)+2：包含 <cls>、tokens_a 的所有词和<sep>\n",
    "    结果：[0,0,0,...,0]（长度=len(tokens_a)+2）\n",
    "    '''\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    # 3. 处理句子B：如果提供了tokens_b\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>'] # 将句子B的词元和末尾的<sep>追加到序列\n",
    "        segments += [1] * (len(tokens_b) + 1) # 为句子B的词元和最后的<sep>分配片段ID 1\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99b9b5",
   "metadata": {},
   "source": [
    "| 组件                      | 作用             | 关键技术            |\n",
    "| ----------------------- | -------------- | --------------- |\n",
    "| **`token_embedding`**   | 词元 → 向量        | `nn.Embedding`  |\n",
    "| **`segment_embedding`** | 句子标记 → 向量      | 区分两个句子          |\n",
    "| **`pos_embedding`**     | 位置信息           | 可学习的参数          |\n",
    "| **`blks`**              | Transformer层堆叠 | `num_layers`次循环 |\n",
    "| **相加操作**                | 融合三种信息         | 逐元素相加           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e58a2d5",
   "metadata": {},
   "source": [
    "**在 blk(X, valid_lens) 内部的作用**\n",
    "\n",
    "每个 EncoderBlock 都包含多头注意力层，valid_lens 用于：\n",
    "\n",
    "1. 生成注意力掩码\n",
    "```Python\n",
    "# 在 EncoderBlock 内部\n",
    "def forward(self, X, valid_lens):\n",
    "    # X 形状: (batch_size, seq_len, num_hiddens)\n",
    "    # valid_lens 形状: (batch_size,)\n",
    "    \n",
    "    # 根据 valid_lens 生成掩码\n",
    "    # 例如 valid_lens=[2, 3]，seq_len=4\n",
    "    # 掩码: [[0, 0, 1, 1], [0, 0, 0, 1]]\n",
    "    # (0=有效, 1=填充)\n",
    "    \n",
    "    # 在注意力计算中，填充位置的注意力分数设为 -inf\n",
    "    # Softmax 后变为 0，模型不关注\n",
    "    attn_output = self.attention(X, X, X, valid_lens)\n",
    "```\n",
    "\n",
    "2. 具体实现细节\n",
    "```Python\n",
    "# 在 MultiHeadAttention 中\n",
    "def sequence_mask(self, valid_lens):\n",
    "    # valid_lens: [2, 3]\n",
    "    # 返回 shape: (batch_size, seq_len)\n",
    "    # [[False, False,  True,  True],   # 第1个样本：后2个位置是填充\n",
    "    #  [False, False, False,  True]]   # 第2个样本：最后1个位置是填充\n",
    "    \n",
    "    # 扩展为注意力分数形状 (batch_size, num_heads, seq_len, seq_len)\n",
    "    mask = self.expand_mask(mask)\n",
    "    return mask\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9105a8d0",
   "metadata": {},
   "source": [
    "| 参数                          | 切片对象             | 作用          | 结果                                      |\n",
    "| --------------------------- | ---------------- | ----------- | --------------------------------------- |\n",
    "|  参数1 `:`            | 批量维度   | 选取所有（大小为1）  | 保持 `(1, ...)`                           |\n",
    "|  参数2 `:X.shape[1]`  | 序列长度维度 | 动态截取到当前序列长度 | 从 `(..., 1000, ...)` → `(..., 50, ...)` |\n",
    "|  参数3 `:`           | 嵌入维度   | 选取全部        | 保持 `(..., ..., 768)`                    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad098c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.564603Z",
     "iopub.status.busy": "2023-08-18T06:58:06.564068Z",
     "iopub.status.idle": "2023-08-18T06:58:06.571897Z",
     "shell.execute_reply": "2023-08-18T06:58:06.571098Z"
    },
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class BERTEncoder(nn.Module):\n",
    "    '''\n",
    "    vocab_size：词汇表大小（如30000）\n",
    "    num_hiddens：隐藏层维度（如768，与词向量维度一致）\n",
    "    norm_shape：LayerNorm的形状（通常是num_hiddens）\n",
    "    ffn_num_input：前馈网络输入维度（通常等于num_hiddens）\n",
    "    ffn_num_hiddens：前馈网络隐藏层维度（如3072）\n",
    "    num_heads：多头注意力头数（如12）\n",
    "    num_layers：Transformer层数（如12）\n",
    "    dropout：Dropout概率（如0.1）\n",
    "    max_len：最大序列长度（位置嵌入的预分配大小）\n",
    "    key_size/query_size/value_size：注意力机制的维度（通常等于num_hiddens）\n",
    "    '''\n",
    "    \"\"\"BERT编码器\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        # 1. 三种嵌入层：词元嵌入、位置嵌入、片段嵌入\n",
    "        # 词元嵌入，将词元ID转换为向量（vocab_size→num_hiddens）\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        # 片段嵌入，区分两个句子（0或1，2种可能）\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        '''\n",
    "        2. Transformer编码器块堆叠\n",
    "        循环创建num_layers个EncoderBlock（Transformer层）\n",
    "        每个块包含：多头自注意力+前馈网络+残差连接+LayerNorm\n",
    "        True：表示使用残差连接\n",
    "        '''\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(\n",
    "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\n",
    "        '''\n",
    "        位置嵌入：\n",
    "        Position Embedding：BERT使用可学习的位置嵌入\n",
    "        torch.randn(1,max_len,num_hiddens)：随机初始化，形状为(1,max_len,num_hiddens)\n",
    "        nn.Parameter：注册为模型参数，参与训练\n",
    "        维度1：批量维度（这里固定为1，因为位置嵌入是共享的）\n",
    "        维度2：序列长度维度（最大长度max_len）\n",
    "        维度3：嵌入维度（num_hiddens）\n",
    "        '''\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "    '''\n",
    "    tokens：词元ID序列，形状(batch_size,seq_len)\n",
    "    segments：段标记序列，形状(batch_size,seq_len)（0或1）\n",
    "    valid_lens：有效长度（用于掩码填充部分），形状(batch_size,)\n",
    "    '''\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）\n",
    "        '''\n",
    "        第1步：嵌入相加\n",
    "        Token Embedding：将词元ID转为向量\n",
    "        Segment Embedding：将段标记ID转为向量\n",
    "        逐元素相加：X形状保持(batch_size,seq_len,num_hiddens)\n",
    "        '''\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        '''\n",
    "        第2步：加上位置嵌入\n",
    "        .data：获取参数值（避免梯度追踪）\n",
    "        [:,:X.shape[1],:]：截取前seq_len个位置（因为pos_embedding预分配了max_len）\n",
    "        逐元素相加：加入位置信息\n",
    "        '''\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        '''\n",
    "        第3步：逐个通过Transformer层\n",
    "        循环通过num_layers个编码器块\n",
    "        每个块执行：自注意力→前馈→残差连接\n",
    "        valid_lens：用于创建掩码，忽略填充部分，真实词元数量（即非填充部分的长度）\n",
    "        '''\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        '''\n",
    "        形状：(batch_size,seq_len,num_hiddens)\n",
    "        含义：每个位置的上下文相关表示（contextualized representation）\n",
    "        '''\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd683c2c",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "假设词表大小为10000，为了演示`BERTEncoder`的前向推断，让我们创建一个实例并初始化它的参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7f639",
   "metadata": {},
   "source": [
    "1. 初始化嵌入层\n",
    "```Python\n",
    "# 在 __init__ 中自动创建\n",
    "self.token_embedding = nn.Embedding(10000, 768)      # 词元嵌入\n",
    "self.segment_embedding = nn.Embedding(2, 768)        # 片段嵌入（0或1）\n",
    "self.pos_embedding = nn.Parameter(torch.randn(1, 1000, 768))  # 位置嵌入（随机初始化）\n",
    "```\n",
    "2. 堆叠2个Transformer层\n",
    "```Python\n",
    "# 在 __init__ 的循环中创建\n",
    "self.blks = nn.Sequential()\n",
    "for i in range(2):  # num_layers = 2\n",
    "    self.blks.add_module(f\"{i}\", d2l.EncoderBlock(...))\n",
    "    # 第0层: EncoderBlock(0)\n",
    "    # 第1层: EncoderBlock(1)\n",
    "```\n",
    "每个 EncoderBlock 包含：\n",
    "- 多头自注意力（4个头）\n",
    "- 前馈网络（768 → 1024 → 768）\n",
    "- 残差连接和LayerNorm\n",
    "3. 总参数量估算\n",
    "```Python\n",
    "# 简化的参数计算（忽略偏置）\n",
    "token_emb = 10000 * 768 ≈ 7.7M\n",
    "segment_emb = 2 * 768 ≈ 1.5K\n",
    "pos_emb = 1000 * 768 ≈ 768K\n",
    "\n",
    "# 每个Transformer层\n",
    "attention = 4 * (768 * 768) * 3 ≈ 7.1M  # Q,K,V投影\n",
    "ffn = 768 * 1024 + 1024 * 768 ≈ 1.6M * 2 ≈ 3.2M\n",
    "\n",
    "# 2层总计\n",
    "total ≈ 7.7M + 0.8M + (7.1M + 3.2M) * 2 ≈ 7.7M + 0.8M + 20.6M ≈ **29M参数**\n",
    "（实际BERT-base有1.1亿参数）\n",
    "```\n",
    "**得到的 encoder 对象**\n",
    "```Python\n",
    "print(encoder)\n",
    "# 输出：\n",
    "BERTEncoder(\n",
    "  (token_embedding): Embedding(10000, 768)\n",
    "  (segment_embedding): Embedding(2, 768)\n",
    "  (blks): Sequential(\n",
    "    (0): EncoderBlock(...)\n",
    "    (1): EncoderBlock(...)\n",
    "  )\n",
    "  (pos_embedding): Parameter containing: [torch.FloatTensor of size 1x1000x768]\n",
    ")\n",
    "```\n",
    "**可执行的操作**\n",
    "```Python\n",
    "# 准备输入\n",
    "tokens = torch.randint(0, 10000, (32, 10))      # 批量32，长度10\n",
    "segments = torch.zeros(32, 10, dtype=torch.long) # 全0（单句）\n",
    "valid_lens = torch.tensor([10] * 32)            # 有效长度10\n",
    "\n",
    "# 前向传播\n",
    "output = encoder(tokens, segments, valid_lens)\n",
    "print(output.shape)  # → torch.Size([32, 10, 768])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9308328c",
   "metadata": {},
   "source": [
    "```python\n",
    "输入：\n",
    "tokens:   (2, 8)      [[123, 4567, ..., 567],\n",
    "                       [4321, 987, ..., 7654]]\n",
    "\n",
    "segments: (2, 8)      [[0,0,0,0,1,1,1,1],\n",
    "                       [0,0,0,1,1,1,1,1]]\n",
    "\n",
    "↓ Token Embedding\n",
    "X_token:  (2, 8, 768) 每个词元ID转为768维向量\n",
    "\n",
    "↓ Segment Embedding\n",
    "X_seg:    (2, 8, 768)  每个片段标记转为768维向量\n",
    "\n",
    "↓ Position Embedding（截取）\n",
    "X_pos:    (2, 8, 768)  前8个位置的嵌入，广播后相加\n",
    "\n",
    "↓ Transformer Block × 2\n",
    "encoded_X: (2, 8, 768)  每个词元的上下文表示\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94237d14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.575485Z",
     "iopub.status.busy": "2023-08-18T06:58:06.574955Z",
     "iopub.status.idle": "2023-08-18T06:58:06.758687Z",
     "shell.execute_reply": "2023-08-18T06:58:06.757737Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "vocab_size=10000：词汇表大小（实际BERT通常是30,000-50,000，这里简化为10,000）\n",
    "num_hiddens=768：隐藏层维度=词向量维度（与BERT-base一致）\n",
    "ffn_num_hiddens=1024：前馈网络隐藏层维度（实际BERT-base是3072，这里缩减）\n",
    "num_heads=4：多头注意力头数（实际BERT-base是12，这里减半）\n",
    "'''\n",
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "'''\n",
    "norm_shape=[768]：LayerNorm的归一化形状（对最后一个维度768归一化）\n",
    "ffn_num_input=768：前馈网络输入维度（等于num_hiddens）\n",
    "num_layers=2：Transformer层数（实际BERT-base是12，这里极度简化）\n",
    "dropout=0.2：Dropout比率（实际BERT常用0.1）\n",
    "'''\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                      ffn_num_hiddens, num_heads, num_layers, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69503ec",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "我们将`tokens`定义为长度为8的2个输入序列，其中每个词元是词表的索引。使用输入`tokens`的`BERTEncoder`的前向推断返回编码结果，其中每个词元由向量表示，其长度由超参数`num_hiddens`定义。此超参数通常称为Transformer编码器的*隐藏大小*（隐藏单元数）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57e87013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.762791Z",
     "iopub.status.busy": "2023-08-18T06:58:06.762204Z",
     "iopub.status.idle": "2023-08-18T06:58:06.780913Z",
     "shell.execute_reply": "2023-08-18T06:58:06.779803Z"
    },
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "第1行：生成随机词元序列\n",
    "torch.randint(low,high,shape)：生成随机整数张量\n",
    "0,vocab_size：词元ID范围是[0,10000)（因为我们定义vocab_size=10000）\n",
    "(2,8)：批量大小为2，每个样本长度为8\n",
    "结果：形状(2,8)的随机词元ID矩阵，模拟两个长度为8的句子\n",
    "'''\n",
    "tokens = torch.randint(0, vocab_size, (2, 8))\n",
    "'''\n",
    "第2行：定义片段标记\n",
    "作用：标记每个词元属于句子A（0）还是句子B（1）\n",
    "第一个样本 [0,0,0,0,1,1,1,1]：\n",
    "    前4个词元（索引0-3）属于句子A\n",
    "    后4个词元（索引4-7）属于句子B\n",
    "第二个样本 [0,0,0,1,1,1,1,1]：\n",
    "    前3个词元属于句子A\n",
    "    后5个词元属于句子B\n",
    "'''\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "'''\n",
    "第3行：执行前向传播\n",
    "tokens：词元ID张量（形状(2,8)）\n",
    "segments：片段标记张量（形状(2,8)）\n",
    "None valid_lens参数设为None：\n",
    "    表示不指定有效长度（即所有位置都有效）\n",
    "    等价于valid_lens=torch.tensor([8,8])\n",
    "    模型不会对填充位置做特殊处理\n",
    "内部流程：\n",
    "    token_embedding：将(2,8)转为(2,8,768)\n",
    "    segment_embedding：将片段标记(2,8)转为(2,8,768)\n",
    "    pos_embedding：截取前8个位置(1,8,768)并广播到(2,8,768)\n",
    "    三者相加得到X（形状(2,8,768)）\n",
    "    通过2个Transformer编码器块（每层有自注意力+前馈网络）\n",
    "    返回最终编码表示encoded_X\n",
    "'''\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a42de",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "## 预训练任务\n",
    "\n",
    "\n",
    "### 掩蔽语言模型（Masked Language Modeling）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc98249b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.786473Z",
     "iopub.status.busy": "2023-08-18T06:58:06.785498Z",
     "iopub.status.idle": "2023-08-18T06:58:06.795323Z",
     "shell.execute_reply": "2023-08-18T06:58:06.794249Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class MaskLM(nn.Module):\n",
    "    \"\"\"BERT的掩蔽语言模型任务\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        '''\n",
    "        初始化MLP预测头\n",
    "        第1个Linear：768→256（将上下文表示映射到隐藏层）\n",
    "        ReLU：非线性激活\n",
    "        LayerNorm：稳定训练\n",
    "        第2个Linear：256→vocab_size（输出词表上的概率分布）\n",
    "\n",
    "        '''\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens, vocab_size))\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        '''\n",
    "        第1步：确定预测位置数量\n",
    "        pred_positions是一个二维张量，形状为(batch_size,num_pred_positions)\n",
    "        示例：pred_positions=[[1,3,5],[2,4,6]]→shape=(2,3)→num_pred_positions=3\n",
    "        这表示每个样本要预测3个位置\n",
    "        '''\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        '''\n",
    "        第2步：展平位置索引\n",
    "        作用：将二维张量展平为一维，方便后续索引\n",
    "        示例：[[1,3,5],[2,4,6]]→[1,3,5,2,4,6]\n",
    "        形状：(2,3)→(6,)（共6个预测位置）\n",
    "        '''\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        '''\n",
    "        第3步：创建批次索引\n",
    "        torch.arange(0,batch_size)：生成[0,1,2,...,batch_size-1]\n",
    "        torch.repeat_interleave(...,num_pred_positions)：将每个批次索引重复3次\n",
    "        结果：[0,0,0,1,1,1]，与展平后的pred_positions一一对应\n",
    "        '''\n",
    "        batch_size = X.shape[0] # 假设X.shape=(2,8,768)→batch_size=2\n",
    "        batch_idx = torch.arange(0, batch_size)  # →tensor([0,1])\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions) # →tensor([0,0,0,1,1,1])\n",
    "        '''\n",
    "        第4步：提取掩蔽位置的表示（核心）\n",
    "        这是高级索引 操作，从X中精确提取需要预测的位置：\n",
    "        假设batch_size=2，num_pred_positions=3，那么batch_idx是np.array（[0,0,0,1,1,1]）\n",
    "        '''\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        '''\n",
    "        第5步：重塑为(batch_size,num_pred_positions,-1)\n",
    "        目的：将展平后的张量恢复为三维结构，方便MLP按批次处理。\n",
    "        (6,768)→(2,3,768)\n",
    "        -1 自动推断为768（特征维度）\n",
    "        '''\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        # \n",
    "        '''\n",
    "        第6步：通过MLP预测\n",
    "        输出含义：每个预测位置在词表上的logits分布\n",
    "        后续会用softmax和交叉熵计算损失\n",
    "        输入: (2,3,768)\n",
    "        经过self.mlp:\n",
    "            Linear(768→256)+ReLU+LayerNorm+Linear(256→10000)\n",
    "        输出: (2,3,10000)\n",
    "        '''\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b3d54",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "为了演示`MaskLM`的前向推断，我们创建了其实例`mlm`并对其进行了初始化。回想一下，来自`BERTEncoder`的正向推断`encoded_X`表示2个BERT输入序列。我们将`mlm_positions`定义为在`encoded_X`的任一输入序列中预测的3个指示。`mlm`的前向推断返回`encoded_X`的所有掩蔽位置`mlm_positions`处的预测结果`mlm_Y_hat`。对于每个预测，结果的大小等于词表的大小。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef4b0b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.800348Z",
     "iopub.status.busy": "2023-08-18T06:58:06.799558Z",
     "iopub.status.idle": "2023-08-18T06:58:06.905961Z",
     "shell.execute_reply": "2023-08-18T06:58:06.905018Z"
    },
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "实例化MLM模型\n",
    "vocab_size=10000：词汇表大小（根据前文）\n",
    "num_hiddens=256：MLP隐藏层维度（根据MaskLM定义）\n",
    "创建的MLM结构：MLP: Linear(768→256)→ReLU→LayerNorm→Linear(256→10000)\n",
    "'''\n",
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "'''\n",
    "定义要预测的位置\n",
    "这是一个2×3的张量，表示2个样本，每个样本需要预测3个位置：\n",
    "    第0个样本（第0行）：预测位置 1,5,2\n",
    "    第1个样本（第1行）：预测位置 6,1,5\n",
    "注意：位置索引是在原始序列中的位置（0-based）。\n",
    "'''\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "'''\n",
    "执行MLM前向传播\n",
    "encoded_X：来自BERT编码器的输出，假设形状为(2,8,768)（2个样本，8个词元，768维）\n",
    "mlm_positions：预测位置，形状 (2,3)\n",
    "内部执行流程（回顾MaskLM.forward）：\n",
    "    1. num_pred_positions=3\n",
    "    2. pred_positions展平→[1,5,2,6,1,5]\n",
    "    3. batch_idx→[0,0,0,1,1,1]\n",
    "    4. 高级索引提取：masked_X=encoded_X[[0,0,0,1,1,1],[1,5,2,6,1,5]]→形状 (6,768)\n",
    "    5. 重塑：(6,768)→(2,3,768)\n",
    "    6. MLP预测：(2,3,768)→(2,3,256)→(2,3,10000)\n",
    "'''\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "'''\n",
    "查看输出形状\n",
    "2：批量大小（2个样本）\n",
    "3：每个样本预测3个位置\n",
    "10000：每个位置在10000个词上的logits分布\n",
    "'''\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fc7ae",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "| 代码行                                               | 作用   | 输入形状                   | 输出形状              | 关键技术    |\n",
    "| ------------------------------------------------- | ---- | ---------------------- | ----------------- | ------- |\n",
    "| `mlm_Y`                                  | 真实标签 | -                      | `(2, 3)`          | 词元索引    |\n",
    "| `nn.CrossEntropyLoss(reduction='none')` | 损失函数 | -                      | -                 | 逐元素损失   |\n",
    "| `mlm_Y_hat.reshape(-1, vocab_size)`     | 重塑预测 | `(2, 3, 10000)`        | `(6, 10000)`      | 展平批量和位置 |\n",
    "| `mlm_Y.reshape(-1)`                     | 重塑标签 | `(2, 3)`               | `(6,)`            | 展平标签    |\n",
    "| `loss(...)`                             | 计算损失 | `(6, 10000)` vs `(6,)` | `(6,)`            | 交叉熵     |\n",
    "| `.shape`                                | 查看结果 | -                      | `torch.Size([6])` | 验证维度    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace75d78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.910802Z",
     "iopub.status.busy": "2023-08-18T06:58:06.910165Z",
     "iopub.status.idle": "2023-08-18T06:58:06.918066Z",
     "shell.execute_reply": "2023-08-18T06:58:06.917108Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. 真实标签\n",
    "含义：真实的词元ID（目标答案），形状 (2,3)\n",
    "    第0个样本的3个预测位置的真实词分别是索引7,8,9\n",
    "    第1个样本的3个预测位置的真实词分别是索引10,20,30\n",
    "'''\n",
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "'''\n",
    "2. 定义损失函数\n",
    "CrossEntropyLoss：交叉熵损失，用于分类任务\n",
    "reduction='none'：返回 每个样本的独立损失，不聚合（不sum/mean）\n",
    "输出形状：将为每个预测位置返回一个标量损失\n",
    "'''\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "'''\n",
    "3. 计算损失\n",
    "mlm_Y_hat.reshape((-1,vocab_size))\n",
    "    mlm_Y_hat形状:(2,3,10000)\n",
    "    reshape后:(6,10000) # -1 自动计算为2*3=6\n",
    "    变成: 6个预测位置，每个对应10000个词的logits\n",
    "mlm_Y.reshape(-1)\n",
    "    mlm_Y形状:(2,3)\n",
    "    reshape后:(6,) # -1 自动计算为2*3=6\n",
    "    变成: 6个真实词索引\n",
    "损失计算过程\n",
    "    第1个预测位置:mlm_Y_hat[0,0,:] vs mlm_Y[0](7)\n",
    "    第2个预测位置:mlm_Y_hat[0,1,:] vs mlm_Y[1](8)\n",
    "    ...\n",
    "    第6个预测位置:mlm_Y_hat[1,2,:] vs mlm_Y[5](30)\n",
    "'''\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "'''\n",
    "4. 查看损失形状\n",
    "预期输出：torch.Size([6])\n",
    "为什么是6？ 因为总共2个样本×3个预测位置=6个独立损失\n",
    "每个元素：对应一个预测位置的交叉熵损失值\n",
    "'''\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad54b6d",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "### 下一句预测（Next Sentence Prediction）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fcf0f0",
   "metadata": {},
   "source": [
    "| 组件                             | 作用         | 输入                        | 输出                     |\n",
    "| ------------------------------ | ---------- | ------------------------- | ---------------------- |\n",
    "|  `NextSentencePred`    | NSP任务头     | `<cls>` 表示 `(batch, 768)` | 二分类logits `(batch, 2)` |\n",
    "|  `encoded_X[:, 0, :]`  | 提取 `<cls>` | `(batch, seq_len, 768)`   | `(batch, 768)`         |\n",
    "| 训练目标                   | 理解句间关系     | 句对输入                      | 0/1标签                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7be502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.922549Z",
     "iopub.status.busy": "2023-08-18T06:58:06.921958Z",
     "iopub.status.idle": "2023-08-18T06:58:06.927273Z",
     "shell.execute_reply": "2023-08-18T06:58:06.926309Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"BERT的下一句预测任务\"\"\"\n",
    "    '''\n",
    "    num_inputs：输入特征维度，通常等于num_hiddens（如768），是<cls>词元的表示维度\n",
    "    nn.Linear(num_inputs,2)：线性层，将768维向量映射到2个输出（二分类：是/否下一句）\n",
    "    输出含义：\n",
    "        logit[0]：不是下一句的分数\n",
    "        logit[1]：是下一句的分数\n",
    "    '''\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "    '''\n",
    "    输入X：<cls>词元的编码表示，形状(batch_size,768)\n",
    "    为什么是<cls>？因为BERT训练时让<cls>聚合整个序列的语义信息，适合分类任务\n",
    "    返回：形状(batch_size,2)的logits，表示二分类的未归一化分数\n",
    "    '''\n",
    "    def forward(self, X):\n",
    "        # X的形状：(batchsize,num_hiddens)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c6890",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "我们可以看到，`NextSentencePred`实例的前向推断返回每个BERT输入序列的二分类预测。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4542505a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.932297Z",
     "iopub.status.busy": "2023-08-18T06:58:06.931348Z",
     "iopub.status.idle": "2023-08-18T06:58:06.939874Z",
     "shell.execute_reply": "2023-08-18T06:58:06.938907Z"
    },
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. 展平编码器输出\n",
    "NextSentencePred的输入是整个序列的拼接，模型需要学习从所有词元中判断句间关系\n",
    "encoded_X原始形状：(batch_size,seq_len,num_hiddens)，例如(2,8,768)\n",
    "    torch.flatten(...,start_dim=1)：从第1维开始展平\n",
    "    第0维（batch）保持不变，第1维及以后合并为一个维度\n",
    "结果形状：(2,8*768)=(2,6144)→每个样本变成一维向量\n",
    "'''\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "# NSP的输入形状:(batchsize，num_hiddens)\n",
    "'''\n",
    "2. 创建NSP预测头\n",
    "encoded_X.shape[-1]展平后=6144（8*768）\n",
    "NextSentencePred(6144)：创建线性层nn.Linear(6144,2)\n",
    "'''\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "'''\n",
    "3. 执行NSP预测\n",
    "输入：encoded_X形状(2,6144)\n",
    "输出：nsp_Y_hat形状(2,2)→每个样本的二分类logits\n",
    "'''\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "'''\n",
    "4. 查看输出形状\n",
    "2：批量大小\n",
    "2：二分类logits（是/否下一句）\n",
    "'''\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8acdafa",
   "metadata": {
    "origin_pos": 37
   },
   "source": [
    "还可以计算两个二元分类的交叉熵损失。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd7737",
   "metadata": {},
   "source": [
    "**形状匹配详解**\n",
    "```Python\n",
    "nsp_Y_hat: 形状 (2, 2)  # 2个样本，2个类别的logits\n",
    "# 例如: tensor([[-0.34,  1.25],   # 样本0: 不是下一句的分数=-0.34, 是下一句的分数=1.25\n",
    "#               [ 2.10, -0.78]])  # 样本1: 不是下一句的分数=2.10, 是下一句的分数=-0.78\n",
    "\n",
    "nsp_y:       形状 (2,)    # 2个样本的真实标签\n",
    "# 例如: tensor([0, 1])\n",
    "\n",
    "# CrossEntropyLoss 会自动:\n",
    "# 1. 对 nsp_Y_hat 做 softmax → 概率\n",
    "# 2. 根据 nsp_y 选取对应类别的概率\n",
    "# 3. 计算 -log(prob) 作为损失\n",
    "```\n",
    "**计算过程（手动模拟）**\n",
    "```Python\n",
    "# 假设 nsp_Y_hat = [[-0.34, 1.25], [2.10, -0.78]]\n",
    "# 样本0: 真实标签=0 (不是下一句)\n",
    "#   softmax([-0.34, 1.25]) = [0.20, 0.80]\n",
    "#   loss0 = -log(0.20) ≈ 1.61\n",
    "\n",
    "# 样本1: 真实标签=1 (是下一句)\n",
    "#   softmax([2.10, -0.78]) = [0.95, 0.05]\n",
    "#   loss1 = -log(0.05) ≈ 2.99\n",
    "\n",
    "# 如果 reduction='none': nsp_l = [1.61, 2.99]\n",
    "# 如果 reduction='mean' (默认): nsp_l = (1.61 + 2.99) / 2 = 2.30\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18e452",
   "metadata": {},
   "source": [
    "**输出取决于 loss 的 reduction 参数**：\n",
    "\n",
    "**情况1：reduction='none'**\n",
    "```Python\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "print(nsp_l.shape)  # → torch.Size([2])\n",
    "# 每个元素对应一个样本的损失\n",
    "# tensor([1.61, 2.99])\n",
    "```\n",
    "**情况2：reduction='mean'（默认）**\n",
    "```Python\n",
    "loss = nn.CrossEntropyLoss()  # reduction='mean'\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "print(nsp_l.shape)  # → torch.Size([])  (标量)\n",
    "# tensor(2.30)\n",
    "```\n",
    "**情况3：reduction='sum'**\n",
    "```Python\n",
    "loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "print(nsp_l.shape)  # → torch.Size([])  (标量)\n",
    "# tensor(4.60)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b1939",
   "metadata": {},
   "source": [
    "| 代码行                                | 功能     | 输入形状               | 输出形状 (reduction='mean') | 输出形状 (reduction='none') |\n",
    "| ---------------------------------- | ------ | ------------------ | ----------------------- | ----------------------- |\n",
    "| **`nsp_y = torch.tensor([0, 1])`** | 真实标签   | -                  | `(2,)` (long)           | `(2,)` (long)           |\n",
    "| **`loss = CrossEntropyLoss()`**    | 定义损失函数 | -                  | -                       | -                       |\n",
    "| **`loss(nsp_Y_hat, nsp_y)`**       | 计算损失   | `(2, 2)` vs `(2,)` | `torch.Size([])` (标量)   | `torch.Size([2])`       |\n",
    "| **`nsp_l.shape`**                  | 查看损失形状 | -                  | `torch.Size([])`        | `torch.Size([2])`       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaf7a84c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.944820Z",
     "iopub.status.busy": "2023-08-18T06:58:06.944049Z",
     "iopub.status.idle": "2023-08-18T06:58:06.951717Z",
     "shell.execute_reply": "2023-08-18T06:58:06.950547Z"
    },
    "origin_pos": 39,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. 定义真实标签\n",
    "含义：NSP任务的真实标签，形状(2,)，表示2个样本的标签\n",
    "标签解释：\n",
    "    0：第一个样本，句子B不是句子A的下一句（负例）\n",
    "    1：第二个样本，句子B是句子A的下一句（正例）\n",
    "'''\n",
    "nsp_y = torch.tensor([0, 1])\n",
    "# 2. 计算交叉熵损失\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "#3. 查看损失形状\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d605fb6",
   "metadata": {
    "origin_pos": 41
   },
   "source": [
    "## 整合代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5c5acd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:58:06.956805Z",
     "iopub.status.busy": "2023-08-18T06:58:06.955956Z",
     "iopub.status.idle": "2023-08-18T06:58:06.966697Z",
     "shell.execute_reply": "2023-08-18T06:58:06.965474Z"
    },
    "origin_pos": 43,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "核心架构参数\n",
    "    vocab_size：词汇表大小（如30000）\n",
    "    num_hiddens：隐藏层维度（768），即词向量维度\n",
    "    num_heads：多头注意力头数（12）\n",
    "    num_layers：Transformer层数（12）\n",
    "    dropout：Dropout比率（0.1）\n",
    "前馈网络参数\n",
    "    ffn_num_input：前馈网络输入维度（768）\n",
    "    ffn_num_hiddens：前馈网络隐藏层维度（3072）\n",
    "输入维度参数（灵活设计）\n",
    "    hid_in_features=768：进入self.hidden前的输入维度\n",
    "    mlm_in_features=768：MLM头的输入维度\n",
    "    nsp_in_features=768：NSP头的输入维度\n",
    "    设计目的：允许不同组件使用不同输入维度（虽然默认值相同）\n",
    "'''\n",
    "#@save\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 hid_in_features=768, mlm_in_features=768,\n",
    "                 nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        '''\n",
    "        1. BERT编码器\n",
    "        功能：将词元ID转换为上下文相关的向量表示\n",
    "        输出：形状(batch_size,seq_len,num_hiddens)\n",
    "        '''\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n",
    "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                    dropout, max_len=max_len, key_size=key_size,\n",
    "                    query_size=query_size, value_size=value_size)\n",
    "        '''\n",
    "        2. NSP隐藏层（特殊设计）\n",
    "        作用：对<cls>词元的表示进行非线性变换\n",
    "        为什么需要：BERT论文中，NSP任务前对<cls>使用Tanh激活，这是特定设计选择\n",
    "        输入：encoded_X[:,0,:]（<cls>表示，形状(batch,768)）\n",
    "        输出：形状(batch,num_hiddens)（保持768维）\n",
    "        '''\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        # 3. MLM预测头\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        '''\n",
    "        4. NSP预测头\n",
    "        输入：self.hidden(...)处理后的<cls>表示\n",
    "        输出：形状(batch,2)（二分类logits）\n",
    "        '''\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "    '''\n",
    "    tokens：词元ID，形状(batch,seq_len)\n",
    "    segments：片段标记，形状(batch,seq_len)\n",
    "    valid_lens：有效长度（可选）\n",
    "    pred_positions：MLM预测位置（可选）\n",
    "    '''\n",
    "    def forward(self, tokens, segments, valid_lens=None,\n",
    "                pred_positions=None):\n",
    "        # 1. 编码器编码\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens) # encoded_X:(batch,seq_len,768)\n",
    "        '''\n",
    "        2. MLM任务（条件执行）\n",
    "        条件判断：如果提供了预测位置，才执行MLM\n",
    "        设计目的：\n",
    "            训练时：pred_positions有值，计算MLM损失\n",
    "            推理/微调时：pred_positions=None，跳过MLM（节省时间）\n",
    "        '''\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引\n",
    "        '''\n",
    "        3. NSP任务（总是执行）\n",
    "        encoded_X[:,0,:]：提取<cls>词元的表示\n",
    "            索引0：<cls>是序列第一个词元\n",
    "            形状：(batch,768)\n",
    "        self.hidden(...)：通过Tanh激活层\n",
    "            形状：(batch,768)\n",
    "        self.nsp(...)：NSP二分类\n",
    "            输出：(batch,2)\n",
    "        '''\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        '''\n",
    "        4. 返回结果\n",
    "        encoded_X：完整编码表示（用于下游任务）\n",
    "        mlm_Y_hat：MLM预测（训练时用）\n",
    "        nsp_Y_hat：NSP预测（训练时用）\n",
    "        '''\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
