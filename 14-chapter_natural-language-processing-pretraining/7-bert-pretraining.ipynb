{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099f849e",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 预训练BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c0979b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:04:26.170037Z",
     "iopub.status.busy": "2023-08-18T07:04:26.168910Z",
     "iopub.status.idle": "2023-08-18T07:04:28.547324Z",
     "shell.execute_reply": "2023-08-18T07:04:28.546158Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import os \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f138ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "d2l.DATA_HUB['wikitext-2'] = (\n",
    "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
    "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
    "\n",
    "#@save\n",
    "def _read_wiki(data_dir):\n",
    "    '''\n",
    "    1. 构建文件路径\n",
    "    data_dir：数据目录路径\n",
    "    wiki.train.tokens：WikiText格式的训练文件\n",
    "    文件格式：纯文本，每行是一个段落，句子用 .（空格+句点+空格）分隔\n",
    "    '''\n",
    "\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    '''\n",
    "    2. 读取文件\n",
    "    读取所有行：每行是一个段落字符串\n",
    "    lines：列表，每个元素是一个字符串（包含换行符）\n",
    "    '''\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # 大写字母转换为小写字母\n",
    "    '''\n",
    "    3. 处理段落\n",
    "    步骤1：line.strip()：移除行首尾的空白字符和换行符\n",
    "    步骤2：.lower()：大写转小写：统一文本格式，减少词汇表大小\n",
    "    例：\"Hello World . This is BERT.\" → \"hello world . this is bert.\"\n",
    "    步骤3：.split(' . ')：按句子分隔符分割，使用' . '（空格+句点+空格）作为分隔符，结果得到一个句子列表\n",
    "    例：\"hello world . this is bert .\" → ['hello world', 'this is bert', '']\n",
    "    步骤4：if len(...)>=2，筛选有效段落 ：只保留至少包含2个句子的段落，目的：为NSP任务准备（需要句对）\n",
    "    '''\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    '''\n",
    "    4. 随机打乱\n",
    "    作用：随机打乱段落顺序\n",
    "    目的：确保训练数据分布均匀，避免模型记住顺序\n",
    "    '''\n",
    "    random.shuffle(paragraphs)\n",
    "    # 返回结果：paragraphs列表，每个元素是句子列表\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4550b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sentence：当前句子A（字符串）\n",
    "next_sentence：句子A的真实下一句（正例）\n",
    "paragraphs：所有段落的列表（三重嵌套结构）\n",
    "'''\n",
    "#@save\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    # 50%概率生成正例\n",
    "    if random.random() < 0.5: # 生成[0,1)之间的随机数，以50%概率保持真实下一句\n",
    "        is_next = True # 标签为正（是下一句\n",
    "    else:\n",
    "        # 50%概率生成负例\n",
    "        # paragraphs是三重列表的嵌套\n",
    "        next_sentence = random.choice(random.choice(paragraphs)) # 随机选择一个段落（二级列表），再在该段落中随机选择一个句子\n",
    "        is_next = False # 标签为负（不是下一句）\n",
    "    return sentence, next_sentence, is_next # next_sentence是真实的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73a84b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "以50%概率生成正例（真实下一句）\n",
    "以50%概率生成负例（随机段落中的随机句子）\n",
    "检查长度约束并转换为BERT输入格式\n",
    "'''\n",
    "#@save\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    # 1. 初始化结果列表：存储当前段落生成的所有NSP训练样本\n",
    "    nsp_data_from_paragraph = []\n",
    "    '''\n",
    "    2. 遍历段落中的句子\n",
    "        遍历索引0到len(paragraph)-1\n",
    "        原因：需要取paragraph[i]和paragraph[i+1]组成句对\n",
    "    '''\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        '''\n",
    "        3. 生成正例/负例\n",
    "        paragraph[i]：当前句子A（字符串）\n",
    "        paragraph[i+1]：句子A的真实下一句B（字符串）\n",
    "        paragraphs：所有段落的嵌套列表（用于负例采样）\n",
    "        返回：\n",
    "            tokens_a：句子A的词元列表\n",
    "            tokens_b：句子B的词元列表（可能是真实下一句，也可能是随机句子）\n",
    "            is_next：布尔值，True表示是真实下一句，False表示随机句子\n",
    "        '''\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
    "            paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        '''\n",
    "        4. 长度检查与过滤\n",
    "        +3的含义：\n",
    "            '<cls>'：1个特殊词元（序列开头）\n",
    "            '<sep>'：2个特殊词元（句子A结尾、句子B结尾）\n",
    "        max_len：最大序列长度限制（如512）\n",
    "        continue：如果超长，跳过该句对，不加入训练集\n",
    "        '''\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        '''\n",
    "        5. 转换为BERT输入格式\n",
    "            tokens：合并后的词元列表（格式：<cls>+tokens_a+<sep>+tokens_b+<sep>）\n",
    "            segments：段标记列表（0表示句子A，1表示句子B）\n",
    "        '''\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        '''\n",
    "        6. 存储样本\n",
    "        tokens：输入词元序列\n",
    "        segments：片段标记\n",
    "        is_next：标签（True/False）\n",
    "        '''\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    # 7. 返回所有样本：返回值：列表，每个元素是一个(tokens,segments,is_next)元组\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f492474",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "tokens:BERT输入序列的词元列表（如['<cls>','我','爱','深','度','学','习','<sep>']）\n",
    "candidate_pred_positions:可能被遮蔽的词元位置索引列表（特殊词元如<cls>、<sep>会被排除）\n",
    "num_mlm_preds:需要预测的词元数量（通常是序列长度的15%）\n",
    "vocab:词表对象，用于随机选择替换词元\n",
    "'''\n",
    "#@save\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n",
    "                        vocab):\n",
    "    # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元\n",
    "    '''\n",
    "    1. 创建输入词元副本\n",
    "    目的：创建可修改的副本，不破坏原始序列\n",
    "    深拷贝：列表推导式确保修改副本不影响原始tokens\n",
    "    '''\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    '''\n",
    "    2. 初始化预测位置和标签列表\n",
    "    pred_positions_and_labels：存储预测位置和对应的真实词元\n",
    "    用途：存储被替换位置及其原始词元的元组\n",
    "    格式：[(位置1,原始词1),(位置2,原始词2),...]\n",
    "    '''\n",
    "    pred_positions_and_labels = []\n",
    "    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
    "    '''\n",
    "    3. 打乱候选预测位置\n",
    "    原因：确保随机性，模型无法预测哪些位置会被掩蔽\n",
    "    candidate_pred_positions：所有可被掩蔽的位置索引列表（通常排除<cls>和<sep>）\n",
    "    '''\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    '''\n",
    "    4. 遍历并选择预测位置\n",
    "    终止条件：达到预设的掩蔽数量（通常为序列长度的15%）\n",
    "    提前退出：避免掩蔽过多词元\n",
    "    '''\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        '''\n",
    "        5. 80-10-10替换策略（核心）\n",
    "        这是BERT MLM任务的关键设计，对每个候选位置：\n",
    "        (1) 80%的时间：将词替换为“<mask>”词元\n",
    "            主要任务：模型必须预测原始词元\n",
    "            训练目标：学习双向上下文理解\n",
    "        '''\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            '''\n",
    "            (2) 10%的时间：保持词不变\n",
    "                目的：减少训练和推理差异（推理时没有<mask>）\n",
    "                信号：模型看到原始词元，但仍需预测它（有点矛盾，但有效）\n",
    "            '''\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            else:\n",
    "                '''\n",
    "                (3) 10%概率替换为随机词\n",
    "                    目的：强制模型不完全依赖<mask>，必须理解上下文\n",
    "                    噪声引入：随机词提供干扰，防止模型\"偷懒\"\n",
    "                '''\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        '''\n",
    "        6. 执行替换并记录 \n",
    "        替换操作：将词元放入副本的指定位置\n",
    "        记录标签：保存原始位置和原始词元，用于后续损失计算\n",
    "        '''\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        '''\n",
    "        作用：将被遮蔽的位置和该位置的原始词元组成元组，存入列表\n",
    "        为什么用tokens而不是mlm_input_tokens？\n",
    "            tokens[mlm_pred_position]：原始词元（如'学习'），作为预测目标（label）\n",
    "            mlm_input_tokens[mlm_pred_position]：已被替换为<mask>或随机词，不再是正确答案\n",
    "        数据结构：[(pos1,label1),(pos2,label2),...]，确保位置与标签一一对应\n",
    "        '''\n",
    "        pred_positions_and_labels.append(\n",
    "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    '''\n",
    "    返回值1：mlm_input_tokens-替换后的词元序列（用于模型输入）\n",
    "    返回值2：pred_positions_and_labels-被遮蔽位置和原始词元的配对列表（用于计算损失）\n",
    "    '''\n",
    "    return mlm_input_tokens, pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11729b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    # 1. 初始化候选位置列表：用于存储所有可以被遮蔽的词元位置索引\n",
    "    candidate_pred_positions = []\n",
    "    # tokens是一个字符串列表\n",
    "    '''\n",
    "    2. 遍历词元序列，排除特殊词元\n",
    "    关键逻辑：特殊词元（<cls>分类标记、<sep>分隔标记）不参与预测\n",
    "    原因：这些词元是BERT输入的结构标记，不是语义内容\n",
    "    结果：candidate_pred_positions=[1,2,3,4]（假设tokens长度为5）\n",
    "    '''\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    '''\n",
    "    3. 计算需要预测的词元数量\n",
    "    遮蔽语言模型任务中预测15%的随机词元\n",
    "    规则：至少预测1个词元（即使序列很短）\n",
    "    比例：15%的词元会被选中\n",
    "    示例：若len(tokens)=20，则num_mlm_preds=3\n",
    "    max(1,...)的作用：防止序列过短时计算结果为0\n",
    "    '''\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    '''\n",
    "    4. 执行遮蔽替换\n",
    "    调用下层函数：传入原始词元、候选位置、预测数量和词表\n",
    "    返回结果：\n",
    "    mlm_input_tokens：已遮蔽的词元序列（如['<cls>','我','<mask>','NLP','<sep>']）\n",
    "    pred_positions_and_labels：被遮蔽位置和原始词元的配对列表，如[(2,'爱')]\n",
    "    '''\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    '''\n",
    "    5. 按位置排序（关键步骤）\n",
    "    作用：确保预测位置按升序排列\n",
    "    为什么需要排序？\n",
    "        _replace_mlm_tokens中random.shuffle()打乱了选择顺序\n",
    "        模型需要固定顺序来处理预测位置\n",
    "        后续分离位置/标签时保证一一对应关系不混乱\n",
    "    '''\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
    "                                       key=lambda x: x[0])\n",
    "    # 分离位置和标签：将元组列表拆分为两个独立列表\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels] # 提取位置\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels] # 提取原始词元\n",
    "    '''\n",
    "    返回最终数据\n",
    "    返回值1：vocab[mlm_input_tokens]-输入词元的ID序列\n",
    "        通过词表将词元转换为整数ID（如[101,2769,103,1920,102]）\n",
    "    返回值2：pred_positions-被遮蔽位置的索引列表\n",
    "    返回值3：vocab[mlm_pred_labels]-预测目标的ID列表\n",
    "    '''\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03acd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "_pad_bert_inputs将多个样本（每个样本长度不同）填充到固定的max_len，并返回7个列表，分别对应：\n",
    "    token序列、段落标记、有效长度\n",
    "    预测位置、预测权重、预测标签\n",
    "    NSP标签\n",
    "'''\n",
    "#@save\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    '''\n",
    "    作用：计算每个样本最多需要预测多少词元（15%的max_len）\n",
    "    示例：若max_len=128，则max_num_mlm_preds=19\n",
    "    '''\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = []\n",
    "    \n",
    "    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n",
    "         is_next) in examples:\n",
    "        # 1.  填充token序列：在序列末尾添加<pad>词元，直到长度达到max_len\n",
    "        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n",
    "            max_len - len(token_ids)), dtype=torch.long)) # 差值\n",
    "        # 2. 填充段落标记：段落标记也填充为0（与pad保持一致），这里的0表示\"属于段落A\"，填充部分也属于A\n",
    "        all_segments.append(torch.tensor(segments + [0] * (\n",
    "            max_len - len(segments)), dtype=torch.long))\n",
    "        # 3. 记录有效长度（不含pad）：记录原始序列的真实长度（不包括填充部分）\n",
    "        # 示例：len(token_ids)=5，有效长度就是5\n",
    "        # 用途：在Transformer的Attention计算中，用valid_len屏蔽pad位置\n",
    "        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n",
    "        # 4. 填充预测位置：预测位置列表也填充到固定长度max_num_mlm_preds\n",
    "        # 填充值：用0填充（0是有效位置，但后面用权重屏蔽）\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n",
    "            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
    "        '''\n",
    "        5. 设计预测权重\n",
    "        作用：区分真实预测位置和填充位置\n",
    "        机制：\n",
    "            真实预测位置→权重为1.0（参与损失计算）\n",
    "            填充位置→权重为0.0（在损失中被屏蔽）\n",
    "        实现：在计算损失时，loss*weight，填充位置的loss被置0\n",
    "        '''\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
    "                max_num_mlm_preds - len(pred_positions)),\n",
    "                dtype=torch.float32))\n",
    "        # 6. 填充预测标签：标签也填充为0（配合权重为0，不会影响损失）\n",
    "        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n",
    "            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        # 7. 收集NSP标签：收集\"是否为下一句\"的二分类标签（0或1）\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
    "            all_mlm_weights, all_mlm_labels, nsp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1ae40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    paragraphs:原始段落列表，每个元素是字符串（如[\"I love NLP. It's fun.\",\"Deep learning rocks!\"]）\n",
    "    max_len:最大序列长度（如128、512）\n",
    "    '''\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # 输入paragraphs[i]是代表段落的句子字符串列表；\n",
    "        # 而输出paragraphs[i]是代表段落的句子列表，其中每个句子都是词元列表\n",
    "        # 1.  词元化（Tokenization）：将每个段落字符串拆分为句子列表，再将每个句子拆分为词元列表\n",
    "        paragraphs = [d2l.tokenize(\n",
    "            paragraph, token='word') for paragraph in paragraphs]\n",
    "        '''\n",
    "        2. 构建词汇表\n",
    "        双重列表推导式：将所有段落的所有句子展平为一个句子列表\n",
    "        min_freq=5：只保留出现≥5次的词（过滤低频词）\n",
    "        reserved_tokens：强制添加的特殊词元：\n",
    "            <pad>：填充标记\n",
    "            <mask>：遮蔽标记\n",
    "            <cls>：分类标记（句首）\n",
    "            <sep>：分隔标记（句间）\n",
    "        '''\n",
    "        sentences = [sentence for paragraph in paragraphs\n",
    "                     for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
    "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        '''\n",
    "        3. 生成NSP（下一句预测）数据\n",
    "        调用函数：_get_nsp_data_from_paragraph为每个段落生成NSP样本\n",
    "        样本格式：每个样本是 (tokens,segments,is_next)\n",
    "            tokens：词元ID列表（如[<cls>,句子A,<sep>,句子B,<sep>]）\n",
    "            segments：段落标记（0/1，区分句子A/B）\n",
    "            is_next：二分类标签（1=是下一句，0=不是）\n",
    "        作用：构建BERT的第一个预训练任务数据\n",
    "        '''\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraphs, self.vocab, max_len))\n",
    "        '''\n",
    "        4. 生成MLM（遮蔽语言模型）数据\n",
    "        操作：对每个NSP样本，生成MLM遮蔽数据\n",
    "        _get_mlm_data_from_tokens 返回：(mlm_input_tokens,pred_positions,mlm_pred_labels)\n",
    "        元组拼接：将MLM数据与NSP数据合并\n",
    "        最终格式：(tokens,pred_positions,mlm_labels,segments,is_next)\n",
    "        '''\n",
    "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "                      + (segments, is_next))\n",
    "                     for tokens, segments, is_next in examples]\n",
    "        # 填充输入\n",
    "        '''\n",
    "        5. 填充并存储数据\n",
    "        调用 _pad_bert_inputs：将所有样本填充到max_len长度\n",
    "        解压赋值：将返回的7个列表分别赋值给类的属性\n",
    "        最终属性：\n",
    "            self.all_token_ids：输入词元ID序列（填充后）\n",
    "            self.all_segments：段落标记（填充后）\n",
    "            self.valid_lens：有效长度（不含填充）\n",
    "            self.all_pred_positions：预测位置（填充后）\n",
    "            self.all_mlm_weights：预测权重（1.0/0.0屏蔽填充）\n",
    "            self.all_mlm_labels：预测标签（填充后）\n",
    "            self.nsp_labels：NSP二分类标签\n",
    "        '''\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
    "         self.all_pred_positions, self.all_mlm_weights,\n",
    "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n",
    "            examples, max_len, self.vocab) # 元组解包赋值\n",
    "    '''\n",
    "    功能：通过索引idx获取单个训练样本\n",
    "    调用方式：dataset[i]等价于dataset.__getitem__(i)\n",
    "    返回：包含7个张量的元组，对应一个BERT训练样本\n",
    "    '''\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
    "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
    "                self.nsp_labels[idx])\n",
    "    '''\n",
    "    作用：返回数据集中有多少个训练样本\n",
    "    原理：self.all_token_ids是一个列表，每个元素对应一个样本，所以它的长度就是样本总数\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c406028",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "作用：加载WikiText-2语料库，返回可迭代的数据加载器和词表\n",
    "batch_size：每个批次的样本数量（如256、512）\n",
    "max_len：序列最大长度（如128、512）\n",
    "'''\n",
    "\n",
    "#@save\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    '''\n",
    "    1. 设置数据加载工作进程\n",
    "    作用：获取CPU核心数，用于并行数据加载\n",
    "    示例：8核CPU→num_workers=4（留出核心给主进程）\n",
    "    目的：加速数据预处理，避免GPU等待数据\n",
    "    '''\n",
    "    num_workers = 0\n",
    "    '''\n",
    "    2. 下载并解压数据集\n",
    "    行为：自动从网络下载WikiText-2数据集并解压\n",
    "    WikiText-2是什么：包含维基百科文章的中等规模语料库（训练集约36MB）\n",
    "    返回：解压后的文件夹路径\n",
    "    '''\n",
    "    data_dir = '..\\\\data\\\\wikitext-2'\n",
    "    '''\n",
    "    3. 读取原始文本\n",
    "    调用_read_wiki：读取所有文本文件，按段落组织\n",
    "    返回格式：paragraphs[i]是第i个段落的原始字符串\n",
    "    '''\n",
    "    paragraphs = _read_wiki(data_dir)\n",
    "    '''\n",
    "    4.  创建数据集实例\n",
    "    调用__init__：执行我们之前解析的完整数据处理流水线\n",
    "    耗时操作：词表构建、NSP/MLM样本生成、填充处理（可能耗时数分钟）\n",
    "    结果：train_set包含所有预处理后的数据（数百万样本）\n",
    "    '''\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    '''\n",
    "    5. 创建数据迭代器\n",
    "    作用：将数据集包装为可迭代对象，支持批量加载\n",
    "    关键参数：\n",
    "    shuffle=True：每个epoch打乱样本顺序，防止模型学到顺序偏见\n",
    "    num_workers=num_workers：多进程并行加载数据\n",
    "    返回：train_iter是PyTorch的DataLoader对象\n",
    "    '''\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                        shuffle=True, num_workers=num_workers)\n",
    "    '''\n",
    "    6. 返回结果\n",
    "    返回值1：train_iter-数据加载器，训练时循环迭代\n",
    "    返回值2：train_set.vocab-词表，用于词元↔ID转换\n",
    "    '''\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d6f91",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "首先，我们加载WikiText-2数据集作为小批量的预训练样本，用于遮蔽语言模型和下一句预测。批量大小是512，BERT输入序列的最大长度是64。注意，在原始BERT模型中，最大长度是512。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95571e6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:04:28.552742Z",
     "iopub.status.busy": "2023-08-18T07:04:28.552374Z",
     "iopub.status.idle": "2023-08-18T07:04:38.456343Z",
     "shell.execute_reply": "2023-08-18T07:04:38.455141Z"
    },
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "批次大小：512个样本/批次（适合GPU内存）\n",
    "序列长度：每个样本最多64个词元（含<cls>、<sep>、填充）\n",
    "'''\n",
    "batch_size, max_len = 512, 64\n",
    "'''\n",
    "train_iter：PyTorch DataLoader对象，可迭代\n",
    "    每次迭代返回7个张量的元组（见上一问的print输出）\n",
    "    自动处理打乱（shuffle）和多进程加载\n",
    "vocab：Vocab对象\n",
    "    词元↔ID的双向映射\n",
    "    len(vocab)查看词表大小（通常~2万）\n",
    "    vocab['<mask>']获取特殊词元ID\n",
    "'''\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb22b86",
   "metadata": {
    "origin_pos": 7
   },
   "source": [
    "## 预训练BERT\n",
    "\n",
    "原始BERT`Devlin.Chang.Lee.ea.2018`有两个不同模型尺寸的版本。基本模型（$\\text{BERT}_{\\text{BASE}}$）使用12层（Transformer编码器块），768个隐藏单元（隐藏大小）和12个自注意头。大模型（$\\text{BERT}_{\\text{LARGE}}$）使用24层，1024个隐藏单元和16个自注意头。值得注意的是，前者有1.1亿个参数，后者有3.4亿个参数。为了便于演示，我们定义了一个小的BERT，使用了2层、128个隐藏单元和2个自注意头。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8798281b",
   "metadata": {},
   "source": [
    "| 参数                    | 值   | 含义                       |\n",
    "| --------------------- | --- | ------------------------ |\n",
    "| `len(vocab)`          | ~2万 | 词汇表大小（动态获取）              |\n",
    "| `num_hiddens=128`     | 128 | **隐藏层维度**（原始BERT是768）    |\n",
    "| `norm_shape=[128]`    | 128 | LayerNorm的归一化维度          |\n",
    "| `ffn_num_input=128`   | 128 | FFN输入维度（通常=num\\_hiddens） |\n",
    "| `ffn_num_hiddens=256` | 256 | **FFN中间层维度**（原始是3072）    |\n",
    "| `num_heads=2`         | 2   | **注意力头数**（原始是12）         |\n",
    "| `num_layers=2`        | 2   | **Transformer层数**（原始是12） |\n",
    "| `dropout=0.2`         | 0.2 | Dropout比率（原始是0.1）        |\n",
    "| `key_size=128`        | 128 | 注意力键维度（通常=num\\_hiddens）  |\n",
    "| `query_size=128`      | 128 | 注意力查询维度                  |\n",
    "| `value_size=128`      | 128 | 注意力值维度                   |\n",
    "| `hid_in_features=128` | 128 | NSP分类器输入维度               |\n",
    "| `mlm_in_features=128` | 128 | MLM预测头输入维度               |\n",
    "| `nsp_in_features=128` | 128 | NSP预测头输入维度               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cc34825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:04:38.461166Z",
     "iopub.status.busy": "2023-08-18T07:04:38.460802Z",
     "iopub.status.idle": "2023-08-18T07:04:38.581653Z",
     "shell.execute_reply": "2023-08-18T07:04:38.580139Z"
    },
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "'''\n",
    "对比：原始BERT-base\n",
    "num_hiddens=768      # 这里是128（缩小6倍）\n",
    "ffn_num_hiddens=3072 # 这里是256（缩小12倍）\n",
    "num_heads=12         # 这里是2（缩小6倍）\n",
    "num_layers=12        # 这里是2（缩小6倍）\n",
    "'''\n",
    "net = d2l.BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],\n",
    "                    ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,\n",
    "                    num_layers=2, dropout=0.2, key_size=128, query_size=128,\n",
    "                    value_size=128, hid_in_features=128, mlm_in_features=128,\n",
    "                    nsp_in_features=128)\n",
    "'''\n",
    "设备配置\n",
    "作用：获取所有可用的GPU列表\n",
    "返回值：\n",
    "    单GPU：[device(type='cuda',index=0)]\n",
    "    多GPU：[device('cuda:0'),device('cuda:1'),...]\n",
    "    无GPU：[device(type='cpu')]\n",
    "'''\n",
    "devices = d2l.try_all_gpus()\n",
    "'''\n",
    "损失函数定义\n",
    "BERT预训练同时计算两个损失：\n",
    "    MLM损失：预测被遮蔽的词元\n",
    "    NSP损失：判断是否为下一句\n",
    "'''\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be063421",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "在定义训练代码实现之前，我们定义了一个辅助函数`_get_batch_loss_bert`。给定训练样本，该函数计算遮蔽语言模型和下一句子预测任务的损失。请注意，BERT预训练的最终损失是遮蔽语言模型损失和下一句预测损失的和。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493cd324",
   "metadata": {},
   "source": [
    "| 参数                         | 形状示例        | 含义                 |\n",
    "| -------------------------- | ----------- | ------------------ |\n",
    "| `tokens_X`                 | `[512, 64]` | 批次词元ID（512样本×64长度） |\n",
    "| `segments_X`               | `[512, 64]` | 0/1标记，区分句子A/B      |\n",
    "| `valid_lens_x.reshape(-1)` | `[512]`     | 每个样本的真实长度（不含pad）   |\n",
    "| `pred_positions_X`         | `[512, 9]`  | MLM任务中需要预测的位置索引    |\n",
    "\n",
    "<BR>\n",
    "\n",
    "| 返回值         | 形状示例                   | 含义                            |\n",
    "| ----------- | ---------------------- | ----------------------------- |\n",
    "| `_`（忽略）     | `[512, 64, 128]`       |  编码后的序列表示 （后续任务不需要）   |\n",
    "| `mlm_Y_hat` | `[512, 9, vocab_size]` |  MLM预测 ：每个预测位置的词表概率   |\n",
    "| `nsp_Y_hat` | `[512, 2]`             |  NSP预测 ：二分类概率（是/否下一句） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fb193",
   "metadata": {},
   "source": [
    "**完整数据流示例**\n",
    "\n",
    "```Python\n",
    "# 输入数据（一个批次）\n",
    "tokens_X = torch.tensor([[101, 1920, 103, 6207, 102, 0, 0, 0],\n",
    "                         [101, 2603, 4263, 103, 102, 0, 0, 0]])  # [2, 8]\n",
    "segments_X = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 0]])  # [2, 8]\n",
    "valid_lens_x = torch.tensor([5, 5])  # [2]\n",
    "pred_positions_X = torch.tensor([[2, 0], [3, 0]])  # [2, 2]\n",
    "\n",
    "# 前向传播\n",
    "_, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X, valid_lens_x, pred_positions_X)\n",
    "\n",
    "# 输出\n",
    "# mlm_Y_hat.shape = [2, 2, vocab_size]  \n",
    "#   → 2个样本，每个2个预测位置，每个位置vocab_size个概率\n",
    "\n",
    "# nsp_Y_hat.shape = [2, 2]\n",
    "#   → 2个样本，每个2个类别（是/否下一句）\n",
    "```\n",
    "**为什么不用编码输出？**\n",
    "\n",
    "```Python\n",
    "# 编码输出用于微调（Fine-tuning）\n",
    "encoded_X, mlm_Y_hat, nsp_Y_hat = net(...)\n",
    "\n",
    "# 示例：文本分类微调\n",
    "cls_output = encoded_X[:, 0, :]  # 取<cls>位置的表示\n",
    "logits = classifier(cls_output)   # 接一个分类头\n",
    "在预训练阶段：我们只关注MLM和NSP的损失，不需要主输出。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b51c53",
   "metadata": {},
   "source": [
    "| 参数                 | 类型                    | 形状示例        | 含义                  |\n",
    "| ------------------ | --------------------- | ----------- | ------------------- |\n",
    "| `net`              | `nn.Module`           | -           | BERT模型实例            |\n",
    "| `loss`             | `nn.CrossEntropyLoss` | -           | 损失函数对象              |\n",
    "| `vocab_size`       | `int`                 | 约2万         | 词汇表大小（用于reshape）    |\n",
    "| `tokens_X`         | `Tensor`              | `[512, 64]` | 批次词元ID              |\n",
    "| `segments_X`       | `Tensor`              | `[512, 64]` | 段落标记（0/1）           |\n",
    "| `valid_lens_x`     | `Tensor`              | `[512]`     | 有效长度（不含pad）         |\n",
    "| `pred_positions_X` | `Tensor`              | `[512, 9]`  | MLM预测位置             |\n",
    "| `mlm_weights_X`    | `Tensor`              | `[512, 9]`  | **权重（1.0/0.0屏蔽填充）** |\n",
    "| `mlm_Y`            | `Tensor`              | `[512, 9]`  | **MLM真实标签**         |\n",
    "| `nsp_y`            | `Tensor`              | `[512]`     | **NSP真实标签**（0/1）    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64b2c84b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:04:38.586837Z",
     "iopub.status.busy": "2023-08-18T07:04:38.585868Z",
     "iopub.status.idle": "2023-08-18T07:04:38.594572Z",
     "shell.execute_reply": "2023-08-18T07:04:38.593478Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
    "                         segments_X, valid_lens_x,\n",
    "                         pred_positions_X, mlm_weights_X,\n",
    "                         mlm_Y, nsp_y):\n",
    "    # 前向传播\n",
    "    '''\n",
    "    关键：valid_lens_x.reshape(-1)\n",
    "        原始形状：[512,1]或[512]\n",
    "        reshape(-1)→[512]（确保是一维）\n",
    "        作用：告诉Transformer哪些位置是有效词元（用于Attention屏蔽pad）\n",
    "     为什么用_忽略第一个返回值？\n",
    "        BERT模型返回3个值：\n",
    "            encoded_X：所有词元的上下文表示（用于微调）\n",
    "            mlm_Y_hat：遮蔽词预测（当前任务需要）\n",
    "            nsp_Y_hat：下一句预测（当前任务需要）\n",
    "            在预训练阶段，我们只关心后两个（MLM和NSP损失），所以用_忽略主输出\n",
    "    '''\n",
    "\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, # 输入词元ID\n",
    "                                  segments_X, # 输入词元ID\n",
    "                                  valid_lens_x.reshape(-1), # 有效长度（reshape是为了适配API）\n",
    "                                  pred_positions_X) # 需要预测的位置\n",
    "    '''\n",
    "    计算遮蔽语言模型损失\n",
    "    模型返回：编码输出（忽略）、MLM预测（[batch,max_pred,vocab_size]）、NSP预测（[batch,2]）\n",
    "    MLM损失计算\n",
    "    形状变换：\n",
    "        mlm_Y_hat：从[32,9,vocab_size]→[288,vocab_size]（288=32×9）\n",
    "        mlm_Y：从[32,9]→[288]\n",
    "        mlm_weights_X：从[32,9]→[288,1]（用于广播）\n",
    "    关键点：乘以mlm_weights_X\n",
    "        真实预测位置：权重=1.0，损失保留\n",
    "        填充位置：权重=0.0，损失被屏蔽\n",
    "    '''\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
    "    mlm_weights_X.reshape(-1, 1)\n",
    "    '''\n",
    "    MLM损失平均\n",
    "    为什么这样平均？：只计算真实预测位置的平均损失\n",
    "        mlm_weights_X.sum()：统计真实预测数量（如18.0）\n",
    "        1e-8：防止除零（极端情况：batch中无预测位置）\n",
    "    '''\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    '''\n",
    "    计算下一句子预测任务的损失\n",
    "    NSP损失计算\n",
    "    二分类任务：nsp_Y_hat形状 [batch,2]，nsp_y形状[batch]\n",
    "    直接计算：无填充问题，无需权重\n",
    "    '''\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    # 总损失：直接相加：两个任务同等重要（原始BERT设计）；返回值：用于反向传播\n",
    "    l = mlm_l + nsp_l\n",
    "    '''\n",
    "    返回结果\n",
    "    三个标量损失值，用于监控训练过程：\n",
    "        mlm_l：MLM任务损失\n",
    "        nsp_l：NSP任务损失\n",
    "        l：总损失\n",
    "    '''\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e553304",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "通过调用上述两个辅助函数，下面的`train_bert`函数定义了在WikiText-2（`train_iter`）数据集上预训练BERT（`net`）的过程。训练BERT可能需要很长时间。以下函数的输入`num_steps`指定了训练的迭代步数，而不是像`train_ch13`函数那样指定训练的轮数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef4adc",
   "metadata": {},
   "source": [
    "| 参数名          | 类型                    | 示例值                     | 含义                        |\n",
    "| ------------ | --------------------- | ----------------------- | ------------------------- |\n",
    "| `train_iter` | `DataLoader`          | `train_iter`            | **训练数据迭代器**（批量加载数据）       |\n",
    "| `net`        | `nn.Module`           | `BERTModel(...)`        | **BERT模型实例**              |\n",
    "| `loss`       | `nn.CrossEntropyLoss` | `nn.CrossEntropyLoss()` | **损失函数**（MLM和NSP共用）       |\n",
    "| `vocab_size` | `int`                 | `len(vocab)` ≈ 17964    | **词汇表大小**（用于MLM输出reshape） |\n",
    "| `devices`    | `List[torch.device]`  | `[cuda:0, cuda:1]`      | **GPU设备列表**（用于并行训练）       |\n",
    "| `num_steps`  | `int`                 | `1000`                  | **总训练步数**（BERT原始论文用100万步） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489bddf",
   "metadata": {},
   "source": [
    "**Q: 为什么不用 for epoch in range(num_epochs)？**\n",
    "\n",
    "A: BERT预训练通常按步数（100万步）而非epoch，确保数据遍历的随机性。\n",
    "\n",
    "**Q: metric[0] / metric[3] 是什么？**\n",
    "\n",
    "A: metric[0]是MLM损失总和，metric[3]是步数，相除得平均每步MLM损失。\n",
    "\n",
    "**Q: timer.stop() 在循环内还是外？**\n",
    "\n",
    "A: 在循环内，但d2l.Timer会自动累积时间，最终timer.sum()返回总时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf154eb",
   "metadata": {},
   "source": [
    "| 循环          | 作用          | 终止条件                            |\n",
    "| ----------- | ----------- | ------------------------------- |\n",
    "| **`while`** | 控制**总训练步数** | `step == num_steps`             |\n",
    "| **`for`**   | 遍历**数据集批次** | 数据集遍历完或`num_steps_reached=True` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b11ab",
   "metadata": {},
   "source": [
    "**Metric累积器**\n",
    "```python\n",
    "metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "#         ↑      ↑      ↑                ↑\n",
    "#         |      |      |                └─ 步数计数（用于平均）\n",
    "#         |      |      └─ 样本数（用于计算速度）\n",
    "#         |      └─ NSP损失总和\n",
    "#         └─ MLM损失总和\n",
    "```\n",
    "**计算方式：**\n",
    "\n",
    "- metric[0] / metric[3] = MLM平均损失\n",
    "- metric[1] / metric[3] = NSP平均损失\n",
    "- metric[2] / timer.sum() = 每秒处理样本数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944353e1",
   "metadata": {},
   "source": [
    "**完整时序图**\n",
    "```Python\n",
    "step = 0\n",
    "↓\n",
    "while step < 1000:\n",
    "    ↓\n",
    "    for batch in train_iter:  # 遍历数据集\n",
    "        ↓\n",
    "        # 处理一个批次\n",
    "        step = 1 → 更新 → animator → step = 2\n",
    "        step = 2 → 更新 → animator → step = 3\n",
    "        ...\n",
    "        step = 1000 → num_steps_reached = True → break\n",
    "        ↓\n",
    "    break  # 退出while循环\n",
    "    ↓\n",
    "打印最终损失和速度\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cd43502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:04:38.599431Z",
     "iopub.status.busy": "2023-08-18T07:04:38.598650Z",
     "iopub.status.idle": "2023-08-18T07:04:38.614756Z",
     "shell.execute_reply": "2023-08-18T07:04:38.613328Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    '''\n",
    "    1. 模型并行化\n",
    "    作用  ：将模型复制到所有可用的GPU上，实现数据并行训练\n",
    "    原理  ：每个GPU处理一部分batch数据，梯度自动汇总\n",
    "    设备0：作为主GPU，负责参数更新\n",
    "    '''\n",
    "    num_workers = 0\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    '''\n",
    "    2. 优化器设置\n",
    "    Adam优化器：自适应学习率，适合BERT这类大规模模型\n",
    "    lr=0.01：初始学习率（实际训练会配合warmup策略）\n",
    "    自动更新：反向传播后调用trainer.step()更新参数\n",
    "    '''\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    '''\n",
    "    3. 计时器与可视化\n",
    "    step：记录已训练的步数\n",
    "    timer：计时器，计算每秒处理多少样本\n",
    "    animator：实时绘制损失曲线（动态更新）\n",
    "    '''\n",
    "    step, timer = 0, d2l.Timer()\n",
    "    animator = d2l.Animator(xlabel='step', ylabel='loss',\n",
    "                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n",
    "    # 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数\n",
    "    '''\n",
    "    4. 累积器\n",
    "    4个槽位分别累积：\n",
    "        mlm_l：MLM损失总和\n",
    "        nsp_l：NSP损失总和\n",
    "        tokens_X.shape[0]：样本数量（用于计算速度）\n",
    "        1：步数计数（用于平均）\n",
    "\n",
    "    '''\n",
    "    metric = d2l.Accumulator(4)\n",
    "    num_steps_reached = False\n",
    "    # 5. 主训练循环\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        # 数据加载\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
    "            mlm_weights_X, mlm_Y, nsp_y in train_iter: # 遍历所有批次\n",
    "            # 数据移动到GPU\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_x = valid_lens_x.to(devices[0])\n",
    "            pred_positions_X = pred_positions_X.to(devices[0])\n",
    "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
    "            # 梯度清零\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "            # 前向传播 + 计算损失\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
    "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
    "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            # 反向传播\n",
    "            l.backward()\n",
    "            # 更新参数\n",
    "            trainer.step()\n",
    "            # 累积统计\n",
    "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "            timer.stop()\n",
    "            '''\n",
    "            每步更新可视化\n",
    "            实时绘图：X轴=step，Y轴=loss\n",
    "            双曲线：显示MLM和NSP损失\n",
    "            平滑：使用累积平均值，避免单步抖动\n",
    "            '''\n",
    "            animator.add(step + 1,\n",
    "                         (metric[0] / metric[3], metric[1] / metric[3]))\n",
    "            step += 1\n",
    "            # 训练终止条件\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "    '''\n",
    "    最终输出\n",
    "        平均损失：MLM和NSP的每步平均损失\n",
    "        训练速度：每秒处理多少句子对（衡量GPU利用率）\n",
    "        设备信息：显示使用的GPU列表（如[cuda:0, cuda:1]）\n",
    "    '''\n",
    "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
    "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08640bff",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "在预训练过程中，我们可以绘制出遮蔽语言模型损失和下一句预测损失。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35e856a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:04:38.619952Z",
     "iopub.status.busy": "2023-08-18T07:04:38.619192Z",
     "iopub.status.idle": "2023-08-18T07:05:00.659514Z",
     "shell.execute_reply": "2023-08-18T07:05:00.658404Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM loss 5.569, NSP loss 0.727\n",
      "5854.1 sentence pairs/sec on [device(type='cuda', index=0)]\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"243.103125pt\" height=\"183.35625pt\" viewBox=\"0 0 243.103125 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2026-01-07T16:18:19.003356</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 243.103125 183.35625 \n",
       "L 243.103125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 34.240625 145.8 \n",
       "L 229.540625 145.8 \n",
       "L 229.540625 7.2 \n",
       "L 34.240625 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 70.112054 145.8 \n",
       "L 70.112054 7.2 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m30692e693c\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m30692e693c\" x=\"70.112054\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(63.749554 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 109.969196 145.8 \n",
       "L 109.969196 7.2 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m30692e693c\" x=\"109.969196\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 20 -->\n",
       "      <g transform=\"translate(103.606696 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 149.826339 145.8 \n",
       "L 149.826339 7.2 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m30692e693c\" x=\"149.826339\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 30 -->\n",
       "      <g transform=\"translate(143.463839 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 189.683482 145.8 \n",
       "L 189.683482 7.2 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m30692e693c\" x=\"189.683482\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 40 -->\n",
       "      <g transform=\"translate(183.320982 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 229.540625 145.8 \n",
       "L 229.540625 7.2 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m30692e693c\" x=\"229.540625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(223.178125 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- step -->\n",
       "     <g transform=\"translate(121.075 174.076563)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"52.099609\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"91.308594\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"152.832031\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 34.240625 132.951784 \n",
       "L 229.540625 132.951784 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"m62d452a1f6\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62d452a1f6\" x=\"34.240625\" y=\"132.951784\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1 -->\n",
       "      <g transform=\"translate(20.878125 136.751002)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 34.240625 108.934843 \n",
       "L 229.540625 108.934843 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62d452a1f6\" x=\"34.240625\" y=\"108.934843\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 2 -->\n",
       "      <g transform=\"translate(20.878125 112.734062)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 34.240625 84.917903 \n",
       "L 229.540625 84.917903 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62d452a1f6\" x=\"34.240625\" y=\"84.917903\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 3 -->\n",
       "      <g transform=\"translate(20.878125 88.717122)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 34.240625 60.900962 \n",
       "L 229.540625 60.900962 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62d452a1f6\" x=\"34.240625\" y=\"60.900962\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 4 -->\n",
       "      <g transform=\"translate(20.878125 64.700181)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 34.240625 36.884022 \n",
       "L 229.540625 36.884022 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62d452a1f6\" x=\"34.240625\" y=\"36.884022\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(20.878125 40.683241)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 34.240625 12.867082 \n",
       "L 229.540625 12.867082 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m62d452a1f6\" x=\"34.240625\" y=\"12.867082\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 6 -->\n",
       "      <g transform=\"translate(20.878125 16.6663)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \n",
       "Q 1688 2584 1439 2293 \n",
       "Q 1191 2003 1191 1497 \n",
       "Q 1191 994 1439 701 \n",
       "Q 1688 409 2113 409 \n",
       "Q 2538 409 2786 701 \n",
       "Q 3034 994 3034 1497 \n",
       "Q 3034 2003 2786 2293 \n",
       "Q 2538 2584 2113 2584 \n",
       "z\n",
       "M 3366 4563 \n",
       "L 3366 3988 \n",
       "Q 3128 4100 2886 4159 \n",
       "Q 2644 4219 2406 4219 \n",
       "Q 1781 4219 1451 3797 \n",
       "Q 1122 3375 1075 2522 \n",
       "Q 1259 2794 1537 2939 \n",
       "Q 1816 3084 2150 3084 \n",
       "Q 2853 3084 3261 2657 \n",
       "Q 3669 2231 3669 1497 \n",
       "Q 3669 778 3244 343 \n",
       "Q 2819 -91 2113 -91 \n",
       "Q 1303 -91 875 529 \n",
       "Q 447 1150 447 2328 \n",
       "Q 447 3434 972 4092 \n",
       "Q 1497 4750 2381 4750 \n",
       "Q 2619 4750 2861 4703 \n",
       "Q 3103 4656 3366 4563 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-36\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798438 86.157813)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_23\">\n",
       "    <path d=\"M 34.240625 22.436411 \n",
       "L 38.226339 23.96669 \n",
       "L 42.212054 17.722885 \n",
       "L 46.197768 13.5 \n",
       "L 50.183482 14.655514 \n",
       "L 54.169196 16.439897 \n",
       "L 58.154911 17.689022 \n",
       "L 62.140625 18.868741 \n",
       "L 66.126339 19.83801 \n",
       "L 70.112054 20.60755 \n",
       "L 74.097768 20.828804 \n",
       "L 78.083482 20.368496 \n",
       "L 82.069196 20.08892 \n",
       "L 86.054911 19.993449 \n",
       "L 90.040625 20.414018 \n",
       "L 94.026339 20.680007 \n",
       "L 98.012054 20.753532 \n",
       "L 101.997768 20.745324 \n",
       "L 105.983482 20.765006 \n",
       "L 109.969196 20.738571 \n",
       "L 113.954911 20.720105 \n",
       "L 117.940625 20.844149 \n",
       "L 121.926339 20.894158 \n",
       "L 125.912054 20.994404 \n",
       "L 129.897768 21.117966 \n",
       "L 133.883482 21.218489 \n",
       "L 137.869196 21.351097 \n",
       "L 141.854911 21.399946 \n",
       "L 145.840625 21.534662 \n",
       "L 149.826339 21.633678 \n",
       "L 153.812054 21.751599 \n",
       "L 157.797768 21.866842 \n",
       "L 161.783482 21.920003 \n",
       "L 165.769196 22.046989 \n",
       "L 169.754911 22.139926 \n",
       "L 173.740625 22.283286 \n",
       "L 177.726339 22.365351 \n",
       "L 181.712054 22.398199 \n",
       "L 185.697768 22.494267 \n",
       "L 189.683482 22.54615 \n",
       "L 193.669196 22.606411 \n",
       "L 197.654911 22.655857 \n",
       "L 201.640625 22.750604 \n",
       "L 205.626339 22.821183 \n",
       "L 209.612054 22.893219 \n",
       "L 213.597768 22.945031 \n",
       "L 217.583482 22.985889 \n",
       "L 221.569196 23.05973 \n",
       "L 225.554911 23.149082 \n",
       "L 229.540625 23.227568 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_24\">\n",
       "    <path d=\"M 34.240625 137.595835 \n",
       "L 38.226339 131.533818 \n",
       "L 42.212054 133.696662 \n",
       "L 46.197768 134.587591 \n",
       "L 50.183482 135.272388 \n",
       "L 54.169196 136.094037 \n",
       "L 58.154911 136.550971 \n",
       "L 62.140625 136.812505 \n",
       "L 66.126339 137.144857 \n",
       "L 70.112054 137.432006 \n",
       "L 74.097768 137.6268 \n",
       "L 78.083482 137.790028 \n",
       "L 82.069196 137.95916 \n",
       "L 86.054911 138.081704 \n",
       "L 90.040625 138.164131 \n",
       "L 94.026339 138.287749 \n",
       "L 98.012054 138.404947 \n",
       "L 101.997768 138.509318 \n",
       "L 105.983482 138.594828 \n",
       "L 109.969196 138.644535 \n",
       "L 113.954911 138.708215 \n",
       "L 117.940625 138.766549 \n",
       "L 121.926339 138.829811 \n",
       "L 125.912054 138.872911 \n",
       "L 129.897768 138.912053 \n",
       "L 133.883482 138.964483 \n",
       "L 137.869196 139.004409 \n",
       "L 141.854911 139.03105 \n",
       "L 145.840625 139.068567 \n",
       "L 149.826339 139.10971 \n",
       "L 153.812054 139.148796 \n",
       "L 157.797768 139.169404 \n",
       "L 161.783482 139.19055 \n",
       "L 165.769196 139.22492 \n",
       "L 169.754911 139.255307 \n",
       "L 173.740625 139.280932 \n",
       "L 177.726339 139.300795 \n",
       "L 181.712054 139.325078 \n",
       "L 185.697768 139.349953 \n",
       "L 189.683482 139.361574 \n",
       "L 193.669196 139.37443 \n",
       "L 197.654911 139.394455 \n",
       "L 201.640625 139.410232 \n",
       "L 205.626339 139.421107 \n",
       "L 209.612054 139.434357 \n",
       "L 213.597768 139.453426 \n",
       "L 217.583482 139.47101 \n",
       "L 221.569196 139.473119 \n",
       "L 225.554911 139.482851 \n",
       "L 229.540625 139.5 \n",
       "\" clip-path=\"url(#p6e0f92054f)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 34.240625 145.8 \n",
       "L 34.240625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 229.540625 145.8 \n",
       "L 229.540625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 34.240625 145.8 \n",
       "L 229.540625 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 34.240625 7.2 \n",
       "L 229.540625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 168.28125 92.678125 \n",
       "L 222.540625 92.678125 \n",
       "Q 224.540625 92.678125 224.540625 90.678125 \n",
       "L 224.540625 62.321875 \n",
       "Q 224.540625 60.321875 222.540625 60.321875 \n",
       "L 168.28125 60.321875 \n",
       "Q 166.28125 60.321875 166.28125 62.321875 \n",
       "L 166.28125 90.678125 \n",
       "Q 166.28125 92.678125 168.28125 92.678125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_25\">\n",
       "     <path d=\"M 170.28125 68.420313 \n",
       "L 180.28125 68.420313 \n",
       "L 190.28125 68.420313 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- mlm -->\n",
       "     <g transform=\"translate(198.28125 71.920313)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6d\" d=\"M 3328 2828 \n",
       "Q 3544 3216 3844 3400 \n",
       "Q 4144 3584 4550 3584 \n",
       "Q 5097 3584 5394 3201 \n",
       "Q 5691 2819 5691 2113 \n",
       "L 5691 0 \n",
       "L 5113 0 \n",
       "L 5113 2094 \n",
       "Q 5113 2597 4934 2840 \n",
       "Q 4756 3084 4391 3084 \n",
       "Q 3944 3084 3684 2787 \n",
       "Q 3425 2491 3425 1978 \n",
       "L 3425 0 \n",
       "L 2847 0 \n",
       "L 2847 2094 \n",
       "Q 2847 2600 2669 2842 \n",
       "Q 2491 3084 2119 3084 \n",
       "Q 1678 3084 1418 2786 \n",
       "Q 1159 2488 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1356 3278 1631 3431 \n",
       "Q 1906 3584 2284 3584 \n",
       "Q 2666 3584 2933 3390 \n",
       "Q 3200 3197 3328 2828 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"97.412109\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6d\" x=\"125.195312\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_26\">\n",
       "     <path d=\"M 170.28125 83.098438 \n",
       "L 180.28125 83.098438 \n",
       "L 190.28125 83.098438 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_15\">\n",
       "     <!-- nsp -->\n",
       "     <g transform=\"translate(198.28125 86.598438)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"63.378906\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"115.478516\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p6e0f92054f\">\n",
       "   <rect x=\"34.240625\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_bert(train_iter, # train_iter=数据加载器：提供批次数据\n",
    "           net, # net=模型：接收数据并预测\n",
    "           loss, # loss=损失函数：计算预测与真实的差距\n",
    "           len(vocab), # vocab_size=词表大小：用于MLM的reshape\n",
    "           devices, # devices=GPU列表：决定并行训练的设备\n",
    "           50) # num_steps=训练步数：控制训练何时结束"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede604ea",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "## 用BERT表示文本\n",
    "\n",
    "在预训练BERT之后，我们可以用它来表示单个文本、文本对或其中的任何词元。下面的函数返回`tokens_a`和`tokens_b`中所有词元的BERT（`net`）表示。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381860bc",
   "metadata": {},
   "source": [
    "| 参数         | 类型          | 示例                     | 含义           |\n",
    "| ---------- | ----------- | ---------------------- | ------------ |\n",
    "| `net`      | `nn.Module` | 训练好的BERT模型             | **BERT模型实例** |\n",
    "| `tokens_a` | `List[str]` | `['i', 'love', 'nlp']` | 第一句的词元列表     |\n",
    "| `tokens_b` | `List[str]` | `['it', 'is', 'fun']`  | 第二句的词元列表（可选） |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77f3b8e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:00.663916Z",
     "iopub.status.busy": "2023-08-18T07:05:00.663281Z",
     "iopub.status.idle": "2023-08-18T07:05:00.669609Z",
     "shell.execute_reply": "2023-08-18T07:05:00.668549Z"
    },
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
    "    '''\n",
    "    1. 构建BERT输入格式\n",
    "    功能：将词元转换为BERT标准输入格式\n",
    "    输出：\n",
    "        tokens：['<cls>','i','love','nlp','<sep>','it','is','fun','<sep>']\n",
    "        segments：[0,0,0,0,0,1,1,1,1]\n",
    "    '''\n",
    "    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "    '''\n",
    "    2. 转换为词元ID（增加batch维度）\n",
    "    unsqueeze(0)：增加batch维度，从[seq_len]→[1,seq_len]\n",
    "    devices[0]：移动到主GPU（如cuda:0）\n",
    "    valid_len：有效长度（告诉模型哪些位置是有效词元，用于Attention屏蔽pad）\n",
    "    '''\n",
    "    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n",
    "    '''\n",
    "    3. 前向传播（只获取编码表示）\n",
    "    调用方式：不传入pred_positions，表示不需要MLM和NSP预测\n",
    "    返回：\n",
    "        encoded_X：最后一层的隐藏状态，形状[1,seq_len,hidden_size]\n",
    "        _：MLM预测（忽略）\n",
    "        _：NSP预测（忽略）\n",
    "    '''\n",
    "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
    "    '''\n",
    "    4. 返回编码\n",
    "    形状：[1,seq_len,num_hiddens]（如[1,9,128]）\n",
    "    用途：用于下游任务（分类、问答等）\n",
    "    '''\n",
    "    return encoded_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0697e",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "考虑“a crane is flying”这句话。回想一下`subsec_bert_input_rep`中讨论的BERT的输入表示。插入特殊标记“&lt;cls&gt;”（用于分类）和“&lt;sep&gt;”（用于分隔）后，BERT输入序列的长度为6。因为零是“&lt;cls&gt;”词元，`encoded_text[:, 0, :]`是整个输入语句的BERT表示。为了评估一词多义词元“crane”，我们还打印出了该词元的BERT表示的前三个元素。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57bf82",
   "metadata": {},
   "source": [
    "| 索引 | 词元       | 编码表示                    | 用途            |\n",
    "| -- | -------- | ----------------------- | ------------- |\n",
    "| 0  | `<cls>`  | `encoded_text_cls`      | **句子级表示**（分类） |\n",
    "| 1  | `a`      | `encoded_text[:, 1, :]` | 冠词表示          |\n",
    "| 2  | `crane`  | `encoded_text_crane`    | **词元表示**（当前词） |\n",
    "| 3  | `is`     | `encoded_text[:, 3, :]` | 动词表示          |\n",
    "| 4  | `flying` | `encoded_text[:, 4, :]` | 动词表示          |\n",
    "| 5  | `<sep>`  | `encoded_text[:, 5, :]` | 分隔符表示         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763864e8",
   "metadata": {},
   "source": [
    "| 变量                   | 形状            | 含义                        |\n",
    "| -------------------- | ------------- | ------------------------- |\n",
    "| `encoded_text`       | `[1, 6, 128]` | **完整序列编码**（所有词元）          |\n",
    "| `encoded_text_cls`   | `[1, 128]`    | 句子表示 （<cls>位置）    |\n",
    "| `encoded_text_crane` | `[1, 128]`    |  词元表示 （\"crane\"位置） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f4249",
   "metadata": {},
   "source": [
    "**1️⃣ encoded_text.shape**\n",
    "\n",
    "```Python\n",
    "encoded_text.shape  # 输出示例: torch.Size([1, 6, 128])\n",
    "返回：完整编码张量的形状\n",
    "含义：\n",
    "1：batch大小（1个句子）\n",
    "6：词元数量（<cls> a crane is flying <sep>）\n",
    "128：隐藏层维度（每个词元的向量维度）\n",
    "```\n",
    "\n",
    "**2️⃣ encoded_text_cls.shape**\n",
    "\n",
    "```Python\n",
    "encoded_text_cls.shape  # 输出示例: torch.Size([1, 128])\n",
    "返回：<cls> 位置的编码的形状\n",
    "含义：\n",
    "1：batch大小\n",
    "128：句子级表示的维度（用于分类任务）\n",
    "```\n",
    "\n",
    "**3️⃣ encoded_text_crane[0][:3]**\n",
    "\n",
    "```Python\n",
    "encoded_text_crane[0][:3]  # 输出示例: tensor([ 0.123, -0.456,  0.789])\n",
    "```\n",
    "**这是最复杂的双重索引操作：**\n",
    "\n",
    "**第一步：[0]**\n",
    "\n",
    "```Python\n",
    "encoded_text_crane = encoded_text[:, 2, :]  # 形状: [1, 128]\n",
    "encoded_text_crane[0]  # 形状: [128]（取出batch中第0个样本）\n",
    "作用：去掉batch维度，得到纯向量\n",
    "```\n",
    "\n",
    "**第二步：[:3]**\n",
    "\n",
    "```Python\n",
    "encoded_text_crane[0][:3]  # 形状: [3]（取前3个数值）\n",
    "作用：取出向量的前3个维度（用于快速查看数值）\n",
    "```\n",
    "\n",
    "**实际含义**\n",
    "| 代码                          | 用途       | 相当于查看           |\n",
    "| --------------------------- | -------- | --------------- |\n",
    "| `encoded_text.shape`        | **整体结构** | 数据维度是否匹配        |\n",
    "| `encoded_text_cls.shape`    | **句子表示** | 分类任务的输入维度       |\n",
    "| `encoded_text_crane[0][:3]` | **具体数值** | 词向量是否合理（有无NAN等） |\n",
    "\n",
    "<br>\n",
    "\n",
    "```Python\n",
    "# 原始张量: [1, 6, 128]\n",
    "#         ↑   ↑   ↑\n",
    "#       batch seq dim\n",
    "\n",
    "encoded_text_cls = encoded_text[:, 0, :]  # [1, 128]（第0个位置）\n",
    "\n",
    "encoded_text_crane = encoded_text[:, 2, :]  # [1, 128]（第2个位置）\n",
    "\n",
    "encoded_text_crane[0]  # [128]（去掉batch）\n",
    "\n",
    "encoded_text_crane[0][:3]  # [3]（只看前3个数值）\n",
    "```\n",
    "\n",
    "- .shape：确认张量结构\n",
    "- [0]：去掉batch维度\n",
    "- [:3]：快速查看数值范围"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc896607",
   "metadata": {},
   "source": [
    "```python\n",
    "encoded_text[:, 2, :]              # [1, 128]  ← 2维张量\n",
    "            ↓ [0]\n",
    "encoded_text_crane[0]              # [128]     ← 1维张量\n",
    "            ↓ [:3]\n",
    "encoded_text_crane[0][:3]          # [3]       ← 1维张量（更短）\n",
    "```\n",
    "- **[0] ：在维度0上取第0个元素 ，结果维度减1**\n",
    "- 1：是形状的描述，不是数据本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1081fda9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:00.673428Z",
     "iopub.status.busy": "2023-08-18T07:05:00.672675Z",
     "iopub.status.idle": "2023-08-18T07:05:00.690133Z",
     "shell.execute_reply": "2023-08-18T07:05:00.689347Z"
    },
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 128]),\n",
       " torch.Size([1, 128]),\n",
       " tensor([2.0407, 0.2718, 2.4522], device='cuda:0', grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.  输入词元\n",
    "原始句子：\"a crane is flying\"\n",
    "词元化：拆分为单词列表\n",
    "后续处理：get_bert_encoding会自动添加<cls>和<sep>\n",
    "'''\n",
    "tokens_a = ['a', 'crane', 'is', 'flying']\n",
    "'''\n",
    "2.  获取完整编码：将句子输入BERT，返回最后一层隐藏状态\n",
    "内部变换：\n",
    "    输入词元→['<cls>','a','crane','is','flying','<sep>']\n",
    "    序列长度：6\n",
    "    encoded_text形状：[1,6,hidden_size]（如[1,6,128]）\n",
    "维度0(batch)：1（1个样本）\n",
    "维度1(sequence)：6（6个词元位置）\n",
    "维度2(feature)：128（每个词元的128维向量）\n",
    "'''\n",
    "encoded_text = get_bert_encoding(net, tokens_a)\n",
    "# 词元：'<cls>','a','crane','is','flying','<sep>'\n",
    "'''\n",
    "3. 提取<cls>\n",
    "索引：[:,0,:]→取所有batch中第0个位置的向量\n",
    "对应词元：<cls>（BERT句首的特殊标记）\n",
    "用途：作为整个句子的语义表示（用于分类任务）\n",
    "形状：[1,128]（句子向量）\n",
    "'''\n",
    "encoded_text_cls = encoded_text[:, 0, :]\n",
    "'''\n",
    "4. 提取'crane'位置的编码（词元表示）\n",
    "索引：[:,2,:]→取第2个位置（从0开始计数）\n",
    "对应词元：'crane'\n",
    "用途：获取特定词元的上下文语义（这里是\"鹤\"的表示）\n",
    "形状：[1,128]（词元向量）\n",
    "'''\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "# 打印形状和数值\n",
    "encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a4e964b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始形状: torch.Size([1, 128])\n",
      "索引后形状: torch.Size([128])\n",
      "索引后内容: tensor([-0.5885, -1.8327,  1.1131, -1.3741,  0.3516,  0.7546,  0.6277,  0.9405,\n",
      "        -0.1464,  0.2532,  1.1525,  0.2386, -0.6403, -0.6194, -0.9736,  0.6938,\n",
      "         0.7303,  0.0992, -0.1656, -2.3645, -1.1288, -0.1972, -0.2952, -0.6164,\n",
      "        -0.0242, -0.2206,  1.2604,  0.0470,  0.4574, -1.3781, -0.4282,  1.5826,\n",
      "        -1.6755,  1.7820,  0.4934,  2.6043,  0.6473, -0.8092, -0.4783,  0.8481,\n",
      "        -1.0425, -1.3514,  1.0731,  1.6297, -0.4732, -0.1220, -0.7563, -0.3926,\n",
      "         0.5917, -1.1796,  0.5022,  0.7052,  0.4224, -1.5483,  1.3076, -1.9778,\n",
      "        -0.1583, -0.8052,  0.5081, -1.2020, -0.6026, -0.4723, -0.3457,  0.5757,\n",
      "        -0.5428, -1.0805,  0.2434,  0.0443,  1.2306, -1.4977, -0.6148,  1.4656,\n",
      "         0.4356,  0.5436, -0.3598, -1.0490,  0.0063,  0.2937, -0.8904, -1.2022,\n",
      "        -0.1572, -1.0594, -0.3172,  0.3986,  1.2105,  2.1909,  0.2386, -0.1031,\n",
      "         0.1857,  3.3664, -0.5066, -0.0348, -0.0569,  1.1733, -0.6947,  1.2174,\n",
      "        -0.8221, -1.3142, -0.7266,  0.6679,  0.6867,  1.3047,  1.1813,  1.2614,\n",
      "        -1.8549,  1.6233, -1.1220,  0.2883,  0.9635, -0.3387,  0.3613, -2.5347,\n",
      "        -0.1608, -0.6776, -0.3248,  0.7054, -2.0947, -0.5949, -0.0702, -1.1005,\n",
      "         0.0893, -0.1516,  2.1135,  0.0686, -0.0341,  0.4235, -1.2676,  0.3607])\n",
      "前3个: tensor([-0.5885, -1.8327,  1.1131])\n"
     ]
    }
   ],
   "source": [
    "# 创建模拟数据\n",
    "encoded_text = torch.randn(1, 6, 128)  # [batch, seq, hidden]\n",
    "encoded_text_crane = encoded_text[:, 2, :]  # [1, 128]\n",
    "\n",
    "print(\"原始形状:\", encoded_text_crane.shape)  # torch.Size([1, 128])\n",
    "\n",
    "# 索引操作\n",
    "result = encoded_text_crane[0]\n",
    "print(\"索引后形状:\", result.shape)  # torch.Size([128])\n",
    "print(\"索引后内容:\", result)        # tensor([...128个数值...])\n",
    "\n",
    "# 再取前3个\n",
    "final = result[:3]\n",
    "print(\"前3个:\", final)  # tensor([0.123, -0.456, 0.789])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ca198",
   "metadata": {
    "origin_pos": 27
   },
   "source": [
    "现在考虑一个句子“a crane driver came”和“he just left”。类似地，`encoded_pair[:, 0, :]`是来自预训练BERT的整个句子对的编码结果。注意，多义词元“crane”的前三个元素与上下文不同时的元素不同。这支持了BERT表示是上下文敏感的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b881bd50",
   "metadata": {},
   "source": [
    "| 代码                                           | 位置索引 | 对应词元    | 形状         | 含义                    |\n",
    "| -------------------------------------------- | ---- | ------- | ---------- | --------------------- |\n",
    "| `encoded_pair_cls = encoded_pair[:, 0, :]`   | 0    | `<cls>` | `[1, 128]` | **句子对级表示**（用于NSP）     |\n",
    "| `encoded_pair_crane = encoded_pair[:, 2, :]` | 2    | `crane` | `[1, 128]` | **\"起重机\"的词元表示**（上下文相关） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ddac4f",
   "metadata": {},
   "source": [
    "**关键：上下文语义差异**\n",
    "\n",
    "**对比单句任务：**\n",
    "\n",
    "**单句 ：\"a crane is flying\" → \"crane\"指\" 鹤**\"（鸟）\n",
    "\n",
    "**句子对 ：\"a crane driver came\" → \"crane\"指\" 起重机**\"（机械）\n",
    "\n",
    "BERT会生成不同的编码向量，体现上下文敏感能力，这是相比Word2Vec的巨大进步。\n",
    "\n",
    "**位置索引映射**\n",
    "```Python\n",
    "Index:    0      1      2       3       4       5      6     7       8      9\n",
    "Token:  <cls>    a   crane driver   came   <sep>    he   just   left   <sep>\n",
    "Segment:   0      0      0       0       0       0      1      1       1      1\n",
    "          └─句子A─────────────────────┘      └─句子B─────────────────┘\n",
    "[:, 0, :] → <cls>向量（用于NSP分类）\n",
    "[:, 2, :] → \"crane\"向量（词义消歧）\n",
    "[:, 6, :] → \"he\"向量（第二个句子起始）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "960c3aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:05:00.694637Z",
     "iopub.status.busy": "2023-08-18T07:05:00.694061Z",
     "iopub.status.idle": "2023-08-18T07:05:00.708881Z",
     "shell.execute_reply": "2023-08-18T07:05:00.707778Z"
    },
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 128]),\n",
       " torch.Size([1, 128]),\n",
       " tensor([2.1561, 0.2198, 2.5821], device='cuda:0', grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 输入数据：任务场景：NSP（下一句预测），判断B是否是A的下一句\n",
    "# 句子A：\"一个起重机司机来了\"\n",
    "# 句子B：\"他刚离开\"\n",
    "tokens_a, tokens_b = ['a','crane','driver','came'],['he','just','left']\n",
    "'''\n",
    "2. 获取句子对编码\n",
    "内部生成的词元序列：['<cls>','a','crane','driver','came','<sep>','he','just','left','<sep>']\n",
    "编码维度：[1,10,128]\n",
    "    1：batch大小\n",
    "    10：总词元数（<cls>+4个A词+<sep>+3个B词+<sep>）\n",
    "    128：隐藏层维度\n",
    "'''\n",
    "encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n",
    "# 词元：'<cls>','a','crane','driver','came','<sep>','he','just',\n",
    "# 'left','<sep>'\n",
    "# 3. 提取表示\n",
    "encoded_pair_cls = encoded_pair[:, 0, :]\n",
    "encoded_pair_crane = encoded_pair[:, 2, :]\n",
    "# 打印结果\n",
    "encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
