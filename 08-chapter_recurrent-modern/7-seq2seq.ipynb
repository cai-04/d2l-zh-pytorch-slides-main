{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd68b087",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#  序列到序列学习（seq2seq）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9aa4b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:31.967521Z",
     "iopub.status.busy": "2023-08-18T07:15:31.966534Z",
     "iopub.status.idle": "2023-08-18T07:15:33.959337Z",
     "shell.execute_reply": "2023-08-18T07:15:33.958486Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d556f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "实现循环神经网络编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbfb3ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:33.963601Z",
     "iopub.status.busy": "2023-08-18T07:15:33.962917Z",
     "iopub.status.idle": "2023-08-18T07:15:33.969272Z",
     "shell.execute_reply": "2023-08-18T07:15:33.968489Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络编码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        '''\n",
    "        super(Seq2SeqEncoder, self)获取Seq2SeqEncoder的父类（即d2l.Encoder）\n",
    "        .__init__(**kwargs)调用父类的构造函数\n",
    "        **kwargs将所有关键字参数原样传递给父类\n",
    "        '''\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        '''\n",
    "        嵌入层：将离散的词索引（大小为vocab_size）映射为密集向量（维度为embed_size）。\n",
    "        输入形状：(batch_size,seq_len)（词索引序列）\n",
    "        输出形状：(batch_size,seq_len,embed_size)（嵌入向量序列）\n",
    "        '''\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        '''\n",
    "        GRU层：处理嵌入序列\n",
    "        embed_size: 输入特征维度（嵌入层输出）\n",
    "        num_hiddens: 隐藏单元数量（GRU输出的维度）\n",
    "        num_layers: GRU层数（堆叠层数）\n",
    "        dropout: 在多层GRU中，层间的Dropout比率（除最后一层外）\n",
    "        '''\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        # 将输入的词索引张量转换为嵌入向量。形状变化：(batch_size,seq_len)→(batch_size,seq_len,embed_size)\n",
    "        X = self.embedding(X) # 第一步：词索引 → 嵌入向量\n",
    "        # PyTorch的GRU要求输入形状为(seq_len,batch_size,input_size)（时间步优先）\n",
    "        # 将batch_size和seq_len 交换位置\n",
    "        # 形状变化：(batch_size,seq_len,embed_size)→(seq_len,batch_size,embed_size)\n",
    "        X = X.permute(1, 0, 2) # 第二步：调整维度顺序\n",
    "        # output: 所有时间步的隐藏状态，形状(seq_len,batch_size,num_hiddens)\n",
    "        # state: 最后时间步的隐藏状态，形状(num_layers,batch_size,num_hiddens)\n",
    "        # 若为双向GRU，则形状为(num_layers*2,batch_size,num_hiddens)\n",
    "        output, state = self.rnn(X) # 第三步：经过GRU处理\n",
    "        # output用于后续注意力机制或解码器;state作为解码器的初始隐藏状态\n",
    "        return output, state # 第四步：返回结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2a4cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "上述编码器的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd2494",
   "metadata": {},
   "source": [
    "| 数据类型                   | 适用场景     | Embedding层支持     |\n",
    "| ---------------------- | -------- | ---------------- |\n",
    "| `torch.long` | 词索引、类别标签 | ✅  必需    |\n",
    "| `torch.float32`        | 模型权重、特征  | ❌  报错    |\n",
    "| `torch.int32`          | 普通整数计算   | ⚠️  不推荐  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1780ca82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:33.972667Z",
     "iopub.status.busy": "2023-08-18T07:15:33.972142Z",
     "iopub.status.idle": "2023-08-18T07:15:34.003637Z",
     "shell.execute_reply": "2023-08-18T07:15:34.002907Z"
    },
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "词汇表大小：10（支持0-9共10个词索引）\n",
    "嵌入维度：8（每个词被编码为8维向量）\n",
    "GRU隐藏单元：16\n",
    "GRU层数：2层堆叠\n",
    "'''\n",
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "'''\n",
    "设置为评估模式\n",
    "关闭 Dropout 等训练时特有的随机性操作\n",
    "保持模型参数不变，确保输出确定性（常用于验证或测试）\n",
    "'''\n",
    "encoder.eval()\n",
    "'''\n",
    "批量大小：4（4个独立样本）\n",
    "序列长度：7（每个样本包含7个词）\n",
    "数据类型：torch.long（整数索引，符合嵌入层输入要求）\n",
    "初始值：全0（模拟4个长度为7的\"全填充\"序列）\n",
    "'''\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "# 输入X形状：(4,7)→经过编码器处理\n",
    "output, state = encoder(X)\n",
    "'''\n",
    "7：序列长度（seq_len），每个时间步输出一个隐藏状态\n",
    "4：批量大小（batch_size）\n",
    "16：隐藏单元数（num_hiddens），即每个时间步的特征维度\n",
    "'''\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a2c1d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.007123Z",
     "iopub.status.busy": "2023-08-18T07:15:34.006595Z",
     "iopub.status.idle": "2023-08-18T07:15:34.011456Z",
     "shell.execute_reply": "2023-08-18T07:15:34.010716Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149187e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb5996",
   "metadata": {},
   "source": [
    "| 步骤 | 操作       | 关键变化                                                             |\n",
    "| -- | -------- | ---------------------------------------------------------------- |\n",
    "| 1  | 嵌入 + 置换  | `(batch, seq)` → `(seq, batch, embed)`                           |\n",
    "| 2  | 复制上下文    | `(batch, hidden)` → `(seq, batch, hidden)`                       |\n",
    "| 3  | 拼接       | `(seq, batch, embed+hidden)`                                     |\n",
    "| 4  | GRU      | `output: (seq, batch, hidden)`, `state: (layers, batch, hidden)` |\n",
    "| 5  | 全连接 + 置换 | `(batch, seq, vocab)`                                            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09143bb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.014841Z",
     "iopub.status.busy": "2023-08-18T07:15:34.014327Z",
     "iopub.status.idle": "2023-08-18T07:15:34.021372Z",
     "shell.execute_reply": "2023-08-18T07:15:34.020591Z"
    },
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    # __init__方法（初始化）\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs) # 继承父类d2l.Decoder的初始化\n",
    "        # 嵌入层：将目标语言的词索引映射为向量（维度embed_size）\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # GRU层：输入维度是embed_size+num_hiddens,\n",
    "        # 多出的num_hiddens用于拼接上下文向量（编码器最后时刻的隐藏状态），帮助解码器聚焦源序列信息\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "        # 全连接层：将GRU输出映射到词汇表大小，生成每个词的预测概率分布\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "    # init_state方法（状态初始化）\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        # 从编码器输出中提取最后隐藏状态（即enc_outputs[1]）作为解码器的初始状态,实现了编码器→解码器的信息传递\n",
    "        return enc_outputs[1]\n",
    "    # 前向传播\n",
    "    def forward(self, X, state):\n",
    "        # 嵌入+维度置换：输入X形状(batch,seq_len)→嵌入后(batch,seq_len,embed)→置换为 (seq_len,batch,embed)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        # 上下文向量：取state的最后一层（state[-1]），形状(batch,num_hiddens)\n",
    "        # 复制扩展：repeat将context在时间步维度上复制seq_len次，变为(seq_len,batch,num_hiddens)，使其能与X拼接\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        # 拼接：在特征维度（dim=2）上合并词嵌入和上下文向量\n",
    "        # 形状：(seq_len,batch,embed_size+num_hiddens)，作为GRU的输入\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        # output: 所有时间步隐藏状态，形状(seq_len,batch,num_hiddens)\n",
    "        # state: 最后隐藏状态，形状(num_layers,batch,num_hiddens)\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        '''\n",
    "        预测+维度恢复：\n",
    "        dense: 将隐藏状态映射为词汇表大小的logits，形状 (seq_len,batch,vocab_size)\n",
    "        permute: 转回(batch,seq_len,vocab_size)，便于后续计算损失\n",
    "        '''\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0c369",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "实例化解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad17a24d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.024844Z",
     "iopub.status.busy": "2023-08-18T07:15:34.024212Z",
     "iopub.status.idle": "2023-08-18T07:15:34.034277Z",
     "shell.execute_reply": "2023-08-18T07:15:34.033517Z"
    },
    "origin_pos": 26,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 参数与编码器必须匹配：num_hiddens=16保证编码器的state能作为解码器的初始状态\n",
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "# 评估模式:关闭随机Dropout，确保输出可复现\n",
    "decoder.eval()\n",
    "# encoder(X)返回编码器的(output,state)\n",
    "# init_state()提取编码器的state（形状(2,4,16)）作为解码器初始隐藏状态\n",
    "state = decoder.init_state(encoder(X))\n",
    "# 输入：X形状(4,7)，初始state形状(2,4,16)\n",
    "output, state = decoder(X, state)\n",
    "'''\n",
    "4：批量大小（batch_size）\n",
    "7：序列长度（seq_len）\n",
    "10：词汇表大小（vocab_size），每个位置输出10个词的概率分布\n",
    "2：GRU层数（num_layers）\n",
    "4：批量大小\n",
    "16：隐藏单元数，用于下一次解码迭代\n",
    "'''\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb22ffb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "通过零值化屏蔽不相关的项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0d7e7",
   "metadata": {},
   "source": [
    "| 索引方式                 | 结果形状        | 结果示例                | 维度变化               |\n",
    "| -------------------- | ----------- | ------------------- | ------------------ |\n",
    "| `arr[None, :]`       | `(1, 3)`    | `[[1, 2, 3]]`       |  行方向  增加维度 |\n",
    "| `arr[:, None]`       | `(3, 1)`    | `[[1], [2], [3]]`   |  列方向  增加维度 |\n",
    "| `arr[None, :, None]` | `(1, 3, 1)` | `[[[1], [2], [3]]]` |  两端  增加维度  |\n",
    "\n",
    "    \n",
    "    None 在逗号左边（如 arr[None, :]）→ 加在行方向（最外层）\n",
    "    None 在逗号右边（如 arr[:, None]）→ 加在列方向（最内层）\n",
    "    : 表示\"保留该维度所有数据\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c5a5f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.037911Z",
     "iopub.status.busy": "2023-08-18T07:15:34.037256Z",
     "iopub.status.idle": "2023-08-18T07:15:34.044866Z",
     "shell.execute_reply": "2023-08-18T07:15:34.044120Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1) # 获取序列最大长度（第二维）\n",
    "    '''\n",
    "    广播机制：生成[0.,1.,2.]（位置索引数组）\n",
    "    [None,:] 添加一维→[[0.,1.,2.]]形状(1,3)\n",
    "    valid_len[:,None]将[1,2]变为[[1],[2]]形状(2,1)\n",
    "    广播比较：[[0.,1.,2.]]<[[1],[2]]\n",
    "    生成布尔掩码矩阵：\n",
    "    '''\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                        device=X.device)[None, :] < valid_len[:, None]\n",
    "    # ~mask取反→无效位置标记为True;将无效位置的值设为value（默认0）\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad002f0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "我们还可以使用此函数屏蔽最后几个轴上的所有项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f38f63d",
   "metadata": {},
   "source": [
    "对于三维输入X,sequence_mask内部执行：<br>\n",
    "```python\n",
    "maxlen = X.size(1) = 3 \n",
    "```\n",
    "获取序列长度维度<br>\n",
    "mask生成（关键步骤） <br>\n",
    "```python\n",
    "mask = torch.arange(3)[None, :] < [1, 2][:, None]\n",
    "```\n",
    "广播后得到形状 (2, 3) 的布尔张量：<br>\n",
    "```python\n",
    "tensor([[ True, False, False],   # 第0个序列：位置0<1有效\n",
    "        [ True,  True, False]])  # 第1个序列：位置0,1<2有效\n",
    "```\n",
    "屏蔽赋值<br>\n",
    "X[~mask] = -1 会将~mask中为True的所有位置对应 X 的元素设-1<br>\n",
    "在三维情况下，(2, 3) 的掩码会广播到 (2, 3, 4)，作用于最后一维的所有4个特征。<br>\n",
    "最终输出结果<br>\n",
    "```python\n",
    "tensor([[[ 1.,  1.,  1.,  1.],  # 第0个序列，第0个时间步（有效）\n",
    "         [-1., -1., -1., -1.],  # 第0个序列，第1个时间步（屏蔽）\n",
    "         [-1., -1., -1., -1.]], # 第0个序列，第2个时间步（屏蔽）\n",
    "\n",
    "        [[ 1.,  1.,  1.,  1.],  # 第1个序列，第0个时间步（有效）\n",
    "         [ 1.,  1.,  1.,  1.],  # 第1个序列，第1个时间步（有效）\n",
    "         [-1., -1., -1., -1.]]]) # 第1个序列，第2个时间步（屏蔽）\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb003c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.048373Z",
     "iopub.status.busy": "2023-08-18T07:15:34.047745Z",
     "iopub.status.idle": "2023-08-18T07:15:34.054283Z",
     "shell.execute_reply": "2023-08-18T07:15:34.053539Z"
    },
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.],\n",
       "         [-1., -1., -1., -1.]],\n",
       "\n",
       "        [[ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "形状：(batch_size=2,seq_len=3,feature_size=4)\n",
    "含义：2个序列，每个序列3个时间步，每个时间步4维特征向量\n",
    "初始值：全为1\n",
    "'''\n",
    "X = torch.ones(2, 3, 4)\n",
    "'''\n",
    "valid_len=torch.tensor([1, 2])：指定每个序列的有效长度\n",
    "第0个序列：只有前1个时间步有效\n",
    "第1个序列：只有前2个时间步有效\n",
    "value=-1：将无效位置的值设为-1\n",
    "'''\n",
    "sequence_mask(X, torch.tensor([1, 2]), value=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d22cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "通过扩展softmax交叉熵损失函数来遮蔽不相关的预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef5fd7",
   "metadata": {},
   "source": [
    "| 张量                       | 形状变化           | 说明            |\n",
    "| ------------------------ | -------------- | ------------- |\n",
    "| `unweighted_loss`        | `(batch, seq)` | 每个token的原始损失  |\n",
    "| `weights`                | `(batch, seq)` | 掩码（有效=1，填充=0） |\n",
    "| **相乘结果**                 | `(batch, seq)` | 填充位置损失被置零     |\n",
    "|  `.mean(dim=1)`  | **`(batch,)`** | 每个样本的平均损失     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0da33ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.057946Z",
     "iopub.status.busy": "2023-08-18T07:15:34.057267Z",
     "iopub.status.idle": "2023-08-18T07:15:34.062428Z",
     "shell.execute_reply": "2023-08-18T07:15:34.061664Z"
    },
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        # 创建与label同形状的全1权重矩阵（初始权重）;形状：(batch_size,seq_len)，每个位置初始权重为1\n",
    "        weights = torch.ones_like(label)\n",
    "        # 调用sequence_mask，将超出有效长度的位置权重设为0;结果：有效位置权重为1，填充位置权重为0的掩码矩阵\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        # 设置损失函数为不缩减模式，返回每个元素的损失（而非直接求和或平均）\n",
    "        self.reduction='none'\n",
    "        '''\n",
    "        维度变换：pred.permute(0, 2, 1)\n",
    "        输入pred形状：(batch,seq_len,vocab_size)\n",
    "        CrossEntropyLoss要求：(batch,vocab_size,seq_len)（类别在第二维）\n",
    "        转换后：(batch,vocab_size,seq_len)\n",
    "        调用父类计算原始损失，形状：(batch,seq_len)（每个位置的交叉熵）\n",
    "        1. 为什么要自定义 forward？\n",
    "        标准nn.CrossEntropyLoss的缺陷：无法处理变长序列填充：它会平等计算所有时间步的损失\n",
    "        示例问题：真实长度[1,2] 的序列被填充为长度3，标准损失会强制模型学习预测第2个填充位置，导致无效梯度\n",
    "        目标：让模型只关注有效token，忽略填充部分\n",
    "        2. 为什么用 super().forward()？\n",
    "        技术限制：不能调用 self.forward()，会形成无限递归,必须显式调用父类实现复用其高效的C++底层计算\n",
    "        '''\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        '''\n",
    "        掩码相乘：填充位置的损失×0=0，有效位置损失保留\n",
    "        序列维度平均：对每个样本的有效部分求平均\n",
    "        输出形状：(batch_size,)（每个样本的平均损失）\n",
    "        '''\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1) # (batch_size, seq_len)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf601f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "代码健全性检查"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ceefb",
   "metadata": {},
   "source": [
    "损失计算内部流程<br>\n",
    "步骤1：生成权重掩码\n",
    "```python\n",
    "weights = torch.ones_like(label)  # (3,4)全1\n",
    "weights = sequence_mask(weights, valid_len)  # 根据有效长度置0\n",
    "```\n",
    "weights 结果：<br>\n",
    "```python\n",
    "[[1, 1, 1, 1],   # 有效长度4：全部有效\n",
    " [1, 1, 0, 0],   # 有效长度2：后2个位置被屏蔽\n",
    " [0, 0, 0, 0]]   # 有效长度0：全部屏蔽\n",
    "```\n",
    "步骤2：计算原始交叉熵损失<br>\n",
    "`unweighted_loss = super(...).forward(pred.permute(0,2,1), label)`<br>\n",
    "- 维度置换：pred 从 (3,4,10) → (3,10,4)，满足 CrossEntropyLoss 要求\n",
    "- 损失值：所有logits为1，真实标签索引为1，每个位置的损失 = -log(softmax(1)) = log(10) ≈ 2.3026\n",
    "- unweighted_loss 形状：(3,4)，值全为 2.3026\n",
    "步骤3：应用掩码并平均<br>\n",
    "`weighted_loss = (unweighted_loss * weights).mean(dim=1)`<br>\n",
    "相乘后（填充位置损失变0）：<br>\n",
    "```python\n",
    "[[2.3026, 2.3026, 2.3026, 2.3026],\n",
    " [2.3026, 2.3026, 0, 0],\n",
    " [0, 0, 0, 0]]\n",
    "```\n",
    "沿seq_len平均（dim=1）：<br>\n",
    "- 样本0: (2.3026×4)/4 = **2.3026**\n",
    "- 样本1: (2.3026×2)/4 = **1.1513**（包含填充，被低估）\n",
    "- 样本2: 0/4 = **0**\n",
    "最终返回结果<br>\n",
    "`tensor([2.3026, 1.1513, 0])  # 形状 (3,)`<br>\n",
    "⚠️ 当前实现的问题<br>\n",
    "- 使用 .mean(dim=1) 会对总长度平均，导致：\n",
    "- 样本1的有效部分损失应为 2.3026，但被平均后变为 1.1513\n",
    "- 低估了短序列的损失贡献"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65239ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.065956Z",
     "iopub.status.busy": "2023-08-18T07:15:34.065339Z",
     "iopub.status.idle": "2023-08-18T07:15:34.073758Z",
     "shell.execute_reply": "2023-08-18T07:15:34.072755Z"
    },
    "origin_pos": 46,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "'''\n",
    "pred=torch.ones(3,4,10)      # 3个样本，4个时间步，10类别预测\n",
    "label=torch.ones((3,4),dtype=torch.long)  # 3个样本，4个时间步，真实标签全为1\n",
    "valid_len=torch.tensor([4,2,0])  # 有效长度：第0个样本4，第1个样本2，第2个样本0\n",
    "'''\n",
    "loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long),\n",
    "     torch.tensor([4, 2, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed27b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf087d6",
   "metadata": {},
   "source": [
    "| 步骤      | 关键操作                             | 数据形状变化                 |\n",
    "| ------- | -------------------------------- | ---------------------- |\n",
    "| 1. 数据加载 | `batch` → `X, Y`                 | `(batch, seq)`         |\n",
    "| 2. 构造输入 | `dec_input` = `<bos>` + `Y[:-1]` | `(batch, seq)`         |\n",
    "| 3. 前向传播 | `net(X, dec_input)`              | `(batch, seq, vocab)`  |\n",
    "| 4. 损失计算 | `loss(Y_hat, Y)`                 | `(batch, seq)` → 掩码后平均 |\n",
    "| 5. 反向传播 | `sum()` → `backward()`           | 梯度计算                   |\n",
    "| 6. 参数更新 | `optimizer.step()`               | 模型更新                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d7b922e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.077404Z",
     "iopub.status.busy": "2023-08-18T07:15:34.076756Z",
     "iopub.status.idle": "2023-08-18T07:15:34.087405Z",
     "shell.execute_reply": "2023-08-18T07:15:34.086461Z"
    },
    "origin_pos": 51,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    '''\n",
    "    1. 权重初始化函数\n",
    "    作用：对网络中所有Linear和GRU层的权重进行Xavier均匀分布初始化\n",
    "    目的：防止深层网络梯度消失/爆炸，确保训练初期信号稳定传播\n",
    "    GRU特殊处理：遍历所有扁平化权重名，只初始化含\"weight\"的参数（不含bias）\n",
    "    Xavier初始化原理\n",
    "    公式：权重从均匀分布U[-a,a]采样，其中a=sqrt(6/(fan_in+fan_out))\n",
    "    效果：\n",
    "    前向传播：Var(output)≈Var(input)，避免信号爆炸或消失\n",
    "    反向传播：Var(grad)≈Var(grad_next)，梯度稳定回流\n",
    "\n",
    "    '''\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names: # 遍历所有扁平化参数名\n",
    "                '''\n",
    "                weight_ih：输入到隐藏状态的权重（包含重置门/更新门/候选隐藏状态）\n",
    "                weight_hh：隐藏状态到隐藏状态的权重\n",
    "                3*hidden_size：GRU有3个门（重置门、更新门、候选隐藏状态）\n",
    "                '''\n",
    "                if \"weight\" in param: # 只选含\"weight\"的（跳过bias）\n",
    "                    nn.init.xavier_uniform_(m._parameters[param]) # 就地初始化\n",
    "    # 2. 训练准备\n",
    "    net.apply(xavier_init_weights) # 应用初始化\n",
    "    net.to(device) # 模型迁移到GPU/CPU\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr) # Adam优化器\n",
    "    loss = MaskedSoftmaxCELoss() # 带填充掩码的损失函数\n",
    "    net.train() # 开启训练模式（启用dropout等）\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                     xlim=[10, num_epochs])\n",
    "    # 3. 数据迭代核心逻辑\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad() # 清空梯度\n",
    "            '''\n",
    "            X：源语言序列（含填充）\n",
    "            X_valid_len：源序列有效长度\n",
    "            Y：目标语言序列（含填充）\n",
    "            Y_valid_len：目标序列有效长度\n",
    "            '''\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "            '''\n",
    "            4. Teacher Forcing机制\n",
    "            <bos>：句子开始标记，每个batch的第一个输入\n",
    "            Y[:,:-1]：目标序列去掉最后一个词（作为解码器的输入）\n",
    "            Y：目标序列，形状(batch_size,seq_len);:-1：切片去掉最后一列（最后一个时间步）\n",
    "            dec_input：将<bos>与目标序列右移一位后拼接;形状：(batch,seq_len)→解码器在t时刻输入的是Y[t-1]\n",
    "            tgt_vocab['<bos>']：获取\"句子开始\"标记的索引（如 1）\n",
    "            [...]*Y.shape[0]：将该索引复制batch_size次\n",
    "            torch.tensor(...,device=device)：创建张量并迁移到GPU/CPU\n",
    "            .reshape(-1,1)：重塑为形状(batch_size, 1)\n",
    "            '''\n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                          device=device).reshape(-1, 1)\n",
    "            # dim=1：在列维度（时间步维度）上拼接;bos（首列）+Y[:,:-1]（后续列）=完整输入序列\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)\n",
    "            # 5. 前向与反向传播\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len) # 编码器-解码器前向\n",
    "            l = loss(Y_hat, Y, Y_valid_len) # 计算掩码损失（形状:(batch,seq)）\n",
    "            l.sum().backward() # 对所有token损失求和并反向传播\n",
    "            d2l.grad_clipping(net, 1) # 梯度裁剪（防止梯度爆炸）\n",
    "            num_tokens = Y_valid_len.sum() # 统计有效token总数\n",
    "            optimizer.step() # 参数更新\n",
    "            with torch.no_grad():\n",
    "                metric.add(l.sum(), num_tokens) # 累总损失&总token数\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(epoch + 1, (metric[0] / metric[1],)) # 每10轮绘图\n",
    "    # 平均损失：所有有效token的总损失 / 总token数;处理速度：每秒处理的token数（评估训练效率）\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / timer.stop():.1f} '\n",
    "        f'tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951a669",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "创建和训练一个循环神经网络“编码器－解码器”模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79f585d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:15:34.091791Z",
     "iopub.status.busy": "2023-08-18T07:15:34.090975Z",
     "iopub.status.idle": "2023-08-18T07:16:11.767145Z",
     "shell.execute_reply": "2023-08-18T07:16:11.765998Z"
    },
    "origin_pos": 55,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.019, 12277.5 tokens/sec on cuda:0\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"262.1875pt\" height=\"183.35625pt\" viewBox=\"0 0 262.1875 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-12-01T22:45:41.516405</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.5.1, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 262.1875 183.35625 \n",
       "L 262.1875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "L 50.14375 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 77.081681 145.8 \n",
       "L 77.081681 7.2 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m860e9f71e5\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m860e9f71e5\" x=\"77.081681\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(70.719181 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 110.754095 145.8 \n",
       "L 110.754095 7.2 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m860e9f71e5\" x=\"110.754095\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(101.210345 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 144.426509 145.8 \n",
       "L 144.426509 7.2 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m860e9f71e5\" x=\"144.426509\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(134.882759 160.398438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 178.098922 145.8 \n",
       "L 178.098922 7.2 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m860e9f71e5\" x=\"178.098922\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(168.555172 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 211.771336 145.8 \n",
       "L 211.771336 7.2 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m860e9f71e5\" x=\"211.771336\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(202.227586 160.398438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m860e9f71e5\" x=\"245.44375\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(235.9 160.398438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(132.565625 174.076563)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 50.14375 118.51157 \n",
       "L 245.44375 118.51157 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <defs>\n",
       "       <path id=\"m619d9f550c\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m619d9f550c\" x=\"50.14375\" y=\"118.51157\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 122.310789)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 50.14375 84.799825 \n",
       "L 245.44375 84.799825 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m619d9f550c\" x=\"50.14375\" y=\"84.799825\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 88.599044)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 50.14375 51.08808 \n",
       "L 245.44375 51.08808 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m619d9f550c\" x=\"50.14375\" y=\"51.08808\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 54.887298)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-31\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 50.14375 17.376334 \n",
       "L 245.44375 17.376334 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m619d9f550c\" x=\"50.14375\" y=\"17.376334\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.878125 21.175553)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 86.157813)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_21\">\n",
       "    <path d=\"M 50.14375 13.5 \n",
       "L 56.878233 56.183383 \n",
       "L 63.612716 81.162422 \n",
       "L 70.347198 96.410299 \n",
       "L 77.081681 106.487887 \n",
       "L 83.816164 113.521215 \n",
       "L 90.550647 119.527325 \n",
       "L 97.285129 123.728015 \n",
       "L 104.019612 126.304109 \n",
       "L 110.754095 129.317194 \n",
       "L 117.488578 130.987786 \n",
       "L 124.22306 132.724923 \n",
       "L 130.957543 133.917138 \n",
       "L 137.692026 135.322033 \n",
       "L 144.426509 136.03975 \n",
       "L 151.160991 136.10266 \n",
       "L 157.895474 137.195229 \n",
       "L 164.629957 137.494236 \n",
       "L 171.36444 137.470378 \n",
       "L 178.098922 137.982851 \n",
       "L 184.833405 138.620184 \n",
       "L 191.567888 138.78559 \n",
       "L 198.302371 138.681177 \n",
       "L 205.036853 138.717129 \n",
       "L 211.771336 139.092499 \n",
       "L 218.505819 138.993409 \n",
       "L 225.240302 138.955555 \n",
       "L 231.974784 139.5 \n",
       "L 238.709267 139.401311 \n",
       "L 245.44375 139.146697 \n",
       "\" clip-path=\"url(#pd5ceccf71a)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 50.14375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 245.44375 145.8 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 145.8 \n",
       "L 245.44375 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 7.2 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pd5ceccf71a\">\n",
       "   <rect x=\"50.14375\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "模型维度：嵌入32维，GRU隐藏层32维\n",
    "网络深度：2层GRU，dropout率0.1（防止过拟合）\n",
    "数据格式：批量64，序列截断/填充至10个词\n",
    "训练配置：学习率0.005，训练300轮，自动选择GPU/CPU\n",
    "'''\n",
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()\n",
    "'''\n",
    "train_iter：训练数据迭代器（每次返回(X, X_valid_len,Y,Y_valid_len)）\n",
    "src_vocab：源语言词表（如英语）\n",
    "tgt_vocab：目标语言词表（如法语）\n",
    "效果：自动下载/加载机器翻译数据集（如WMT），构建词汇表，添加<bos>,<eos>,<pad>标记\n",
    "'''\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "'''\n",
    "编码器：len(src_vocab)→源语言词汇表大小（如10000）\n",
    "解码器：len(tgt_vocab)→目标语言词汇表大小（可能不同）\n",
    "封装：EncoderDecoder将编码器和解码器组合为完整Seq2Seq模型\n",
    "参数传递：编码器的最终隐藏状态自动作为解码器初始状态\n",
    "'''\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers,\n",
    "                        dropout)\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "'''\n",
    "初始化：Xavier初始化所有权重\n",
    "迭代训练：300轮循环\n",
    "每轮batch处理：\n",
    "X→源语言序列（含填充）\n",
    "Y→目标语言序列（含填充）\n",
    "dec_input=<bos>+Y[:-1]→Teacher Forcing输入\n",
    "Y_hat=net(X,dec_input)→模型预测\n",
    "loss(Y_hat,Y,Y_valid_len)→计算掩码损失\n",
    "反向传播+梯度裁剪+Adam优化\n",
    "监控：每10轮绘制损失曲线\n",
    "输出：最终平均损失&训练速度（tokens/sec）\n",
    "'''\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0deec2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb9b0a",
   "metadata": {},
   "source": [
    "为什么需要enc_valid_len？<br>\n",
    "编码器：使用 enc_valid_len 在GRU计算中忽略填充部分，确保state只由真实内容生成<br>\n",
    "解码器init_state：虽然此处未直接使用enc_valid_len，但为支持更复杂的初始化（如注意力机制）预留接口<br>\n",
    "\n",
    "状态传递的物理意义<br>\n",
    "```\n",
    "# 类比：编码器读完整个源句，形成\"理解\"（state）\n",
    "# 解码器以这个\"理解\"为起点，开始逐词生成翻译\n",
    "编码器状态 → [源句语义向量] → 解码器初始状态\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7510bee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:16:11.771151Z",
     "iopub.status.busy": "2023-08-18T07:16:11.770496Z",
     "iopub.status.idle": "2023-08-18T07:16:11.779631Z",
     "shell.execute_reply": "2023-08-18T07:16:11.778678Z"
    },
    "origin_pos": 58,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps,\n",
    "                    device, save_attention_weights=False):\n",
    "    \"\"\"序列到序列模型的预测\"\"\"\n",
    "    # 1. 设置评估模式:关闭Dropout、BatchNorm等训练时特有的随机行为,确保输出确定性和可复现性\n",
    "    net.eval()\n",
    "    '''\n",
    "    2. 源序列预处理\n",
    "    1. 分词并转为索引\n",
    "    src_sentence.lower().split(' ')：将输入转为小写并按空格分词→词元列表\n",
    "    src_vocab[...]：将每个词元映射为词汇表中的整数索引\n",
    "    +[src_vocab['<eos>']]：在末尾添加 结束符<eos>的索引\n",
    "    '''\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [\n",
    "        src_vocab['<eos>']]\n",
    "    '''\n",
    "    2. 记录有效长度\n",
    "    作用：将序列的有效长度（含<eos>）记录为张量→(1,)\n",
    "    必要性：告知编码器实际内容长度，以便在GRU计算中忽略填充部分\n",
    "    '''\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    '''\n",
    "    3. 截断或填充到固定长度\n",
    "    若序列过长（>num_steps）： 截断保留前num_steps个词\n",
    "    若序列过短（<num_steps）：用填充符<pad>补足到num_steps\n",
    "    '''\n",
    "    src_tokens = d2l.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    '''\n",
    "    4. 转为张量并添加batch维度\n",
    "    torch.tensor(...,dtype=torch.long,device=device)：\n",
    "    将Python列表转为PyTorch张量;指定类型torch.long（嵌入层要求的索引类型）;迁移到GPU/CPU\n",
    "    torch.unsqueeze(...,dim=0)：\n",
    "    在第0维增加一个维度，将形状从(seq_len,)→(1, seq_len)\n",
    "    1表示batch size=1（预测时通常一次处理一个句子）\n",
    "    最终enc_X形状：(1,num_steps)\n",
    "    '''\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    '''\n",
    "    3. 编码器前向传播\n",
    "    1. 编码器前向传播\n",
    "    输入：\n",
    "    enc_X：源序列，形状(batch_size,num_steps);enc_valid_len：有效长度，形状(batch_size,)\n",
    "    输出：enc_outputs是一个元组(output,state)\n",
    "    output：编码器所有时间步的隐藏状态，形状 (num_steps,batch,num_hiddens)\n",
    "    state：编码器最后时刻的隐藏状态，形状 (num_layers,batch,num_hiddens)\n",
    "    '''\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    '''\n",
    "    2. 初始化解码器状态\n",
    "    dec_state=编码器的最终隐藏状态enc_outputs[1]\n",
    "    形状保持：(num_layers,batch,num_hiddens)→(2,1,32)\n",
    "    作用：将源序列的语义信息 完整传递给解码器作为初始记忆\n",
    "    '''\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    '''\n",
    "    4. 解码器初始输入\n",
    "    BOS标记：创建仅含<bos>的初始输入;形状：(1,1)（batch=1,seq_len=1）\n",
    "    tgt_vocab['<bos>']：获取\"句子开始\"标记的整数索引（例如 1）;[...]：将其包装为单元素列表[1]\n",
    "    torch.tensor(...,dtype=torch.long,device=device)：\n",
    "    转为PyTorch张量;类型为torch.long（嵌入层要求）;迁移到GPU/CPU;形状(1,)：一维张量\n",
    "    torch.unsqueeze(...,dim=0)：\n",
    "    在第0维增加维度，形状从(1,)→(1, 1)\n",
    "    第一个 1：batch_size=1（单句预测）\n",
    "    第二个 1：当前序列长度=1（仅含<bos>）\n",
    "    '''\n",
    "    dec_X = torch.unsqueeze(torch.tensor(\n",
    "        [tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    '''\n",
    "    output_seq：空列表，用于累积预测的token索引（如[5,10,7]）\n",
    "    attention_weight_seq：空列表，用于存储每个时间步的注意力权重矩阵（用于可视化）\n",
    "    核心作用：自回归生成的起点\n",
    "    这两行是循环解码的初始化：\n",
    "    dec_X：初始输入仅为<bos>标记，后续循环中被上一步的预测结果替换\n",
    "    output_seq：逐步收集所有预测，最终转换为完整句子\n",
    "    '''\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    '''\n",
    "    5. 自回归解码循环\n",
    "    解码：Y 形状 (1,1,vocab_size)，是当前时间步的预测分布\n",
    "    贪心采样：argmax(dim=2)在词汇表维度取最大值索引→dec_X形状(1,1)\n",
    "    提取token：squeeze 去掉batch维，item()转为Python整数\n",
    "    存储注意力：可选保存每层/每头的注意力权重（用于可视化）\n",
    "    终止判断：生成EOS则跳出循环\n",
    "    记录结果：将预测词索引加入输出序列\n",
    "    '''\n",
    "    for _ in range(num_steps): # 最多生成num_steps个词\n",
    "        '''\n",
    "        单步解码\n",
    "        输入：dec_X：当前时间步的输入词索引，形状(1,1)(batch,seq_len);dec_state：解码器上一时刻的隐藏状态\n",
    "        输出：Y：当前时间步的预测分布，形状(1,1,vocab_size);dec_state：更新后的隐藏状态，供下一步使用\n",
    "        '''\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        '''\n",
    "        2. 贪心选择概率最高的词\n",
    "        Y.argmax(dim=2)：在词汇表维度（vocab_size）取最大值的索引\n",
    "        结果：预测词的索引，形状(1,1) ，作为下一步的输入\n",
    "        '''\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        # 3. 提取Python整数:压缩batch维：(1,1)→(1,)→再转为标量;类型转换：转为Python int，便于后续判断和存储\n",
    "        # item()将只含单个元素的张量转换为Python原生标量 \n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        # 4. 保存注意力权重:条件：若设置保存标志，记录当前步的注意力分布;用途：可视化对齐关系（源词→目标词）\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(net.decoder.attention_weights) # 保存注意力权重\n",
    "        # 5. 终止判断:提前终止：一旦生成结束符<eos>，立即停止循环;效率：避免继续生成无意义的填充词\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break # 遇到EOS终止生成\n",
    "        # 6. 存储预测结果:累积：将当前预测词索引加入输出列表;最终：output_seq=[5,10,7,...]\n",
    "        output_seq.append(pred)\n",
    "    '''\n",
    "    tgt_vocab.to_tokens(output_seq)：将索引列表转为词元列表\n",
    "    [5,10,7]→['bonjour','monde','!']\n",
    "    ' '.join(...)：用空格连接为完整句子\n",
    "    返回：(翻译结果, 注意力权重列表)\n",
    "    '''\n",
    "    return ' '.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09b1309",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "BLEU的代码实现<br>\n",
    "$e^{(min(0,1-\\frac{len_{label}}{len_{label}}))\\prod_{n=1}^{k}p_{n}^{\\frac{1}{2^n} }  }$<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9135ade0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:16:11.784109Z",
     "iopub.status.busy": "2023-08-18T07:16:11.783827Z",
     "iopub.status.idle": "2023-08-18T07:16:11.791568Z",
     "shell.execute_reply": "2023-08-18T07:16:11.790396Z"
    },
    "origin_pos": 62,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def bleu(pred_seq, label_seq, k):  \n",
    "    \"\"\"计算BLEU\"\"\"\n",
    "    # 1. 预处理序列:分词：将预测和标签字符串按空格分割为词元列表;计算长度：获取两个序列的词数\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    '''\n",
    "    2. 长度惩罚项（Brevity Penalty）\n",
    "    目的：惩罚过短的预测（机器翻译中常见的\"偷懒\"行为）\n",
    "    若len_pred<len_label（预测更短）：1-len_label/len_pred为负，惩罚生效\n",
    "    若len_pred>=len_label：惩罚为0，指数后为1（无惩罚）\n",
    "    标签 \"a b c\"（3词），预测 \"a b\"（2词）：exp(1-3/2) = exp(-0.5) ≈ 0.61\n",
    "    标签 \"a b c\"（3词），预测 \"a b c d\"（4词）：exp(0) = 1\n",
    "    '''\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    # 3. n-gram精度计算\n",
    "    for n in range(1, k + 1): # 遍历1-gram到k-gram\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        # 构建标签n-gram字典:滑动窗口：在标签序列上提取所有n-gram（连续n个词）;统计频次：记录每个n-gram出现次数\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        # 统计预测n-gram匹配数\n",
    "        for i in range(len_pred - n + 1):\n",
    "            '''\n",
    "            精确匹配：预测n-gram必须在标签n-gram字典中存在\n",
    "            截断计数：每个标签n-gram最多被匹配其出现次数次\n",
    "            示例（标签\"a b a\"，预测\"a a b\"）：\n",
    "            n=1：匹配'a'两次，'b'一次 → num_matches=3（正确）\n",
    "            若无截断机制：预测的两个'a'会错误匹配标签的一个'a'两次\n",
    "            遍历标签 \"a b a\"：\n",
    "            i=0: label_subs['a'] = 1\n",
    "            i=1: label_subs['b'] = 1\n",
    "            i=2: label_subs['a'] = 2（累加）\n",
    "            '''\n",
    "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0: # 若该n-gram在标签中存在且未耗尽\n",
    "                num_matches += 1\n",
    "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1 # 消耗一次匹配机会（防止重复计数）\n",
    "        '''\n",
    "        4. 计算加权n-gram精度\n",
    "        分子：num_matches（正确预测的n-gram数）\n",
    "        分母：len_pred - n + 1（预测序列中n-gram总数）\n",
    "        权重：math.pow(0.5, n)，高阶n-gram权重指数衰减\n",
    "        1-gram权重：0.5²=0.5;2-gram权重：0.5²=0.25;3-gram权重：0.5³=0.125\n",
    "        几何平均：各阶精度相乘，避免某一阶主导\n",
    "        整体公式:BLEU=BP×exp(Σₖwₙ×log(Pₙ))\n",
    "        其中：\n",
    "        BP：长度惩罚项;Pₙ：n-gram精度（正确数/预测总数）;wₙ=0.5ⁿ\n",
    "        '''\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b215362",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "将几个英语句子翻译成法语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "653f0dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:16:11.796025Z",
     "iopub.status.busy": "2023-08-18T07:16:11.795107Z",
     "iopub.status.idle": "2023-08-18T07:16:11.818936Z",
     "shell.execute_reply": "2023-08-18T07:16:11.817788Z"
    },
    "origin_pos": 64,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va , maintenant . recul, bleu 0.000\n",
      "i lost . => j'ai perdu ., bleu 1.000\n",
      "he's calm . => <unk> <unk> <unk> ., bleu 0.000\n",
      "i'm home . => je suis chez moi retard ., bleu 0.803\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. 取英语句子 \"he's calm .\"\n",
    "   ↓\n",
    "2. 编码器编码为上下文向量\n",
    "   ↓\n",
    "3. 解码器自回归生成 \"il est calme .\"\n",
    "   ↓\n",
    "4. 与参考译文 \"il est calme .\" 对比\n",
    "   ↓\n",
    "5. BLEU计算：n-gram匹配率 + 长度惩罚\n",
    "   ↓\n",
    "6. 输出: he's calm . => il est calme ., bleu 1.000\n",
    "engs：4个英语句子（源语言），包含标点符号和缩写\n",
    "fras：4个对应的法语参考译文（目标语言），作为评估标准\n",
    "'''\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    '''\n",
    "    translation：模型生成的法语译文（如 \"va !\"）\n",
    "    attention_weight_seq：注意力权重序列（用于可视化对齐）\n",
    "    translation：模型的预测输出\n",
    "    fra：人工参考译文（ground truth）\n",
    "    k=2：计算到2-gram（BLEU-2），衡量词对的匹配程度\n",
    "    返回值：0~1之间的分数，越高说明翻译质量越好\n",
    "    '''\n",
    "    translation, attention_weight_seq = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "    print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "required_libs": [],
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
