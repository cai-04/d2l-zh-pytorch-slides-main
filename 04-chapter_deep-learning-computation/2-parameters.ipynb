{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23850d90",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 参数管理\n",
    "\n",
    "我们首先看一下具有单隐藏层的多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7ef7a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:09.649068Z",
     "iopub.status.busy": "2023-08-18T07:01:09.648305Z",
     "iopub.status.idle": "2023-08-18T07:01:10.928992Z",
     "shell.execute_reply": "2023-08-18T07:01:10.927959Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4223],\n",
       "        [-0.3488]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8), # 第1层：Linear(4, 8)-全连接层，将4维输入映射到8维\n",
    "                    nn.ReLU(), # 第2层：ReLU()-激活函数，引入非线性\n",
    "                    nn.Linear(8,1)) # 第3层：Linear(8, 1)-全连接层，将8维映射到1维输出\n",
    "# 生成形状为 (2, 4) 的随机张量(2个样本，每个样本 4个特征)\n",
    "X = torch.rand(size=(2, 4)) \n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbaff55",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "参数访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e2fff9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.933865Z",
     "iopub.status.busy": "2023-08-18T07:01:10.933267Z",
     "iopub.status.idle": "2023-08-18T07:01:10.939922Z",
     "shell.execute_reply": "2023-08-18T07:01:10.938931Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0937, -0.0804, -0.2165,  0.2342, -0.0191,  0.3365,  0.0420, -0.3358]])), ('bias', tensor([-0.1681]))])\n"
     ]
    }
   ],
   "source": [
    "# 访问Sequential容器中索引为2的模块（从0开始计数）\n",
    "# .state_dict()：获取该模块的参数字典，包含所有可学习参数（权重和偏置）\n",
    "# weight  ：形状为 (1, 8) 的权重矩阵；bias  ：形状为 (1,) 的偏置向量\n",
    "print(net[2].state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e174dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "目标参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0682fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.945104Z",
     "iopub.status.busy": "2023-08-18T07:01:10.944250Z",
     "iopub.status.idle": "2023-08-18T07:01:10.951764Z",
     "shell.execute_reply": "2023-08-18T07:01:10.950790Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.1681], requires_grad=True)\n",
      "tensor([-0.1681])\n"
     ]
    }
   ],
   "source": [
    "# 查看参数类型，type\n",
    "print(type(net[2].bias))\n",
    "# 查看参数对象本身，Parameter\n",
    "print(net[2].bias)\n",
    "# 查看底层数据张量，Tensor\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf4d55b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.956378Z",
     "iopub.status.busy": "2023-08-18T07:01:10.955542Z",
     "iopub.status.idle": "2023-08-18T07:01:10.961810Z",
     "shell.execute_reply": "2023-08-18T07:01:10.960767Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查网络中第二个全连接层的权重梯度是否为None\n",
    "'''\n",
    "net[2]：访问Sequential中索引为2的模块，即nn.Linear(8,1)输出层\n",
    ".weight：获取该层的权重参数（形状为(1,8)）\n",
    ".grad：获取该权重的梯度（损失函数对权重的导数，用于梯度下降）\n",
    "'''\n",
    "net[2].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b54ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916939ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.966725Z",
     "iopub.status.busy": "2023-08-18T07:01:10.965969Z",
     "iopub.status.idle": "2023-08-18T07:01:10.972600Z",
     "shell.execute_reply": "2023-08-18T07:01:10.971655Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "net[0]：选择 Sequential 中索引为0的模块（第一个 nn.Linear(4,8)）\n",
    ".named_parameters()：返回该层参数的可迭代对象，每个元素是(参数名,参数张量)元组\n",
    "列表推导式：提取每个参数的名称和形状\n",
    "* 解包：将列表元素解开，作为多个独立参数传给 print()，实现单行输出\n",
    "'''\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "# net.named_parameters()：获取所有可训练参数（包括所有子模块）\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116207ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.977269Z",
     "iopub.status.busy": "2023-08-18T07:01:10.976623Z",
     "iopub.status.idle": "2023-08-18T07:01:10.983222Z",
     "shell.execute_reply": "2023-08-18T07:01:10.982309Z"
    },
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1681])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "net.state_dict()：获取整个网络的参数字典（OrderedDict），键是参数名（如 '0.weight'、'2.bias'）\n",
    "['2.bias']：通过键名访问第二层（索引2）的偏置参数\n",
    ".data：提取参数的原始数据张量，不包含梯度追踪信息\n",
    "'''\n",
    "net.state_dict()['2.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707279d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712e31fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:10.988088Z",
     "iopub.status.busy": "2023-08-18T07:01:10.987352Z",
     "iopub.status.idle": "2023-08-18T07:01:10.998245Z",
     "shell.execute_reply": "2023-08-18T07:01:10.997197Z"
    },
    "origin_pos": 28,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3113],\n",
       "        [-0.3113]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "X(2,4) \n",
    "  → block0: Linear(4→8) → ReLU → Linear(8→4) → ReLU\n",
    "  → block1: 同上\n",
    "  → block2: 同上\n",
    "  → block3: 同上\n",
    "  → Linear(4→1)\n",
    "输出: (2,1)\n",
    "'''\n",
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "# block2内部结构=block1→block1→block1→block1\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    # 将block1重复4次串联，形成一个深层子网络\n",
    "    for i in range(4):\n",
    "        # 动态添加子模块\n",
    "        net.add_module(f'block {i}', block1()) \n",
    "    return net\n",
    "'''\n",
    "结构：block2（4层嵌套）+输出层（4→1）\n",
    "总层数：4个子模块×2个Linear层+1个输出层=9个Linear层\n",
    "输入输出：X（如(2,4)）→ 标量（如(2,1)）\n",
    "'''\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a2644",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "设计了网络后，我们看看它是如何工作的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d7717d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.002889Z",
     "iopub.status.busy": "2023-08-18T07:01:11.002264Z",
     "iopub.status.idle": "2023-08-18T07:01:11.007643Z",
     "shell.execute_reply": "2023-08-18T07:01:11.006464Z"
    },
    "origin_pos": 33,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939ba4d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.012522Z",
     "iopub.status.busy": "2023-08-18T07:01:11.011839Z",
     "iopub.status.idle": "2023-08-18T07:01:11.018508Z",
     "shell.execute_reply": "2023-08-18T07:01:11.017590Z"
    },
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4338, -0.2437, -0.4322,  0.2146,  0.3343,  0.3795, -0.3809, -0.0874])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "rgnet (Sequential)\n",
    "└── [0] block2 (Sequential)\n",
    "    ├── [0] block 0 (block1 → Sequential)\n",
    "    ├── [1] block 1 (block1 → Sequential)  ← 选中这一层\n",
    "    │   ├── [0] Linear(4, 8)               ← 再选这一层\n",
    "    │   └── [1] ReLU()\n",
    "    └── [2] block 2 (block1)\n",
    "    └── [3] block 3 (block1)\n",
    "└── [1] Linear(4, 1)\n",
    "rgnet[0]：选择 Sequential 中第0个模块→即block2()（4个block1的容器）\n",
    "rgnet[0][1]：在 block2 中索引为1的模块→即第2个 block1()\n",
    "rgnet[0][1][0]：在该 block1 中索引为0的模块→即第一个nn.Linear(4,8)\n",
    ".bias：获取该线性层的偏置参数对象（Parameter）\n",
    ".data：提取底层数据张量（无梯度追踪）\n",
    "'''\n",
    "\n",
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b45fbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "内置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f00d5e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.023955Z",
     "iopub.status.busy": "2023-08-18T07:01:11.023046Z",
     "iopub.status.idle": "2023-08-18T07:01:11.033287Z",
     "shell.execute_reply": "2023-08-18T07:01:11.032096Z"
    },
    "origin_pos": 47,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0047, -0.0063, -0.0102, -0.0056]), tensor(0.))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear: # 检查模块是否为Linear层\n",
    "        # # 权重初始化：正态分布N(0, 0.01²)（原地）\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias) # # 偏置初始化：全零（原地）\n",
    "# 递归应用：此函数会被 apply() 自动传递给网络的每个子模块\n",
    "net.apply(init_normal)\n",
    "'''\n",
    "net[0]：访问第一层（如 nn.Linear(4,8)）\n",
    ".weight.data[0]：获取权重张量的第0行数据（形状 (8,)）\n",
    ".bias.data[0]：获取偏置张量的第0个元素（标量）\n",
    "返回值：(tensor([...]), tensor(0.))\n",
    "'''\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ee306c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.038321Z",
     "iopub.status.busy": "2023-08-18T07:01:11.037607Z",
     "iopub.status.idle": "2023-08-18T07:01:11.049009Z",
     "shell.execute_reply": "2023-08-18T07:01:11.047793Z"
    },
    "origin_pos": 52,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        # nn.init.constant_：原地将权重全部填充为指定值 1（所有元素相同）\n",
    "        nn.init.constant_(m.weight, 1) # 所有权重设为常数1\n",
    "        nn.init.zeros_(m.bias) # 偏置全零\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478059aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "对某些块应用不同的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a90ffaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.054335Z",
     "iopub.status.busy": "2023-08-18T07:01:11.053550Z",
     "iopub.status.idle": "2023-08-18T07:01:11.063215Z",
     "shell.execute_reply": "2023-08-18T07:01:11.062244Z"
    },
    "origin_pos": 57,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5579, -0.2446,  0.1389,  0.6962])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        # init_xavier：专为打破对称性设计，权重从[-a, a]区间均匀采样（a=sqrt(6/(fan_in + fan_out))）\n",
    "        # 适合ReLU激活函数，防止梯度消失/爆炸\n",
    "        nn.init.xavier_uniform_(m.weight) # Xavier均匀分布初始化\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42) # 常数42初始化\n",
    "# 应用于第0层（如 Linear(4,8)）,net[0] 所有 Linear 层 → Xavier初始化\n",
    "net[0].apply(init_xavier)  \n",
    "# 应用于第2层（如 Linear(8,1)）,net[2] 所有 Linear 层 → 常数42初始化\n",
    "net[2].apply(init_42) \n",
    "print(net[0].weight.data[0]) # 查看第0层权重第0行\n",
    "print(net[2].weight.data) # 查看第2层所有权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a70ae16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "自定义初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9166f6e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.068164Z",
     "iopub.status.busy": "2023-08-18T07:01:11.067460Z",
     "iopub.status.idle": "2023-08-18T07:01:11.079228Z",
     "shell.execute_reply": "2023-08-18T07:01:11.078069Z"
    },
    "origin_pos": 66,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  7.7417, -9.3673, -8.4343],\n",
       "        [ 5.8698,  0.0000,  0.0000, -9.7179]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        # 步骤1：均匀分布初始化\n",
    "        nn.init.uniform_(m.weight, -10, 10) # 权重从U[-10,10]采样\n",
    "        # 步骤2：稀疏化（保留大值，置零小值）\n",
    "        '''\n",
    "        创建布尔掩码（abs() >= 5），保留绝对值≥5的权重，其余置零\n",
    "        效果：约50%的权重被置零（因为均匀分布中≥5的概率约10%，但绝对值≥5包含正负两个方向，所以总概率约20%。\n",
    "        注意：如果区间是[-10,10]，则|w|≥5的概率是 (10-5)/10 = 0.5，即50%）\n",
    "        实际上，对于 U[-10,10] 分布，|w| >= 5 的概率是 50%（因为区间一半是[-10,-5]∪[5,10]）\n",
    "        最终结果：稀疏权重矩阵（约50%非零，50%为零）\n",
    "        mask = (m.weight.data.abs() >= 5)  # 布尔掩码: True/False\n",
    "        m.weight.data = m.weight.data * mask  # 乘法时True→1, False→0\n",
    "        布尔张量在数值运算中会自动转为 0 和 1：\n",
    "        True  转换为 1，weight * 1 = weight（保留原值）\n",
    "        False  转换为 0，weight * 0 = 0（置零）\n",
    "        '''\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2] # 查看第0层权重的前两行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b9af1f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.084158Z",
     "iopub.status.busy": "2023-08-18T07:01:11.083416Z",
     "iopub.status.idle": "2023-08-18T07:01:11.092672Z",
     "shell.execute_reply": "2023-08-18T07:01:11.091537Z"
    },
    "origin_pos": 71,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000,  8.7417, -8.3673, -7.4343])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "net[0]：访问第0层（如 Linear(4, 8)）\n",
    ".weight.data：获取权重参数的底层数据张量\n",
    "[:]：全切片，选中所有元素\n",
    "+= 1：原地操作，每个元素值 +1\n",
    "'''\n",
    "net[0].weight.data[:] += 1\n",
    "# [0, 0]：访问第0行第0列的特定元素（第一个输出神经元对第一个输入特征的权重）\n",
    "net[0].weight.data[0, 0] = 42\n",
    "# [0]：获取第0行（即第一个输出神经元对应的所有输入权重）\n",
    "# 返回值：形状为 (4,) 的张量\n",
    "net[0].weight.data[0] # 显示第0行所有权值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9031168e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "参数绑定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69660fa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:01:11.097767Z",
     "iopub.status.busy": "2023-08-18T07:01:11.096948Z",
     "iopub.status.idle": "2023-08-18T07:01:11.108904Z",
     "shell.execute_reply": "2023-08-18T07:01:11.107763Z"
    },
    "origin_pos": 77,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared = nn.Linear(8, 8)\n",
    "# .Sequential：不会复制shared，而是保持引用（net[2]和net[4]指向同一块内存）\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), # shared：一个 Linear(8,8)实例对象\n",
    "                    shared, nn.ReLU(), # 索引2：第1次使用\n",
    "                    shared, nn.ReLU(), # 索引4：第2次使用（同一对象！）\n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 比较两层的第0行权重\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "# 修改 net[2]的权重，net[4]同步变化\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 再次验证仍为 True，确认内存地址完全相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "required_libs": [],
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": true,
   "overlay": "<div class='my-top-right'><img height=80px src='http://d2l.ai/_static/logo-with-text.png'/></div><div class='my-top-left'></div>",
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
